---
title: "Linear regression for inferential and predictive modeling with examples in Julia"
description: "A tutorial on building and interpreting linear regression models for inferential and predictive modeling with examples in R."
author: 
    - name: "Dhruva Sambrani"
      orcid: "0000-0001-6254-4757"
date: "2023-08-09"
categories: [Linear Regression, Inferential Modeling, Predictive Modeling, Julia]
format:
  html:
    code-fold: false
jupyter: julia-1.9
citation: true
google-scholar: true
draft: true
execute: 
  eval: false
---

::: {.callout-note collapse="true"}
### Update history

2023-08-09 First draft
:::

# Introduction

This article is an extension of [Farmer, Rohit. 2023. "Linear Regression for Inferential and Predictive Modeling." July 13, 2023.](https://www.dataalltheway.com/posts/013-linear-regression) . Please check out the parent article for the theoretical background.

## Import Packages

```{julia}
import Pkg
Pkg.activate(".")
using IJulia
using PalmerPenguins 
using DataFrames
using Statistics
using Plots
using Pipe
using LsqFit
using GLM
using StatsPlots
using Statistics
using StatsBase
```

# Getting the data

Let us also drop the rows which have any missing data.

```{julia}
data = dropmissing(DataFrame(PalmerPenguins.load()))
```

The data already does not include the `year` data, and so we don't need to change anything.

```{julia}
first(data, 20)
```

# Exploratory Plots

We `groupby` `species` and `sex`, `select`ing the length of number of rows. Then, we simply find the `unique` rows.

```{julia}
g_data = @pipe data |>
	groupby(_, [:species, :sex]) |>
	combine(_, :sex=>length=>:counts) |>
	unique
labels = map(zip(g_data[!, :species], g_data[!, :sex])) do (i,j)
    "$(i)-$(j)"
end
plot(labels, g_data[!, :counts], st=:bar, title="Counts", label=false)
```

```{julia}
@. linear_model(x, p) = p[1]*x + p[2]
```

```{julia}
function plot_corrs(data, variable)
	p = plot(title=variable)
	ds1 = @pipe data |>
		_[!, [:species, variable, :body_mass_g]] |>
		groupby(_, :species)

	for sdf in ds1
		color = palette(:auto)[length(p.series_list)÷2 + 1]
		scatter!(sdf[!, variable], sdf[!, :body_mass_g],
			label=sdf[!, :species][1],
			markersize=2,
			color=color
		)
		fit = curve_fit(
			linear_model,
			sdf[!, variable], sdf[!, :body_mass_g], [0., 0.],
		)
		plot!(
			sdf[!, variable],
			linear_model(sdf[!, variable], fit.param),
			label=sdf[!, :species][1],
			lw = 2,
			color=color
		)
	end
	return p
end
```

```{julia}
plot(
	plot_corrs(data, :bill_depth_mm),
	plot_corrs(data, :bill_length_mm),
	plot_corrs(data, :flipper_length_mm),
	layout=(1,3),
	size=(1300, 400),
	ylabel="body_mass_g"
)
```

# First Model

```{julia}
fit_model_1 = lm(
	@formula(body_mass_g ~ bill_depth_mm + bill_length_mm + flipper_length_mm + sex),
	data,
)
```

We can do the ftest against the null model as

```{julia}
ftest(fit_model_1.model)
```

## Diagnostic Plots

```{julia}
x = predict(fit_model_1)
y = residuals(fit_model_1)
ystd = (y .- mean(y))/std(y);
```

```{julia}
scatter(x, y, title="Residuals vs Prediction", label=false)
```

```{julia}
qqplot(Normal(), ystd, title="qqplot of the standardized residuals")
```

```{julia}
scatter(x, sqrt.(abs.(ystd)), ylabel="√(Std Residuals)", xlabel="Prediction", label=false)
```

```{julia}
cdist = cooksdistance(fit_model_1)
plot(cdist, st=:sticks, title="Cook's Distance", label="")
high_dist = findall(>(0.02), cdist)
hist_dist_annots = map(high_dist) do i
	(i, cdist[i]+0.002, text(string(i), 8))
end
annotate!(hist_dist_annots)
hline!([0.02], linestyle=:dash, label=false)
```

::: callout-caution
## Unimplemented function

The leverage function is not implemented in the `GLM.jl` package yet, but there is a [PR in draft](https://github.com/JuliaStats/GLM.jl/pull/510).
:::

# Second Model

```{julia}
fit_model_2 = lm(
	@formula(body_mass_g ~ species + island + bill_depth_mm + bill_length_mm + flipper_length_mm + sex),
	data,
)
```

# Comparing the two models

`GLM.jl` provides an interface to run an FTest to compare if either model is better than the other.

```{julia}
ftest(fit_model_1.model, fit_model_2.model)
```

Inferring this test is left as an exercise to the reader, with reference to [the previous blogpost on Parametric tests](https://dataalltheway.com/posts/010-parametric-hypothesis-tests-r/#sec-f-test).

# Using the model as a prediction

This is also left as an exercise to the reader. You can partition the data using the following function:

```{julia}
using Random
function partition_data(data, train_frac = 0.7)
    n = nrow(data)
    idx = shuffle(1:n)
    train_idx = @view idx[1:floor(Int, train_frac*n)]
    test_idx = @view idx[(floor(Int, train_frac*n)+1):n]
    data[train_idx, :], data[test_idx, :]
end
```
