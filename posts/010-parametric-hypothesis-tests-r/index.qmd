---
title: "Parametric hypothesis tests with examples in R"
description: "A tutorial on parametric hypothesis tests with examples in R."
author: 
  - name: "Rohit Farmer"
    orcid: "0000-0003-4197-3047"
date: "2022-11-10"
categories: [How To, Parametric Tests, T-test, Z-test, F-test]
format:
  html:
    code-fold: false
    code-tools:
      source: false
      toggle: true
citation: true
google-scholar: true
bibliography: references.bib
---

::: {.callout-note collapse="true"}
### Update history

2022-11-10 First unfinished draft.
:::

# Introduction

A hypothesis test is a statistical test used to determine whether there is enough evidence to support a hypothesis. For example, there is a difference between the average height of males and females. A parametric hypothesis test assumes that the data you want to test is (or approximately is) [normally distributed](https://en.wikipedia.org/wiki/Normal_distribution). For example, height is normally distributed in a population. In addition, your data needs to be symmetrical since normally distributed data is symmetrical. If your data does not have the appropriate properties, then you use a non-parametric test.

There are different parametric tests, but the most common is the t-test. Below is the list of the four parametric hypothesis tests we will explore here.

-   Z-test (@sec-z-test)
-   T-test (@sec-t-test)
-   F-test

::: callout-tip
If you are primarily interested in code examples, please follow the navigation on the right; alternatively, you can test live code on Kaggle.
:::

Besides deciding which hypothesis test to use to answer the question at hand, we also need to decide a couple of other parameters, for example, whether the test would be one sample or two samples, paired or un-paired, and one or two-tailed. Below is a brief description of each parameter.

## Paired vs. unpaired tests

Paired tests are used to compare two related data groups, ***such as before and after measurements**.* Unpaired tests are used to compare two unrelated data groups, such as men and women.

## One sample vs. two sample tests

There are key differences between one-sample and two-sample hypothesis testing. Firstly, in one sample hypothesis testing, we are testing the mean of a single sample ***against a known population mean***. In contrast, two-sample hypothesis testing involves ***comparing the means of two independent samples***. Secondly, one-sample hypothesis testing only requires a single sample size, while two-sample hypothesis testing requires two. Finally, one-sample hypothesis testing is typically used when the population standard deviation is known, while two-sample hypothesis testing is used when the population standard deviation is unknown.

## One vs. two tailed tests

In a hypothesis test, the null and alternate hypotheses are stated in terms of population parameters. These hypotheses are:

-   Null hypothesis ($H_0$): The value of the population parameter is equal to the hypothesized value.
-   The alternate hypothesis ($H_1$): The value of the population parameter is not equal to the hypothesized value.

The hypothesis test is based on a sample from the population. This sample is used to test the hypotheses by deriving a test statistic. Finally, the value of the test statistic is compared to a critical value. The critical value depends on the alpha level, which is the likelihood of rejecting the null hypothesis when it is true.

The null and alternate hypotheses determine the direction of the test. There are two types of hypothesis tests: ***one-tailed and two-tailed tests***.

### One-tailed Test

A one-tailed test is conducted when the null and alternate hypotheses are stated in terms of ***"greater than" or "less than".***

For example, let's say that a company wants to test a new advertising campaign. The null hypothesis ($H_0$) is that the new campaign will have no effect on sales. The alternate hypothesis ($H_1$) is that the new campaign will increase sales.

The null hypothesis is stated as:

$H_0$ : The population mean is less than or equal to 10%.

The alternate hypothesis is stated as:

$H_1$ : The population mean is greater than 10%.

The test is conducted by taking a sample of data and calculating the mean. Then, the mean is compared to the critical value. ***The null hypothesis is rejected if the mean is greater than the critical value.***

### Two-tailed Test

A two-tailed test is conducted when the null and alternate hypotheses are stated in terms of ***"not equal to".***

Taking our advertising campaign example. The null hypothesis ($H_0$) is that the new campaign will have no effect on sales. The alternate hypothesis ($H_1$) is that the new campaign will increase or decrease sales.

The null hypothesis is stated as:

$H_0$ : The population mean is equal to 10%.

The alternate hypothesis is stated as:

$H_1$ : The population mean is not equal to 10%.

The test is conducted by taking a sample of data and calculating the mean. Then, the mean is compared to the critical value. ***The null hypothesis is rejected if the mean is not equal to the critical value.***

## The method

No matter which hypothesis testing method is selected following are the steps that are executed:

1.  Firstly, identify the null hypothesis $H_0:\mu = \mu_0$
2.  Then identify the alternative hypothesis $H_1$ and decide if it is of the form $H_1 : \mu \neq \mu_0$ (a two-tailed test) or if there is a specific direction for how the mean changes $H_1 : \mu > \mu_0$ or $H_1 : \mu < \mu_0$, (a one-tailed test).
3.  Next, calculate the test statistic. Compare the test statistic to the critical values and obtain a range for the p-value which is the probability that the difference between the two groups is due to chance. The test is usually used with a significance level of 0.05, which means that there is a 5% chance that the difference between the two groups is due to chance. However, per recommendations from the American Statistical Association we need to be careful when we state statements like ***"[statistically significant](https://www.tandfonline.com/toc/utas20/73/sup1)"***.
4.  Form conclusions. If your test statistic is greater than the critical values in the table, it is significant. You can reject the null hypothesis at that level, otherwise you accept it.

## Dataset

For our example exercises, we will use the dataset from Open Case Studies [***"exploring global patterns of obesity across rural and urban regions"***](https://www.opencasestudies.org/ocs-bp-rural-and-urban-obesity) [@wright] . Body mass index ([BMI](https://www.cdc.gov/healthyweight/assessing/bmi/adult_bmi/index.html)) is often used as a proxy for **adiposity** (the condition of having excess body fat) and is measured as an individual's weight in kilograms ($kg$) or pounds ($lbs$) divided by the individual's height in meters ($m^2$) squared. Recently, an article published in *Nature* evaluated and compared the BMI of populations in rural and urban communities around the world [@risingr2019]. The article challenged the widely-held view that increased urbanization was one of the major reasons for increased global obesity rates. This view came about because many countries around the world have shown increased urbanization levels in parallel with increased obesity rates. In this article, however, the NCD-RisC argued that this might not be the case and that in fact for most regions around the world, BMI measurements are increasing in rural populations just as much, if not more so, than urban populations. Furthermore, this study suggested that obesity has particularly increased in female populations in rural regions:

> "We noted a persistently higher rural BMI, especially for women."

We will fetch the cleaned version of the data from the the [Open Case Studies GitHub repository](https://github.com//opencasestudies/ocs-bp-rural-and-urban-obesity) as data wrangling is out of the scope of this tutorial.

```{r}
suppressMessages(library(tidyverse))
suppressMessages(library(DT))
suppressMessages(library(kableExtra))
dat <- readr::read_csv("https://raw.githubusercontent.com/opencasestudies/ocs-bp-rural-and-urban-obesity/master/data/wrangled/BMI_long.csv", show_col_types = FALSE)

DT::datatable(dat)
```

# Z-test {#sec-z-test}

A z-test is used to compare two population means when the sample size is large and the population variance is known.

These are some conditions for using this type of test:

-   The data must be **normally distributed**.
-   All data points must be **independent**.
-   For each sample the **variances must be equal**.

@eq-z-test shows the formula for a z-test.

$$
    z = \dfrac{\bar{x} - \mu_0}{\sqrt{\dfrac{\sigma^2}{n}}}
$$ {#eq-z-test}

Where $\bar{x}$ is the sample mean, $\mu_0$ is the population mean, $\sigma^2$ is the population variance, and ${n}$ is the number of samples.

## Two sample unpaired z-test

$$
\begin{equation} z = \dfrac{\bar{x_1} - \bar{x_2}}{\sqrt{\dfrac{\sigma_1^2}{n_1} +\dfrac{\sigma_2^2}{n_2}}} \end{equation}
$$ {#eq-z-test-un-paired}

Where $\bar{x_1}$ and $\bar{x_2}$ are the sample means, $\sigma_1^2$ and $\sigma_2^2$ are the population variances, and ${n_1}$ and ${n_2}$ are the number of samples.

### Example code in R

```{r}
suppressMessages(library(PASWR2))

# Calculate population standard deviations
sig_x <- dplyr::filter(dat, Sex == "Women", Year == 1985) %>%
  dplyr::pull(BMI) %>% na.omit() %>% sd()
sig_y <- dplyr::filter(dat, Sex == "Women", Year == 2017) %>%
  dplyr::pull(BMI) %>% na.omit() %>% sd()

# Fetch a random sample of BMI data for women in the year 1985 and 2017
x1 <- dplyr::filter(dat, Sex == "Women", Year == 1985) %>%
  dplyr::pull(BMI) %>% na.omit() %>%
  sample(.,300)
x2 <- dplyr::filter(dat, Sex == "Women", Year == 2017) %>%
  dplyr::pull(BMI) %>% na.omit() %>%
  sample(.,300)

# Perform a two sample (unpaired) t-test between x1 and x2
(z_res <- PASWR2::z.test(x1, x2, mu = 0, sigma.x = sig_x, sigma.y = sig_y))
```

```{r}
# Fetch z-test result metrics and present them in a tidy table
broom::tidy(z_res) %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```

## Two sample paired z-test

$$
\begin{equation} z= \dfrac{\bar{d}- D}{\sqrt{\dfrac{\sigma_d^2}{n}}} \end{equation}
$$ {#eq-z-test-paired}

Where $\bar{d}$ is the mean of the differences between the samples, $D$ is the hypothesised mean of the differences (usually this is zero), $n$ is the sample size and $\sigma_d^2$ is the population variance of the differences.

### Example code in R

```{r}
# Fetch a first 300 samples of BMI data for women in the year 1985 and 2017
x1 <- dplyr::filter(dat, Sex == "Women", Year == 1985) %>%
  dplyr::pull(BMI) %>% na.omit() 
x1 <- x1[1:300]
x2 <- dplyr::filter(dat, Sex == "Women", Year == 2017) %>%
  dplyr::pull(BMI) %>% na.omit()
x2 <- x2[1:300]

# Perform a two sample (unpaired) t-test between x1 and x2
(z_res <- PASWR2::z.test(x1, x2, sigma.x = sig_x, sigma.y = sig_y, sigma.d = abs(sig_y - sig_x), paired = TRUE))
```

```{r}
# Fetch z-test result metrics and present them in a tidy table
broom::tidy(z_res) %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```

# T-test {#sec-t-test}

The t-test is used to determine whether two groups or one group and a known mean are significantly different from each other. The t-test can be used to compare means, or to compare proportions and can be used to compare two independent groups, or two dependent groups. The t-test is based on the t-statistic, which for a one group/sample where the mean for comparison is known is calculated using the following formula:

$$
\begin{equation} t = \dfrac{\bar{x}-\mu}{\sqrt{\dfrac{s^2}{n}}} \end{equation}
$$ {#eq-t-test}

Where $\bar x$ is the mean of the first group (one sample), $\mu$ is the known mean, $s^2$ is the standard deviation of the first group, and $n$ is the number of observations in the first group.

### Example code in R

```{r}
# Fetch the BMI data for men from rural areas in the year 2017
x <- dplyr::filter(dat, Sex == "Men", Region == "Rural", Year == 2017) %>%
  dplyr::pull(BMI) 

# Perform one sample t-test between x and a known mean = 24.5
(t_res <- t.test(x, mu = 24.5))

```

```{r}
# Fetch t-test result metrics and present them in a tidy table
broom::tidy(t_res) %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```

### Two sample unpaired (independent) t-test

$$
\begin{equation} t = \dfrac{\bar{x_1} - \bar{x_2}}{\sqrt{s^2\bigg(\dfrac{1}{n_1} + \dfrac{1}{n_2}\bigg)}} \end{equation}
$$ {#eq-t-test-independent}

Where the pooled standard deviation $s$ is:

$$
\begin{equation} s =\sqrt{\dfrac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 +n_2 -2}} \end{equation} 
$$ {#eq-pooled-sd}

Where $\bar x_1$ and $\bar x_2$ are the means from the two samples, likewise $n_1$ and $n_2$ are the sample sizes and $s_1^2$ and $s_2^2$ are the sample variances. This test statistic you will compare to *t*-tables on $(n1+n2−2)$ degrees of freedom.

### Example code in R

```{r}
# Fetch the BMI data for women from rural and urban areas in the year 1985
x1 <- dplyr::filter(dat, Sex == "Women", Region == "Rural", Year == 1985) %>%
  dplyr::pull(BMI) 
x2 <- dplyr::filter(dat, Sex == "Women", Region == "Urban", Year == 1985) %>%
  dplyr::pull(BMI) 

# Perform a two sample (unpaired) t-test between x1 and x2
(t_res <- t.test(x1, x2, var.equal = TRUE))
```

We use the `var.equal = TRUE` option here to use the pooled standard deviation $s$ @eq-pooled-sd.

::: callout-note
For the pooled variance *t*-test to be appropriate you rely on the assumption that the two samples come from the same population and have equal variance.
:::

```{r}
# Fetch t-test result metrics and present them in a tidy table
broom::tidy(t_res) %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```

### Two sample paired (dependent) t-test

$$
\begin{equation} t = \dfrac{\bar{d}}{\sqrt{\dfrac{s^2}{n}}} \end{equation}
$$ {#eq-t-test-paired}

Where $\bar d$ is the mean of the differences between the samples. You will compare the *t*-statistic to the critical values in a *t*-table on $(n−1)$ degrees of freedom. Here $s$ is the standard deviation of the differences.

### Example code in R

```{r}
# Perform a two sample (paired) t-test between x1 and x2
(t_res <- t.test(x1, x2, paired = TRUE))
```

```{r}
# Fetch t-test result metrics and present them in a tidy table
broom::tidy(t_res) %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```

# References

::: {#refs}
:::

## Website(s)

-   [Parametric Hypothesis Tests (Psychology)](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/psychology/parametric-hypothesis-tests.html#:~:text=A%20parametric%20test%20relies%20upon,use%20a%20non%2Dparametric%20test.)
