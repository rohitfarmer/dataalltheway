{
  "hash": "500ff65c9fb24712bbc3d9ed48043570",
  "result": {
    "markdown": "---\ntitle: \"Type inference in readr and arrow\"\ndescription: \"A discussion on how CSV type inference works in the R packages readr and arrow, and the differences between the two.\"\nauthor: \n  - name: \"Nic Crane\"\n    affiliations:\n      - name: \"https://thisisnic.github.io/\"\ndate: \"2022-11-21\"\ncategories: [Readr, Apache Arrow]\nformat:\n  html:\n    code-fold: false\n    code-tools:\n      source: false\n      toggle: false\ncitation:\n  type: post-weblog\n  url: \"https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/\"\ngoogle-scholar: true\n---\n\n\n::: {.callout-note collapse=\"true\"}\n### Update history\n\n2022-11-23 This article is cross-posted from <https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/> with permission.\n:::\n\n# Introduction\n\nThe CSV format is widely used in data science, and at its best works well as a simple human-readable format that is widely known and understood. The simplicity of CSVs though, as a basic text format also has its drawbacks. One is that it contains no information about data types of its columns, and if you're working with CSVs in an application more complex than a text editor, those data types must be inferred by whatever is reading the data.\n\nIn this blog post, I'm going to discuss how CSV type inference works in the R packages [readr](https://readr.tidyverse.org/) and [arrow](https://arrow.apache.org/docs/r/), and highlight the differences between the two.\n\nBefore I get started though, I'd like to acknowledge that this post is an exercise in examining the underlying mechanics of the two packages. In practice, I've found that when working with datasets small enough to fit in-memory, it's much more fruitful to either read in the data first and then manipulate it into the required shape, or just specify the column types up front. Still, the strategies for automatic guessing are interesting to explore.\n\n<p align=\"center\">\n\n![](images/readr_hex.png){width=\"200px\"} ![](images/arrow-logo_hex_black-txt_white-bg.png){width=\"200px\"}\n\n</p>\n\n\n\n\n\n# How does type inference work in readr?\n\nSince readr version 2.0.0 (released in July 2020), there was a significant overhaul of the underlying code, which subsequently depended on the [vroom](https://vroom.r-lib.org/) package.\n\nThe type inference is done by a C++ function in vroom called `guess_type__()` which guesses types in the following order:\n\n-   Does the column contain 0 rows? If yes, return \"character\"\n\n-   Are all values missing? If yes, return \"logical\"\n\n-   Tries to parse column to each of these formats and returns the first one it successfully parses:\n\n    -   Logical\n\n    -   Integer (though the default is to not look for these)\n\n    -   Double\n\n    -   Number (a special type which can remove characters from strings representing numbers and then convert them to doubles)\n\n    -   Time\n\n    -   Date\n\n    -   Datetime\n\n    -   Character\n\nThe ordering above in the parsing bullet point goes from most to least strict in terms of the conditions which have to be met to successfully parse an input as that data type. For example, for a column to be of logical type, it can only contain a small subset of values representing true (`T`, `t`, `True`, `TRUE`, `true`), false (`F`, `f`, `False`, `FALSE`, `false`) or NA, which is why this is the most strict type, but all of the other types could be read in as character data, which is the least strict and why this is last in the order.\n\n# How does type inference work in arrow?\n\nIn arrow, `read_csv_arrow()` handles CSV reading, and much of its interface has been designed to closely follow the excellent APIs of `readr::read_csv()` and `vroom::vroom()`. The intention here is that users can use parameter names they're familiar with from the aforementioned readers when using arrow, and get the same results. The underlying code is pretty different though.\n\nIn addition, Arrow has a different set of possible data types compared to R; see [the Arrow docs](https://arrow.apache.org/docs/r/articles/arrow.html) for more information about the mapping between R data types and Arrow types.\n\nIn [the Arrow docs](https://arrow.apache.org/docs/cpp/csv.html#data-types), we can see that types are inferred in this order:\n\n-   Null\n\n-   Int64\n\n-   Boolean\n\n-   Date32\n\n-   Timestamp (with seconds unit)\n\n-   Float64\n\n-   Dictionary\\<String\\> (if ConvertOptions::auto_dict_encode is true)\n\n-   Dictionary\\<Binary\\> (if ConvertOptions::auto_dict_encode is true)\n\n-   String\n\n-   Binary\n\nNote that if you use `arrow::read_csv_arrow()` with parameter `as_data_frame = TRUE` (the default), the Arrow data types are then converted to R data types.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_data <- data.frame(x = c(1, 2, 3), y = c(\"a\", \"b\", \"c\"), z = c(1.1, 2.2, 3.3))\n\nreadr::write_csv(simple_data, \"simple_data.csv\")\n\n# columns are arrow's int64, string, and double (aka float64) types\narrow::read_csv_arrow(\"simple_data.csv\", as_data_frame = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n3 rows x 3 columns\n$x <int64>\n$y <string>\n$z <double>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# columns converted to R's integer, character, and double types\narrow::read_csv_arrow(\"simple_data.csv\", as_data_frame = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n      x y         z\n  <int> <chr> <dbl>\n1     1 a       1.1\n2     2 b       2.2\n3     3 c       3.3\n```\n:::\n:::\n\n\n# What are the main differences between readr and arrow type inference?\n\nAlthough there appear to be quite a few differences between the order of type inference when comparing arrow and readr, in practice, this doesn't have much effect. Type inference for logical/boolean and integer values are the opposite way round, but given that the underlying data that translates into these types looks very different, they're not going to be mixed up. The biggest differences come from custom behaviours which are specific to readr and arrow; I've outlined them below.\n\n### Guessing integers\n\nIn the code for readr, the default setting is for numeric values to always be read in as doubles but never integers. If you want readr to guess that a column may be an integer, you need to read it in as character data, and then call `type_convert()`. This isn't necessarily a great workflow though, and in most cases it would make sense to just manually specify the column type instead of having it inferred.\n\nIn arrow, if data can be represented as integers but not doubles, then it will be.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nint_or_dbl <- data.frame(\n  x = c(1L, 2L, 3L)\n)\n\nreadr::write_csv(int_or_dbl, \"int_or_dbl.csv\")\n\nreadLines(\"int_or_dbl.csv\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"x\" \"1\" \"2\" \"3\"\n```\n:::\n\n```{.r .cell-code}\n# double\nreadr::read_csv(\"int_or_dbl.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 3 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): x\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 1\n      x\n  <dbl>\n1     1\n2     2\n3     3\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# integer via inference\nreadr::read_csv(\"int_or_dbl.csv\", col_types = list(.default = col_character())) %>%\n  type_convert(guess_integer = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  x = col_integer()\n)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 1\n      x\n  <int>\n1     1\n2     2\n3     3\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# integer via specification\nreadr::read_csv(\"int_or_dbl.csv\", col_types = list(col_integer()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 1\n      x\n  <int>\n1     1\n2     2\n3     3\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# integer via inference\narrow::read_csv_arrow(\"int_or_dbl.csv\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 1\n      x\n  <int>\n1     1\n2     2\n3     3\n```\n:::\n:::\n\n\n## 32-bit integers\n\nAnother difference between readr and arrow is the difference between how integers larger than 32 bits are read in. Natively, R can only support 32-bit integers, though it can support 64-bit integers via the [bit64](https://cran.r-project.org/web/packages/bit64/index.html) package. If we create a CSV with one column containing the largest integer that R can natively support, and then another column containing that value plus 1, we get different behaviour when we import this data with readr and arrow. In readr, when we enable integer guessing, the smaller value is read in as an integer, and the larger value is read in as a double. However, once we move over to manually specifying column types, we can use `vroom::col_big_integer()` to use bit64 and get us a large integer column. The arrow package also uses bit64, and its integer guessing results in 64-bit integer via inference.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsixty_four <- data.frame(x = 2^31 - 1, y = 2^31)\n\nreadr::write_csv(sixty_four, \"sixty_four.csv\")\n\n# doubles by default\nreadr::read_csv(\"sixty_four.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 1 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n           x          y\n       <dbl>      <dbl>\n1 2147483647 2147483648\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 32 bit integer or double depending on value size\nreadr::read_csv(\"sixty_four.csv\", col_types = list(.default = col_character())) %>%\n  type_convert(guess_integer = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  x = col_integer(),\n  y = col_double()\n)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n           x          y\n       <int>      <dbl>\n1 2147483647 2147483648\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# integers by specification\nreadr::read_csv(\n  \"sixty_four.csv\",\n  col_types = list(x = col_integer(), y = vroom::col_big_integer())\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n           x          y\n       <int>    <int64>\n1 2147483647 2147483648\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# integers by inference\narrow::read_csv_arrow(\"sixty_four.csv\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n           x          y\n       <int>    <int64>\n1 2147483647 2147483648\n```\n:::\n:::\n\n\n## The \"number\" parsing strategy\n\nOne really cool feature in readr is the \"number\" parsing strategy. This allows values which have been stored as character data with commas to separate the thousands to be read in as doubles. This is not supported in arrow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnumber_type <- data.frame(\n  x = c(\"1,000\", \"1,250\")\n)\n\nreadr::write_csv(number_type, \"number_type.csv\")\n\n# double type, but parsed in as number in column spec shown below\nreadr::read_csv(\"number_type.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 2 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nnum (1): x\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 1\n      x\n  <dbl>\n1  1000\n2  1250\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in as character data in Arrow\narrow::read_csv_arrow(\"number_type.csv\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 1\n  x    \n  <chr>\n1 1,000\n2 1,250\n```\n:::\n:::\n\n\n## Dictionaries/Factors\n\nAnyone who's been around long enough might remember that R's native CSV reading function `read.csv()` had a default setting of importing character columns as factors (I definitely have `read.csv(..., stringAsFactors=FALSE)` carved into a groove in some dark corner of my memory). This default was changed in version 4.0.0, released in April 2020, reflecting the fact that in most cases users want their string data to be imported as characters unless otherwise specified. Still, some datasets contain character data which users do want to import as factors. In readr, this can be controlled by manually specifying the column as a factor\n\nIn arrow, if you don't want to individually specify column types, you can set up an option to import character columns as dictionaries (the Arrow equivalent of factors), which are converted into factors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndict_type <- data.frame(\n  x = c(\"yes\", \"no\", \"yes\", \"no\")\n)\n\nreadr::write_csv(dict_type, \"dict_type.csv\")\n\n# character data\nreadr::read_csv(\"dict_type.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 4 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): x\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 1\n  x    \n  <chr>\n1 yes  \n2 no   \n3 yes  \n4 no   \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# factor data\nreadr::read_csv(\"dict_type.csv\", col_types = list(x = col_factor()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 1\n  x    \n  <fct>\n1 yes  \n2 no   \n3 yes  \n4 no   \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up the option. there's an open ticket to make this code a bit nicer to read.\nauto_dict_option <- arrow::CsvConvertOptions$create(auto_dict_encode = TRUE)\narrow::read_csv_arrow(\"dict_type.csv\", convert_options = auto_dict_option)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 1\n  x    \n  <fct>\n1 yes  \n2 no   \n3 yes  \n4 no   \n```\n:::\n:::\n\n\n## Custom logical/boolean values\n\nAnother slightly niche but potentially useful piece of functionality available in arrow is the ability to customise which values can be parsed as logical/boolean type and how they translate to `TRUE`/`FALSE`. This can be achieved by setting some custom conversion options.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalternative_true_false <- arrow::CsvConvertOptions$create(\n  false_values = \"no\", true_values = \"yes\"\n)\narrow::read_csv_arrow(\"dict_type.csv\", convert_options = alternative_true_false)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 1\n  x    \n  <lgl>\n1 TRUE \n2 FALSE\n3 TRUE \n4 FALSE\n```\n:::\n:::\n\n\n## Using schemas for manual control of data types\n\nAlthough relying on the reader itself to guess your column types can work well, what if you want more precise control?\n\nIn readr, you can use the `col_types` parameter to specify column types. You can use the same parameter in arrow to use R type specifications.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngiven_types <- data.frame(x = c(1, 2, 3), y = c(4, 5, 6))\n\nreadr::write_csv(given_types, \"given_types.csv\")\n\nreadr::read_csv(\"given_types.csv\", col_types = list(col_integer(), col_double()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n      x     y\n  <int> <dbl>\n1     1     4\n2     2     5\n3     3     6\n```\n:::\n:::\n\n\nYou can also use this shortcode specification. Here, \"i\" means integer and \"d\" means double.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreadr::read_csv(\"given_types.csv\", col_types = \"id\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n      x     y\n  <int> <dbl>\n1     1     4\n2     2     5\n3     3     6\n```\n:::\n:::\n\n\nIn arrow you can use the shortcodes (though not the `col_*()` functions), but you must specify the column names.\n\nWe skip the first row as our data has a header row - this is the same behaviour as when we use both names and types in `readr::read_csv()` which then assumes that the header row is data if we don't skip it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narrow::read_csv_arrow(\"given_types.csv\", col_names = c(\"x\", \"y\"), col_types = \"id\", skip = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n      x     y\n  <int> <dbl>\n1     1     4\n2     2     5\n3     3     6\n```\n:::\n:::\n\n\nWhat if you want to use Arrow types instead of R types though? In this case, you need to use a schema. I won't go into detail here, but in short, schemas are lists of fields, each of which contain a field name and a data type. You can specify a schema like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# this gives the same result as before - because our Arrow data has been converted to the relevant R type\narrow::read_csv_arrow(\"given_types.csv\", schema = schema(x = int8(), y = float32()), skip = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n      x     y\n  <int> <dbl>\n1     1     4\n2     2     5\n3     3     6\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# BUT, if you don't read it in as a data frame you'll see the Arrow type\narrow::read_csv_arrow(\"given_types.csv\", schema = schema(x = int8(), y = float32()), skip = 1, as_data_frame = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n3 rows x 2 columns\n$x <int8>\n$y <float>\n```\n:::\n:::\n\n\n# Parquet\n\nAn alternative approach is to use Parquet format, which stores the data types along with the data. This means that if you're sharing your data with others, you don't need to worry about it being read in as the wrong data types. In a follow-up post I'll explore the Parquet format and compare management of data types in CSVs and Parquet.\n\n## Further Reading\n\nIf you want a much more detailed discussion of Arrow data types, see [this excellent blog post](https://blog.djnavarro.net/posts/2022-03-04_data-types-in-arrow-and-r/) by Danielle Navarro.\n\n# Thanks\n\nHuge thanks to everyone who helped review and tweak this blog post, and special thanks to [Jenny Bryan](https://github.com/jennybc) who gave some really helpful feedback on the content on readr/vroom!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}