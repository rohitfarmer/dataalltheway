{
  "hash": "4c6d8601d7477d3326fe91a052a4523b",
  "result": {
    "markdown": "---\ntitle: Linear regression for inferential and predictive modeling with examples in Julia\ndescription: A tutorial on building and interpreting linear regression models for inferential and predictive modeling with examples in R.\nauthor:\n  - name: Dhruva Sambrani\n    orcid: 0000-0001-6254-4757\ndate: '2023-08-09'\ncategories:\n  - Linear Regression\n  - Inferential Modeling\n  - Predictive Modeling\n  - Julia\nformat:\n  html:\n    code-fold: false\ncitation: true\ngoogle-scholar: true\ndraft: true\nexecute:\n  eval: false\n---\n\n::: {.callout-note collapse=\"true\"}\n### Update history\n\n2023-08-09 First draft\n:::\n\n# Introduction\n\nThis article is an extension of [Farmer, Rohit. 2023. \"Linear Regression for Inferential and Predictive Modeling.\" July 13, 2023.](https://www.dataalltheway.com/posts/013-linear-regression) . Please check out the parent article for the theoretical background.\n\n## Import Packages\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nimport Pkg\nPkg.activate(\".\")\nusing IJulia\nusing PalmerPenguins \nusing DataFrames\nusing Statistics\nusing Plots\nusing Pipe\nusing LsqFit\nusing GLM\nusing StatsPlots\nusing Statistics\nusing StatsBase\n```\n:::\n\n\n# Getting the data\n\nLet us also drop the rows which have any missing data.\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\ndata = dropmissing(DataFrame(PalmerPenguins.load()))\n```\n:::\n\n\nThe data already does not include the `year` data, and so we don't need to change anything.\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nfirst(data, 20)\n```\n:::\n\n\n# Exploratory Plots\n\nWe `groupby` `species` and `sex`, `select`ing the length of number of rows. Then, we simply find the `unique` rows.\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\ng_data = @pipe data |>\n\tgroupby(_, [:species, :sex]) |>\n\tcombine(_, :sex=>length=>:counts) |>\n\tunique\nlabels = map(zip(g_data[!, :species], g_data[!, :sex])) do (i,j)\n    \"$(i)-$(j)\"\nend\nplot(labels, g_data[!, :counts], st=:bar, title=\"Counts\", label=false)\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\n@. linear_model(x, p) = p[1]*x + p[2]\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nfunction plot_corrs(data, variable)\n\tp = plot(title=variable)\n\tds1 = @pipe data |>\n\t\t_[!, [:species, variable, :body_mass_g]] |>\n\t\tgroupby(_, :species)\n\n\tfor sdf in ds1\n\t\tcolor = palette(:auto)[length(p.series_list)÷2 + 1]\n\t\tscatter!(sdf[!, variable], sdf[!, :body_mass_g],\n\t\t\tlabel=sdf[!, :species][1],\n\t\t\tmarkersize=2,\n\t\t\tcolor=color\n\t\t)\n\t\tfit = curve_fit(\n\t\t\tlinear_model,\n\t\t\tsdf[!, variable], sdf[!, :body_mass_g], [0., 0.],\n\t\t)\n\t\tplot!(\n\t\t\tsdf[!, variable],\n\t\t\tlinear_model(sdf[!, variable], fit.param),\n\t\t\tlabel=sdf[!, :species][1],\n\t\t\tlw = 2,\n\t\t\tcolor=color\n\t\t)\n\tend\n\treturn p\nend\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\nplot(\n\tplot_corrs(data, :bill_depth_mm),\n\tplot_corrs(data, :bill_length_mm),\n\tplot_corrs(data, :flipper_length_mm),\n\tlayout=(1,3),\n\tsize=(1300, 400),\n\tylabel=\"body_mass_g\"\n)\n```\n:::\n\n\n# First Model\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\nfit_model_1 = lm(\n\t@formula(body_mass_g ~ bill_depth_mm + bill_length_mm + flipper_length_mm + sex),\n\tdata,\n)\n```\n:::\n\n\nWe can do the ftest against the null model as\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nftest(fit_model_1.model)\n```\n:::\n\n\n## Diagnostic Plots\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nx = predict(fit_model_1)\ny = residuals(fit_model_1)\nystd = (y .- mean(y))/std(y);\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.julia .cell-code}\nscatter(x, y, title=\"Residuals vs Prediction\", label=false)\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.julia .cell-code}\nqqplot(Normal(), ystd, title=\"qqplot of the standardized residuals\")\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.julia .cell-code}\nscatter(x, sqrt.(abs.(ystd)), ylabel=\"√(Std Residuals)\", xlabel=\"Prediction\", label=false)\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.julia .cell-code}\ncdist = cooksdistance(fit_model_1)\nplot(cdist, st=:sticks, title=\"Cook's Distance\", label=\"\")\nhigh_dist = findall(>(0.02), cdist)\nhist_dist_annots = map(high_dist) do i\n\t(i, cdist[i]+0.002, text(string(i), 8))\nend\nannotate!(hist_dist_annots)\nhline!([0.02], linestyle=:dash, label=false)\n```\n:::\n\n\n::: callout-caution\n## Unimplemented function\n\nThe leverage function is not implemented in the `GLM.jl` package yet, but there is a [PR in draft](https://github.com/JuliaStats/GLM.jl/pull/510).\n:::\n\n# Second Model\n\n::: {.cell execution_count=15}\n``` {.julia .cell-code}\nfit_model_2 = lm(\n\t@formula(body_mass_g ~ species + island + bill_depth_mm + bill_length_mm + flipper_length_mm + sex),\n\tdata,\n)\n```\n:::\n\n\n# Comparing the two models\n\n`GLM.jl` provides an interface to run an FTest to compare if either model is better than the other.\n\n::: {.cell execution_count=16}\n``` {.julia .cell-code}\nftest(fit_model_1.model, fit_model_2.model)\n```\n:::\n\n\nInferring this test is left as an exercise to the reader, with reference to [the previous blogpost on Parametric tests](https://dataalltheway.com/posts/010-parametric-hypothesis-tests-r/#sec-f-test).\n\n# Using the model as a prediction\n\nThis is also left as an exercise to the reader. You can partition the data using the following function:\n\n::: {.cell execution_count=17}\n``` {.julia .cell-code}\nusing Random\nfunction partition_data(data, train_frac = 0.7)\n    n = nrow(data)\n    idx = shuffle(1:n)\n    train_idx = @view idx[1:floor(Int, train_frac*n)]\n    test_idx = @view idx[(floor(Int, train_frac*n)+1):n]\n    data[train_idx, :], data[test_idx, :]\nend\n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}