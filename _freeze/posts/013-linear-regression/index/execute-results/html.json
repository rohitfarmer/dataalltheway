{
  "hash": "887bd68c509d8d60660abfd26e38cbf1",
  "result": {
    "markdown": "---\ntitle: \"Linear regression for inferential and predictive modeling\"\ndescription: \"A tutorial on building and interpreting linear regression models for inferential and predictive modeling with examples in R.\"\nauthor: \n    - name: \"Rohit Farmer\"\n      orcid: \"0000-0003-4197-3047\"\ndate: \"2023-07-13\"\ncategories: [Linear Regression, Inferential Modeling, Predictive Modeling, R]\nformat:\n  html:\n    code-fold: false\n    code-tools:\n      source: false\n      toggle: true\nexecute: \n  eval: true\ncitation: true\ngoogle-scholar: true\ndraft: false\nbibliography: references.bib\n---\n\n\n::: {.callout-note collapse=\"true\"}\n### Update history\n\n2023-07-13 First draft\n:::\n\n# Introduction\n\nLinear regression is a statistical method used to model the linear relationship between a dependent variable (target) and one or more independent variables (predictors/features). It is one of the most widely used techniques in predictive modeling and is often used to understand the underlying relationship between different variables and to make predictions based on this relationship.\n\nIn linear regression, the goal is to find the **line of best fit** that minimizes the distance between the data points and the line. The line is represented by the equation $$Y = mX + b$$ {#eq-ymxc} $Y$ is the dependent variable, $X$ is the independent variable, $m$ is the slope of the line, and $b$ is the intercept. The slope represents the change in $Y$ for a given change in $X$, and the intercept is the point at which the line crosses the $Y-axis$.\n\nTo understand the mathematical reasoning behind linear regression, consider the simple example:\n\nSuppose we have a dataset containing the weight and heights of a group of individuals. We can plot the data on a scatter plot and observe the relationship between weight and height (@fig-lineof-best-fit). If there is a linear relationship between the two variables, we can use linear regression to model this relationship. However, for the practical explanation in this tutorial we will use a more interesting and research based dataset than this simple example of linear relationship between age and height.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The line (red) of best fit between weight and height. The blue dots are the data points and the green lines shows the distance between the data points and the line.](index_files/figure-html/fig-lineof-best-fit-1.png){#fig-lineof-best-fit width=672}\n:::\n:::\n\n\n# Dataset {#dataset}\n\nIn this tutorial we will use [\"palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data\"](%22https://cran.r-project.org/web/packages/palmerpenguins/index.html%22) by [Allison Horst](https://github.com/allisonhorst/palmerpenguins). The simplified version of Palmerpenguins dataset contains size measurements for 344 adult foraging Ad√©lie, Chinstrap, and Gentoo penguins observed on islands in the Palmer Archipelago near Palmer Station, Antarctica. The original data were collected and made available by Dr. Kristen Gorman and the Palmer Station Long Term Ecological Research (LTER) Program.\n\n[![Meet the Palmer penguins. Artwork by \\@allison_horst.](images/penguins.png){fig-align=\"center\" width=\"531\"}](https://allisonhorst.github.io/palmerpenguins/articles/art.html)\n\n## Culmen measurements\n\nWhat are culmen length & depth? The culmen is \"the upper ridge of a bird's beak\" (definition from Oxford Languages). In the simplified `penguins` subset, culmen length and depth have been updated to variables named `bill_length_mm` and `bill_depth_mm`.\n\nFor this penguin data, the bill/culmen length and depth are measured as shown below.\n\n[![Culmen measurements. Artwork by \\@allison_horst.](images/culmen_depth.png){fig-align=\"center\" width=\"548\"}](https://allisonhorst.github.io/palmerpenguins/articles/art.html)\n\n## Data filtering and transformation\n\nBefore we can build our model(s) let's filter the data for non useful features or missing values. The unfiltered dataset contains 344 rows and 7 columns. We will first drop `year` column and then remove any row with no value for any features. Below is the code to load the dataset and filter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nlibrary(palmerpenguins)\ndf_penguins <- penguins %>% dplyr::select(-c(year)) %>% na.omit()\n\nDT::datatable(df_penguins, options = list(pageLength = 5))\n```\n\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-8e1fe96c0688a849169a\" style=\"width:100%;height:auto;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-8e1fe96c0688a849169a\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"258\",\"259\",\"260\",\"261\",\"262\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"272\",\"273\",\"274\",\"275\",\"276\",\"277\",\"278\",\"279\",\"280\",\"281\",\"282\",\"283\",\"284\",\"285\",\"286\",\"287\",\"288\",\"289\",\"290\",\"291\",\"292\",\"293\",\"294\",\"295\",\"296\",\"297\",\"298\",\"299\",\"300\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"307\",\"308\",\"309\",\"310\",\"311\",\"312\",\"313\",\"314\",\"315\",\"316\",\"317\",\"318\",\"319\",\"320\",\"321\",\"322\",\"323\",\"324\",\"325\",\"326\",\"327\",\"328\",\"329\",\"330\",\"331\",\"332\",\"333\"],[\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\"],[\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\"],[39.1,39.5,40.3,36.7,39.3,38.9,39.2,41.1,38.6,34.6,36.6,38.7,42.5,34.4,46,37.8,37.7,35.9,38.2,38.8,35.3,40.6,40.5,37.9,40.5,39.5,37.2,39.5,40.9,36.4,39.2,38.8,42.2,37.6,39.8,36.5,40.8,36,44.1,37,39.6,41.1,36,42.3,39.6,40.1,35,42,34.5,41.4,39,40.6,36.5,37.6,35.7,41.3,37.6,41.1,36.4,41.6,35.5,41.1,35.9,41.8,33.5,39.7,39.6,45.8,35.5,42.8,40.9,37.2,36.2,42.1,34.6,42.9,36.7,35.1,37.3,41.3,36.3,36.9,38.3,38.9,35.7,41.1,34,39.6,36.2,40.8,38.1,40.3,33.1,43.2,35,41,37.7,37.8,37.9,39.7,38.6,38.2,38.1,43.2,38.1,45.6,39.7,42.2,39.6,42.7,38.6,37.3,35.7,41.1,36.2,37.7,40.2,41.4,35.2,40.6,38.8,41.5,39,44.1,38.5,43.1,36.8,37.5,38.1,41.1,35.6,40.2,37,39.7,40.2,40.6,32.1,40.7,37.3,39,39.2,36.6,36,37.8,36,41.5,46.1,50,48.7,50,47.6,46.5,45.4,46.7,43.3,46.8,40.9,49,45.5,48.4,45.8,49.3,42,49.2,46.2,48.7,50.2,45.1,46.5,46.3,42.9,46.1,47.8,48.2,50,47.3,42.8,45.1,59.6,49.1,48.4,42.6,44.4,44,48.7,42.7,49.6,45.3,49.6,50.5,43.6,45.5,50.5,44.9,45.2,46.6,48.5,45.1,50.1,46.5,45,43.8,45.5,43.2,50.4,45.3,46.2,45.7,54.3,45.8,49.8,49.5,43.5,50.7,47.7,46.4,48.2,46.5,46.4,48.6,47.5,51.1,45.2,45.2,49.1,52.5,47.4,50,44.9,50.8,43.4,51.3,47.5,52.1,47.5,52.2,45.5,49.5,44.5,50.8,49.4,46.9,48.4,51.1,48.5,55.9,47.2,49.1,46.8,41.7,53.4,43.3,48.1,50.5,49.8,43.5,51.5,46.2,55.1,48.8,47.2,46.8,50.4,45.2,49.9,46.5,50,51.3,45.4,52.7,45.2,46.1,51.3,46,51.3,46.6,51.7,47,52,45.9,50.5,50.3,58,46.4,49.2,42.4,48.5,43.2,50.6,46.7,52,50.5,49.5,46.4,52.8,40.9,54.2,42.5,51,49.7,47.5,47.6,52,46.9,53.5,49,46.2,50.9,45.5,50.9,50.8,50.1,49,51.5,49.8,48.1,51.4,45.7,50.7,42.5,52.2,45.2,49.3,50.2,45.6,51.9,46.8,45.7,55.8,43.5,49.6,50.8,50.2],[18.7,17.4,18,19.3,20.6,17.8,19.6,17.6,21.2,21.1,17.8,19,20.7,18.4,21.5,18.3,18.7,19.2,18.1,17.2,18.9,18.6,17.9,18.6,18.9,16.7,18.1,17.8,18.9,17,21.1,20,18.5,19.3,19.1,18,18.4,18.5,19.7,16.9,18.8,19,17.9,21.2,17.7,18.9,17.9,19.5,18.1,18.6,17.5,18.8,16.6,19.1,16.9,21.1,17,18.2,17.1,18,16.2,19.1,16.6,19.4,19,18.4,17.2,18.9,17.5,18.5,16.8,19.4,16.1,19.1,17.2,17.6,18.8,19.4,17.8,20.3,19.5,18.6,19.2,18.8,18,18.1,17.1,18.1,17.3,18.9,18.6,18.5,16.1,18.5,17.9,20,16,20,18.6,18.9,17.2,20,17,19,16.5,20.3,17.7,19.5,20.7,18.3,17,20.5,17,18.6,17.2,19.8,17,18.5,15.9,19,17.6,18.3,17.1,18,17.9,19.2,18.5,18.5,17.6,17.5,17.5,20.1,16.5,17.9,17.1,17.2,15.5,17,16.8,18.7,18.6,18.4,17.8,18.1,17.1,18.5,13.2,16.3,14.1,15.2,14.5,13.5,14.6,15.3,13.4,15.4,13.7,16.1,13.7,14.6,14.6,15.7,13.5,15.2,14.5,15.1,14.3,14.5,14.5,15.8,13.1,15.1,15,14.3,15.3,15.3,14.2,14.5,17,14.8,16.3,13.7,17.3,13.6,15.7,13.7,16,13.7,15,15.9,13.9,13.9,15.9,13.3,15.8,14.2,14.1,14.4,15,14.4,15.4,13.9,15,14.5,15.3,13.8,14.9,13.9,15.7,14.2,16.8,16.2,14.2,15,15,15.6,15.6,14.8,15,16,14.2,16.3,13.8,16.4,14.5,15.6,14.6,15.9,13.8,17.3,14.4,14.2,14,17,15,17.1,14.5,16.1,14.7,15.7,15.8,14.6,14.4,16.5,15,17,15.5,15,16.1,14.7,15.8,14,15.1,15.2,15.9,15.2,16.3,14.1,16,16.2,13.7,14.3,15.7,14.8,16.1,17.9,19.5,19.2,18.7,19.8,17.8,18.2,18.2,18.9,19.9,17.8,20.3,17.3,18.1,17.1,19.6,20,17.8,18.6,18.2,17.3,17.5,16.6,19.4,17.9,19,18.4,19,17.8,20,16.6,20.8,16.7,18.8,18.6,16.8,18.3,20.7,16.6,19.9,19.5,17.5,19.1,17,17.9,18.5,17.9,19.6,18.7,17.3,16.4,19,17.3,19.7,17.3,18.8,16.6,19.9,18.8,19.4,19.5,16.5,17,19.8,18.1,18.2,19,18.7],[181,186,195,193,190,181,195,182,191,198,185,195,197,184,194,174,180,189,185,180,187,183,187,172,180,178,178,188,184,195,196,190,180,181,184,182,195,186,196,185,190,182,190,191,186,188,190,200,187,191,186,193,181,194,185,195,185,192,184,192,195,188,190,198,190,190,196,197,190,195,191,184,187,195,189,196,187,193,191,194,190,189,189,190,202,205,185,186,187,208,190,196,178,192,192,203,183,190,193,184,199,190,181,197,198,191,193,197,191,196,188,199,189,189,187,198,176,202,186,199,191,195,191,210,190,197,193,199,187,190,191,200,185,193,193,187,188,190,192,185,190,184,195,193,187,201,211,230,210,218,215,210,211,219,209,215,214,216,214,213,210,217,210,221,209,222,218,215,213,215,215,215,215,210,220,222,209,207,230,220,220,213,219,208,208,208,225,210,216,222,217,210,225,213,215,210,220,210,225,217,220,208,220,208,224,208,221,214,231,219,230,229,220,223,216,221,221,217,216,230,209,220,215,223,212,221,212,224,212,228,218,218,212,230,218,228,212,224,214,226,216,222,203,225,219,228,215,228,215,210,219,208,209,216,229,213,230,217,230,222,214,215,222,212,213,192,196,193,188,197,198,178,197,195,198,193,194,185,201,190,201,197,181,190,195,181,191,187,193,195,197,200,200,191,205,187,201,187,203,195,199,195,210,192,205,210,187,196,196,196,201,190,212,187,198,199,201,193,203,187,197,191,203,202,194,206,189,195,207,202,193,210,198],[3750,3800,3250,3450,3650,3625,4675,3200,3800,4400,3700,3450,4500,3325,4200,3400,3600,3800,3950,3800,3800,3550,3200,3150,3950,3250,3900,3300,3900,3325,4150,3950,3550,3300,4650,3150,3900,3100,4400,3000,4600,3425,3450,4150,3500,4300,3450,4050,2900,3700,3550,3800,2850,3750,3150,4400,3600,4050,2850,3950,3350,4100,3050,4450,3600,3900,3550,4150,3700,4250,3700,3900,3550,4000,3200,4700,3800,4200,3350,3550,3800,3500,3950,3600,3550,4300,3400,4450,3300,4300,3700,4350,2900,4100,3725,4725,3075,4250,2925,3550,3750,3900,3175,4775,3825,4600,3200,4275,3900,4075,2900,3775,3350,3325,3150,3500,3450,3875,3050,4000,3275,4300,3050,4000,3325,3500,3500,4475,3425,3900,3175,3975,3400,4250,3400,3475,3050,3725,3000,3650,4250,3475,3450,3750,3700,4000,4500,5700,4450,5700,5400,4550,4800,5200,4400,5150,4650,5550,4650,5850,4200,5850,4150,6300,4800,5350,5700,5000,4400,5050,5000,5100,5650,4600,5550,5250,4700,5050,6050,5150,5400,4950,5250,4350,5350,3950,5700,4300,4750,5550,4900,4200,5400,5100,5300,4850,5300,4400,5000,4900,5050,4300,5000,4450,5550,4200,5300,4400,5650,4700,5700,5800,4700,5550,4750,5000,5100,5200,4700,5800,4600,6000,4750,5950,4625,5450,4725,5350,4750,5600,4600,5300,4875,5550,4950,5400,4750,5650,4850,5200,4925,4875,4625,5250,4850,5600,4975,5500,5500,4700,5500,4575,5500,5000,5950,4650,5500,4375,5850,6000,4925,4850,5750,5200,5400,3500,3900,3650,3525,3725,3950,3250,3750,4150,3700,3800,3775,3700,4050,3575,4050,3300,3700,3450,4400,3600,3400,2900,3800,3300,4150,3400,3800,3700,4550,3200,4300,3350,4100,3600,3900,3850,4800,2700,4500,3950,3650,3550,3500,3675,4450,3400,4300,3250,3675,3325,3950,3600,4050,3350,3450,3250,4050,3800,3525,3950,3650,3650,4000,3400,3775,4100,3775],[\"male\",\"female\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>species<\\/th>\\n      <th>island<\\/th>\\n      <th>bill_length_mm<\\/th>\\n      <th>bill_depth_mm<\\/th>\\n      <th>flipper_length_mm<\\/th>\\n      <th>body_mass_g<\\/th>\\n      <th>sex<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[3,4,5,6]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nAfter dropping the year column and filtering for the missing values (NAs), we are left with 333 rows and 7 columns 6 with features and 1 with the target.\n\n***Note: In machine learning, especially while using multiple predictors/features, it's advised to transform all the numeric data so the values are on the same scale. However, since the three numeric features are on the same scale (i.e., mili meters) in this data set, it is not required.***\n\n# Building the linear regression model for inferential modeling\n\nUtilizing the dataset prepared above, we will first build two models with slightly different feature sets for inferential modeling, i.e., to understand how the features influence the model prediction and what features are more informative than the others.\n\n## First model\n\n**In the first model**, we will regress body mass in grams, i.e., the body_mass_g column in the table below on three numeric features `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and one categorical feature `sex` (female and male).\n\nWe will use the `lm` function from the base R to fit the linear regression model and the `summary()` function to summarize the output of the model fitting. The summary provides several useful statistics that can be used to interpret the model.\n\nTo execute the `lm()` function in R, we must provide a formula describing the model to fit and the input data. The formula for our model can be written as\n\n$$body\\_mass\\_g \\sim bill\\_length\\_mm + bill\\_depth\\_mm + flipper\\_length\\_mm + sex$$ {#eq-model-formula}\n\nThe variable on the right side of the tilde symbol is the target, and the variables on the left, separated by plus signs, are the features.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the linear model\nfit1 <- lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + sex, data = df_penguins)\n\n# Get the summary of the model\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    sex, data = df_penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-927.81 -247.66    5.52  220.60  990.45 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -2288.465    631.580  -3.623 0.000337 ***\nbill_length_mm       -2.329      4.684  -0.497 0.619434    \nbill_depth_mm       -86.088     15.570  -5.529 6.58e-08 ***\nflipper_length_mm    38.826      2.448  15.862  < 2e-16 ***\nsexmale             541.029     51.710  10.463  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 340.8 on 328 degrees of freedom\nMultiple R-squared:  0.823,\tAdjusted R-squared:  0.8208 \nF-statistic: 381.3 on 4 and 328 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nLet's go through the summary of our fitted linear regression model which typically includes the following statistics (from top to bottom):\n\n1.  **Call**: Call section shows the parameters that were passed to the `lm()` function.\n\n2.  **Residuals**: Residuals represent the differences between the actual and predicted values of the dependent variable (`body_mass_g`). The summary shows statistics such as minimum, first quartile (1Q), median, third quartile (3Q), and maximum values of the residuals.\n\n3.  **Coefficients**: The coefficients table presents the estimated coefficients for each predictor variable. The `Estimate` column shows the estimated effect of each variable on the dependent variable. The `Std. Error` column indicates the standard error of the estimate, which measures the variability of the coefficient. The `t value` column represents the t-statistic, which assesses the significance of each coefficient. The `Pr(>|t|)` column provides the p-value, which indicates the probability of observing a t-statistic as extreme as the one calculated if the null hypothesis were true (null hypothesis: the coefficient is not significant).\n\n    -   The `(Intercept)` coefficient represents the estimated body mass when all the predictor variables are zero.\n\n    -   The `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm` coefficients indicate the estimated change in body mass associated with a one-unit increase in each respective predictor.\n\n    -   The `sexmale` coefficient represents the estimated difference in body mass between males and females (since it's a binary variable). Here `female` is the reference value, which means that males are `541.029 g` heavier than females.\n\n    The significance of each coefficient can be determined based on the corresponding p-value. In this case, the `(Intercept)`, `bill_depth_mm`, `flipper_length_mm`, and `sexmale` coefficients are all highly significant (p \\< 0.001), indicating that they have a significant effect on the body mass. However, the `bill_length_mm` coefficient is not statistically significant (p = 0.619), suggesting that it may not have a significant effect on body mass.\n\n4.  **Residual standard error**: This value represents the standard deviation of the residuals (340.8), providing an estimate of the model's prediction error. ***A lower residual standard error indicates a better fit.***\n\n5.  **Multiple R-squared and Adjusted R-squared**: The multiple R-squared value (0.823) indicates the proportion of variance in the dependent variable that can be explained by the predictor variables. The adjusted R-squared (0.8208) adjusts the R-squared value for the number of predictors in the model, penalizing the addition of irrelevant predictors. ***A higher R-squared value indicates a better fit.***\n\n6.  **F-statistic and p-value**: The F-statistic tests the overall significance of the model by comparing the fit of the current model with a null model (no predictors). The associated p-value (\\< 2.2e-16) suggests that the model is highly significant and provides a better fit than the null model. ***A low p-value indicates that the model is statistically significant.***\n\nIn conclusion, this linear regression model suggests that the numerical features `bill_depth_mm`, `flipper_length_mm`, and the categorical feature `sex` are significant predictors of the body mass of penguins. However, `bill_length_mm` does not appear to be a significant predictor in this model.\n\n## Second model\n\n**In the second model**, we will regress body mass in grams, i.e., the body_mass_g column in the table below on three numeric features `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and three categorical features `species` (Adelie, Chinstrap, and Gentoo), `island` (Biscoe, Dream, and Torgersen), and `sex` (female and male). The formula for this second model will be\n\n$$body\\_mass\\_g \\sim species + island + bill\\_length\\_mm + bill\\_depth\\_mm + flipper\\_length\\_mm + sex$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the linear model\nfit2 <- lm(body_mass_g ~ species + island + bill_length_mm + bill_depth_mm + flipper_length_mm + sex, data = df_penguins)\n\n# Get the summary of the model\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = body_mass_g ~ species + island + bill_length_mm + \n    bill_depth_mm + flipper_length_mm + sex, data = df_penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-779.20 -167.35   -3.16  179.37  914.27 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -1500.029    575.822  -2.605 0.009610 ** \nspeciesChinstrap   -260.306     88.551  -2.940 0.003522 ** \nspeciesGentoo       987.761    137.238   7.197 4.30e-12 ***\nislandDream         -13.103     58.541  -0.224 0.823032    \nislandTorgersen     -48.064     60.922  -0.789 0.430722    \nbill_length_mm       18.189      7.136   2.549 0.011270 *  \nbill_depth_mm        67.575     19.821   3.409 0.000734 ***\nflipper_length_mm    16.239      2.939   5.524 6.80e-08 ***\nsexmale             387.224     48.138   8.044 1.66e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 287.9 on 324 degrees of freedom\nMultiple R-squared:  0.8752,\tAdjusted R-squared:  0.8721 \nF-statistic: 284.1 on 8 and 324 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nSimilar to the model summary of `fit1` , `fit2` also have the same evaluation metrics; however, in the coefficient section now we have a few more rows related to the two additional features `species` and `island`.\n\n-   The `speciesChinstrap` and `speciesGentoo` coefficients represent the estimated differences in body mass between the corresponding penguin species and the reference category. Here the reference category is Ad√©lie. Reference category is picked in alphabetical order.\n\n-   The `islandDream` and `islandTorgersen` coefficients represent the estimated differences in body mass between penguins from the corresponding islands and the reference island Biscoe.\n\nIn this model, the following predictors are significant:\n\n-   `speciesChinstrap`, `speciesGentoo`, `bill_depth_mm`, `flipper_length_mm`, and `sexmale` have highly significant coefficients (p \\< 0.01).\n\n-   `bill_length_mm` has a marginally significant coefficient (p = 0.011). *Bill length was not significant when it was used without `species` and `island` features* in the model.\n\nThe significance of `islandDream` and `islandTorgersen` is not supported by the data, as indicated by non-significant p-values.\n\n## Comparing the two models\n\nWe can make two important observations by comparing the two models we built above. First, changing features may influence the coefficient/significance of other features. We see that in the first model, `bill_length_mm` was not significant, but in the second model, it is marginally significant. Second, different sets of features may represent different proportions of variance in the dependent variable that the predictor variables can explain. The second model shows that R-squared values have increased to 0.8752 and 0.8721 from 0.823 and 0.8208. This means that the two additional variables helped capture more variance. Therefore, choosing an appropriate set of features that best describes the complexity of the data is important.\n\n# Building the linear regression model for predictive modeling\n\nBuilding a linear regression model for predictive modeling is no different from inferential modeling. However, while evaluating the model performance, we focus on the predictive accuracy of the unseen data (test data) rather than the significance of features in the training data. The set of features may influence the model's predictive performance; therefore, using the most informative features is important. However, while optimizing a model for the highest predictive performance, our metric is the overall prediction accuracy.\n\nWe will again build two models with the same set of features as before. However, we need to split the data into training and test sets randomly. Conventionally the train and test splits are 80% and 20% of the data, respectively.\n\n### Train test split\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)  # Set a seed for reproducibility\n\n# Generate random indices for train-test split\ntrain_indices <- sample(1:nrow(df_penguins), round(0.8 * nrow(df_penguins)))\n\n# Create training set\ntrain_data <- df_penguins[train_indices, ]\n\n# Create test set by excluding the training indices\ntest_data <- df_penguins[-train_indices, ]\n```\n:::\n\n\nNote:\n\n1.  We use the base R features to split the data into train and test sets. However, several R packages can do this in one line with more advanced features such as stratification. Examples of R packages are `tidymodels` and `caret`.\n\n2.  Since the process of random sampling and splitting the data is stochastic, it is recommended to do it multiple times, train and test the models on each split, and take the average of performance metrics. It will ensure that the effect that we see is not by chance.\n\n## First model\n\n**In the first model**, similar to before, we will regress `body_mass_g` on three numeric features `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and one categorical feature, `sex` (female and male). However, this time we will first train the model only on the training data and then test the model on the test data. So let's first build the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the linear model\nfit1_train <- lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + sex, data = train_data)\n\n# Get the summary of the model\nsummary(fit1_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    sex, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-939.59 -248.45   -7.64  229.83  994.91 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -2027.1777   730.7280  -2.774  0.00593 ** \nbill_length_mm       -0.4218     5.4003  -0.078  0.93781    \nbill_depth_mm       -92.3046    18.2396  -5.061 7.89e-07 ***\nflipper_length_mm    37.5979     2.7917  13.468  < 2e-16 ***\nsexmale             544.0535    60.4370   9.002  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 350.4 on 261 degrees of freedom\nMultiple R-squared:  0.8161,\tAdjusted R-squared:  0.8133 \nF-statistic: 289.6 on 4 and 261 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nNow let's test the model on the test data. To test the model, we must first predict the values of our target using the features from the test dataset. Then the predicted target values are compared to the original target values from the test data set (the ground truth) to compute an accuracy metric.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict on the test data set.\npredict1 <- predict(fit1_train, newdata = test_data)\n\n# The predicted values. They are in the same unit as the original target i.e. body_mass_g\ncat(predict1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3625.943 3534.313 3806.928 3785.672 3381.555 3723.386 3449.419 3963.758 3883.43 4038.404 3804.938 3348.516 4159.798 3502.267 3335.694 4161.442 3524.614 4285.875 3673.597 3192.804 4141.798 4133.245 4275.416 4027.821 4164.188 4074.142 3729.952 3410.047 5638.751 5289.111 4501.428 4829.097 5195.706 5263.446 4698.645 5478.621 5232.544 4830.154 5384.665 5505.299 4500.286 4716.475 4689.841 5262.308 4584.462 5450.085 4821.684 5573.252 5683.931 4482.669 4960.761 5347.704 3979.395 4102.772 3311.751 3379.997 4147.781 3491.439 4356.094 4131.268 3444.227 3595.172 3753.652 3668.3 4166.329 4440.224 3669.948\n```\n:::\n:::\n\n\n1.  **Mean Squared Error (MSE):** MSE measures the average squared difference between the predicted and actual values. It provides a measure of the model's overall prediction error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmse <- mean((test_data$body_mass_g - predict1)^2)\n\nprint(mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 91057.22\n```\n:::\n:::\n\n\n2.  **Root Mean Squared Error (RMSE):** RMSE is the square root of MSE and provides a more interpretable measure of the model's prediction error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse <- sqrt(mse)\n\nprint(rmse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 301.7569\n```\n:::\n:::\n\n\n3.  **Mean Absolute Error (MAE):** MAE measures the average absolute difference between the predicted and actual values. It represents the average magnitude of the prediction errors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmae <- mean(abs(test_data$body_mass_g - predict1))\n\nprint(mae)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 240.685\n```\n:::\n:::\n\n\nMSE, RMSE, and MAE give a measure of the prediction error, with lower values indicating better model performance and more accurate predictions. The RMSE is often preferred over MSE because it is in the same unit as the dependent variable, making it easier to interpret in the context of the problem. MAE does not square the prediction errors as the MSE does. Consequently, the MAE does not overly penalize larger errors, making it more robust to outliers in the data.\n\n## Second model\n\n**In the second model**, similar to before, we will regress `` body_mass_g` `` on three numeric features `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and three categorical features `species` (Adelie, Chinstrap, and Gentoo), `island` (Biscoe, Dream, and Torgersen), and `sex` (female and male). However, as we did in the first model above we will first train the model only on the training data and then test the model on the test data. So let's first build the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the linear model\nfit2_train <- lm(body_mass_g ~ species + island + bill_length_mm + bill_depth_mm + flipper_length_mm + sex, data = train_data)\n\n# Get the summary of the model\nsummary(fit2_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = body_mass_g ~ species + island + bill_length_mm + \n    bill_depth_mm + flipper_length_mm + sex, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-773.35 -159.93  -12.94  168.29  922.46 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -1601.876    636.617  -2.516 0.012472 *  \nspeciesChinstrap   -335.663     95.511  -3.514 0.000521 ***\nspeciesGentoo      1013.366    145.792   6.951 2.99e-11 ***\nislandDream          -2.507     65.309  -0.038 0.969413    \nislandTorgersen     -17.043     67.611  -0.252 0.801179    \nbill_length_mm       25.903      7.706   3.361 0.000894 ***\nbill_depth_mm        75.761     22.036   3.438 0.000683 ***\nflipper_length_mm    14.367      3.142   4.573 7.49e-06 ***\nsexmale             364.997     53.517   6.820 6.48e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 285.8 on 257 degrees of freedom\nMultiple R-squared:  0.8796,\tAdjusted R-squared:  0.8758 \nF-statistic: 234.6 on 8 and 257 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nNow let's test the model on the test data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict on the test data set.\npredict2 <- predict(fit2_train, newdata = test_data)\n\n# The predicted values. They are in the same unit as the original target i.e. body_mass_g\ncat(predict2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3590.195 3624.511 4353.636 3781.747 3468.289 4206.503 3390.542 3988.718 4232.997 3965.01 3975.727 3417.981 4194.577 3225.112 3229.69 4021.976 3419.276 4256.824 3866.844 3273.355 4009.004 4147.367 3992.548 3880.737 4198.07 3832.495 3845.419 3310.229 5610.905 5355.166 4720.985 4604.059 5239.927 5425.792 4613.011 5505.981 5300.919 4711.534 5252.184 5459.304 4618.691 4722.829 4853.032 5495.73 4827.623 5494.399 4758.6 5718.334 5460.369 4582.038 5169.073 5457.311 3981.178 4106.045 3245.918 3400.699 3879.755 3354.457 4253.043 4292.449 3112.631 3480.889 3342.37 3550.431 4031.654 4206.217 3621.641\n```\n:::\n:::\n\n\nAnd compute the accuracy metrics.\n\n1.  **Mean Squared Error (MSE)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmse <- mean((test_data$body_mass_g - predict2)^2)\n\nprint(mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 90171.39\n```\n:::\n:::\n\n\n2.  **Root Mean Squared Error (RMSE)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse <- sqrt(mse)\n\nprint(rmse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 300.2855\n```\n:::\n:::\n\n\n3.  **Mean Absolute Error (MAE)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmae <- mean(abs(test_data$body_mass_g - predict2))\n\nprint(mae)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 254.6932\n```\n:::\n:::\n\n\nComparing the predictive performance of the two models, the difference between the metrics is minimal/negligible. Therefore, the inclusion or exclusion of species and island features does not have an appreciable effect on the predictive performance of the models based on the train test dataset used.\n\n------------------------------------------------------------------------\n\n# Extended theory\n\n## What are some real-life cases with linear relationships between the dependent and the independent variables?\n\nThere are many real-life cases in which a linear relationship exists between the dependent and independent variables. Here are a few examples:\n\nThe relationship between the weight of an object and the force needed to lift it. If we plot the weight of an object on the x-axis and the force required to lift it on the y-axis, we can use linear regression to model the relationship between the two variables.\n\nThe relationship between the number of hours a student studies and their test scores. If we plot the number of hours a student studies on the x-axis and their test scores on the y-axis, we can use linear regression to model the relationship between the two variables.\n\nThe relationship between the size of a company and its profitability. If we plot the size of a company on the x-axis and its profitability on the y-axis, we can use linear regression to model the relationship between the two variables.\n\nThe relationship between the age of a car and its fuel efficiency. If we plot the age of a car on the x-axis and its fuel efficiency on the y-axis, we can use linear regression to model the relationship between the two variables.\n\nIn each of these cases, there is a linear relationship between the dependent and independent variables, which can be modeled using linear regression.\n\n## What are the real-life cases where a linear regression will not work?\n\nLinear regression is not suitable for modeling relationships between variables that are not linear. Some examples of real-life cases where linear regression will not work include:\n\nThe relationship between the temperature of a substance and its volume. The volume of a substance changes non-linearly with temperature, so linear regression would not be appropriate for modeling this relationship.\n\nThe relationship between the price of a product and the demand for it. The demand for a product often follows an inverse relationship with price, so linear regression would not be appropriate for modeling this relationship.\n\nThe relationship between the weight of an object and the time it takes to fall to the ground. The time it takes for an object to fall to the ground increases non-linearly with its weight, so linear regression would not be appropriate for modeling this relationship.\n\nThe relationship between the age of a person and their risk of developing a certain disease. The risk of developing a disease often increases non-linearly with age, so linear regression would not be appropriate for modeling this relationship.\n\nIn each of these cases, there is a non-linear relationship between the variables, and linear regression would not be able to accurately model this relationship. Instead, other techniques such as polynomial regression or logistic regression may be more appropriate.\n\n## What are some scientific, medical, or engineering breakthroughs where linear regression was used?\n\nLinear regression is a widely used statistical technique that has been applied to a variety of scientific, medical, and engineering fields. Here are a few examples of famous breakthroughs where linear regression was used:\n\nIn the field of medicine, linear regression has been used to understand the relationship between various factors and the risk of developing certain diseases. For example, researchers have used linear regression to understand the relationship between diet and the risk of developing diabetes [@Panagiotakos2005-pi].\n\nIn the field of engineering, linear regression has been used to understand the relationship between various factors and the strength of materials. For example, researchers have used linear regression to understand the relationship between the composition of steel and its strength [@Singh_Tumrate2021-po].\n\nIn the field of economics, linear regression has been used to understand the relationship between various factors and the performance of stocks. For example, researchers have used linear regression to understand the relationship between a company's earnings and its stock price.\n\nIn the field of psychology, linear regression has been used to understand the relationship between various factors and human behavior. For example, researchers have used linear regression to understand the relationship between personality traits and job performance.\n\nThese are just a few examples of the many ways in which linear regression has been used to make scientific, medical, and engineering breakthroughs.\n\n\n```{=html}\n<!---\n## How is linear regression models are different from linear mixed-effects models?\n\nLinear regression models and linear mixed-effects models are similar in that they are both used to model the relationship between a dependent variable and one or more independent variables. However, there are a few key differences between the two types of models:\n\nLinear regression models assume that the errors between the observed and predicted values of the dependent variable are independent and identically distributed (i.i.d.), whereas linear mixed-effects models allow for the errors to be correlated and for the variance of the errors to vary across groups.\n\nLinear regression models are used to model the relationship between variables in a single dataset, whereas linear mixed-effects models are used to model the relationship between variables in multiple datasets that have a hierarchical structure. For example, linear mixed-effects models can be used to model the relationship between variables in a study with multiple subjects, where each subject has multiple observations.\n\nLinear regression models are estimated using ordinary least squares (OLS), whereas linear mixed-effects models are estimated using maximum likelihood estimation (MLE). This means that the parameters in a linear mixed-effects model are estimated by finding the values that maximize the likelihood of the data given the model, whereas the parameters in a linear regression model are estimated by minimizing the sum of the squared errors between the observed and predicted values of the dependent variable.\n\nOverall, linear regression models are suitable for modeling relationships between variables in a single dataset when the errors are i.i.d., whereas linear mixed-effects models are more suitable for modeling relationships between variables in multiple datasets with a hierarchical structure and correlated errors.\n-->\n```\n\n## How does multicollinearity affect linear regression? How to mitigate and interpret models built with multiple colinear variables.\n\nMulticollinearity is a situation in which two or more independent variables are highly correlated with each other. This can be a problem in linear regression because it can lead to unstable and unreliable estimates of the regression coefficients.\n\nThere are a few ways that multicollinearity can affect linear regression:\n\nIt can cause the standard errors of the regression coefficients to be inflated, leading to a decrease in the statistical power of the model.\n\nIt can make it difficult to interpret the individual contributions of the independent variables to the dependent variable because the coefficients of the correlated variables will be correlated as well.\n\nIt can make the model more sensitive to small changes in the data, leading to unstable predictions.\n\nTo mitigate the effects of multicollinearity, you can do the following:\n\nRemove one or more of the correlated variables from the model.\n\nUse regularization techniques such as Lasso or Ridge regression, which can help to shrink the coefficients of correlated variables towards zero.\n\nTransform the correlated variables by taking their logarithm, square root, or other non-linear transformation.\n\nUse variable selection techniques such as backward elimination or forward selection to select a subset of uncorrelated variables for the model.\n\nTo interpret a model built with multiple collinear variables, you can look at the R-squared value and the p-values of the individual variables to assess their contribution to the model. However, it is important to bear in mind that the estimates of the regression coefficients may be unstable and unreliable due to the presence of multicollinearity.\n\n\n```{=html}\n<!--\n## Is there any other method that resembles linear regression?\n\nThere are several statistical techniques that resemble linear regression, in the sense that they are used to model the relationship between a dependent variable and one or more independent variables. Some examples of these techniques include:\n\nPolynomial regression: This technique is used to model relationships between variables that are non-linear. It involves fitting a polynomial function to the data, which can be used to make predictions about the dependent variable based on the independent variables.\n\nLogistic regression: This technique is used to model binary outcomes, such as whether a patient will recover from a disease or not. It involves fitting a logistic curve to the data, which can be used to estimate the probability of the dependent variable taking on one of the two possible values.\n\nStepwise regression: This technique is used to select a subset of relevant independent variables for the model by adding or removing variables based on their statistical significance. It can be used to build a linear regression model with a smaller number of variables, which can be helpful in situations where there are many potential independent variables.\n\nRidge regression: This technique is a variant of linear regression that adds a regularization term to the objective function. The regularization term helps to shrink the coefficients of correlated variables towards zero, which can help to reduce the effects of multicollinearity.\n\nLasso regression: This technique is another variant of linear regression that adds a regularization term to the objective function. Unlike Ridge regression, which shrinks the coefficients of all variables towards zero, Lasso regression shrinks the coefficients of some variables all the way to zero, effectively eliminating them from the model. This can be helpful in situations where there are many correlated variables and we want to select a subset of the most important variables for the model.\n-->\n```\n\n# References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/datatables-binding-0.26/datatables.js\"></script>\n<script src=\"../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../../site_libs/dt-core-1.12.1/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/dt-core-1.12.1/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/dt-core-1.12.1/js/jquery.dataTables.min.js\"></script>\n<link href=\"../../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}