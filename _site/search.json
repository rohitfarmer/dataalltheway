[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data All The Way",
    "section": "",
    "text": "Welcome to Data All The Way, “yet another” tutorial website with concepts, methods, and example code on data science, statistics, and machine learning. All the posts with example code accompany a Kaggle or Google Colab notebook for a live demo. In addition, posts are citable, and some posts are hosted on Zenodo with a permanent DOI and indexed in Data All The Way community. If you want to become a contributing author, please write to rohit [dot] farmer [at] dataalltheway [dot] com. Comments and suggestions are always welcomed. To learn more about my other projects and research, please visit https://rohitfarmer.com."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Data All The Way",
    "section": "Recent posts",
    "text": "Recent posts"
  },
  {
    "objectID": "posts/013-linear-regression/index.html",
    "href": "posts/013-linear-regression/index.html",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2023-07-13 First draft"
  },
  {
    "objectID": "posts/013-linear-regression/index.html#culmen-measurements",
    "href": "posts/013-linear-regression/index.html#culmen-measurements",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "Culmen measurements",
    "text": "Culmen measurements\nWhat are culmen length & depth? The culmen is “the upper ridge of a bird’s beak” (definition from Oxford Languages). In the simplified penguins subset, culmen length and depth have been updated to variables named bill_length_mm and bill_depth_mm.\nFor this penguin data, the bill/culmen length and depth are measured as shown below.\n\n\n\nCulmen measurements. Artwork by @allison_horst."
  },
  {
    "objectID": "posts/013-linear-regression/index.html#data-filtering-and-transformation",
    "href": "posts/013-linear-regression/index.html#data-filtering-and-transformation",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "Data filtering and transformation",
    "text": "Data filtering and transformation\nBefore we can build our model(s) let’s filter the data for non useful predictors or missing values. The unfiltered dataset contains 344 rows and 7 columns. We will first drop year column and then remove any row with no value for any predictors. Below is the code to load the dataset and filter.\n\nlibrary(tidyverse)\nlibrary(kableExtra)\n\nlibrary(palmerpenguins)\ndf_penguins &lt;- penguins %&gt;% dplyr::select(-c(year)) %&gt;% na.omit()\n\ndf_penguins %&gt;%  kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F) %&gt;%\n  scroll_box(width = \"100%\", height = \"300px\")\n\n\n\n\nTable 1: Filtered dataset\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n\n\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nfemale\n\n\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmale\n\n\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nfemale\n\n\nAdelie\nTorgersen\n38.6\n21.2\n191\n3800\nmale\n\n\nAdelie\nTorgersen\n34.6\n21.1\n198\n4400\nmale\n\n\nAdelie\nTorgersen\n36.6\n17.8\n185\n3700\nfemale\n\n\nAdelie\nTorgersen\n38.7\n19.0\n195\n3450\nfemale\n\n\nAdelie\nTorgersen\n42.5\n20.7\n197\n4500\nmale\n\n\nAdelie\nTorgersen\n34.4\n18.4\n184\n3325\nfemale\n\n\nAdelie\nTorgersen\n46.0\n21.5\n194\n4200\nmale\n\n\nAdelie\nBiscoe\n37.8\n18.3\n174\n3400\nfemale\n\n\nAdelie\nBiscoe\n37.7\n18.7\n180\n3600\nmale\n\n\nAdelie\nBiscoe\n35.9\n19.2\n189\n3800\nfemale\n\n\nAdelie\nBiscoe\n38.2\n18.1\n185\n3950\nmale\n\n\nAdelie\nBiscoe\n38.8\n17.2\n180\n3800\nmale\n\n\nAdelie\nBiscoe\n35.3\n18.9\n187\n3800\nfemale\n\n\nAdelie\nBiscoe\n40.6\n18.6\n183\n3550\nmale\n\n\nAdelie\nBiscoe\n40.5\n17.9\n187\n3200\nfemale\n\n\nAdelie\nBiscoe\n37.9\n18.6\n172\n3150\nfemale\n\n\nAdelie\nBiscoe\n40.5\n18.9\n180\n3950\nmale\n\n\nAdelie\nDream\n39.5\n16.7\n178\n3250\nfemale\n\n\nAdelie\nDream\n37.2\n18.1\n178\n3900\nmale\n\n\nAdelie\nDream\n39.5\n17.8\n188\n3300\nfemale\n\n\nAdelie\nDream\n40.9\n18.9\n184\n3900\nmale\n\n\nAdelie\nDream\n36.4\n17.0\n195\n3325\nfemale\n\n\nAdelie\nDream\n39.2\n21.1\n196\n4150\nmale\n\n\nAdelie\nDream\n38.8\n20.0\n190\n3950\nmale\n\n\nAdelie\nDream\n42.2\n18.5\n180\n3550\nfemale\n\n\nAdelie\nDream\n37.6\n19.3\n181\n3300\nfemale\n\n\nAdelie\nDream\n39.8\n19.1\n184\n4650\nmale\n\n\nAdelie\nDream\n36.5\n18.0\n182\n3150\nfemale\n\n\nAdelie\nDream\n40.8\n18.4\n195\n3900\nmale\n\n\nAdelie\nDream\n36.0\n18.5\n186\n3100\nfemale\n\n\nAdelie\nDream\n44.1\n19.7\n196\n4400\nmale\n\n\nAdelie\nDream\n37.0\n16.9\n185\n3000\nfemale\n\n\nAdelie\nDream\n39.6\n18.8\n190\n4600\nmale\n\n\nAdelie\nDream\n41.1\n19.0\n182\n3425\nmale\n\n\nAdelie\nDream\n36.0\n17.9\n190\n3450\nfemale\n\n\nAdelie\nDream\n42.3\n21.2\n191\n4150\nmale\n\n\nAdelie\nBiscoe\n39.6\n17.7\n186\n3500\nfemale\n\n\nAdelie\nBiscoe\n40.1\n18.9\n188\n4300\nmale\n\n\nAdelie\nBiscoe\n35.0\n17.9\n190\n3450\nfemale\n\n\nAdelie\nBiscoe\n42.0\n19.5\n200\n4050\nmale\n\n\nAdelie\nBiscoe\n34.5\n18.1\n187\n2900\nfemale\n\n\nAdelie\nBiscoe\n41.4\n18.6\n191\n3700\nmale\n\n\nAdelie\nBiscoe\n39.0\n17.5\n186\n3550\nfemale\n\n\nAdelie\nBiscoe\n40.6\n18.8\n193\n3800\nmale\n\n\nAdelie\nBiscoe\n36.5\n16.6\n181\n2850\nfemale\n\n\nAdelie\nBiscoe\n37.6\n19.1\n194\n3750\nmale\n\n\nAdelie\nBiscoe\n35.7\n16.9\n185\n3150\nfemale\n\n\nAdelie\nBiscoe\n41.3\n21.1\n195\n4400\nmale\n\n\nAdelie\nBiscoe\n37.6\n17.0\n185\n3600\nfemale\n\n\nAdelie\nBiscoe\n41.1\n18.2\n192\n4050\nmale\n\n\nAdelie\nBiscoe\n36.4\n17.1\n184\n2850\nfemale\n\n\nAdelie\nBiscoe\n41.6\n18.0\n192\n3950\nmale\n\n\nAdelie\nBiscoe\n35.5\n16.2\n195\n3350\nfemale\n\n\nAdelie\nBiscoe\n41.1\n19.1\n188\n4100\nmale\n\n\nAdelie\nTorgersen\n35.9\n16.6\n190\n3050\nfemale\n\n\nAdelie\nTorgersen\n41.8\n19.4\n198\n4450\nmale\n\n\nAdelie\nTorgersen\n33.5\n19.0\n190\n3600\nfemale\n\n\nAdelie\nTorgersen\n39.7\n18.4\n190\n3900\nmale\n\n\nAdelie\nTorgersen\n39.6\n17.2\n196\n3550\nfemale\n\n\nAdelie\nTorgersen\n45.8\n18.9\n197\n4150\nmale\n\n\nAdelie\nTorgersen\n35.5\n17.5\n190\n3700\nfemale\n\n\nAdelie\nTorgersen\n42.8\n18.5\n195\n4250\nmale\n\n\nAdelie\nTorgersen\n40.9\n16.8\n191\n3700\nfemale\n\n\nAdelie\nTorgersen\n37.2\n19.4\n184\n3900\nmale\n\n\nAdelie\nTorgersen\n36.2\n16.1\n187\n3550\nfemale\n\n\nAdelie\nTorgersen\n42.1\n19.1\n195\n4000\nmale\n\n\nAdelie\nTorgersen\n34.6\n17.2\n189\n3200\nfemale\n\n\nAdelie\nTorgersen\n42.9\n17.6\n196\n4700\nmale\n\n\nAdelie\nTorgersen\n36.7\n18.8\n187\n3800\nfemale\n\n\nAdelie\nTorgersen\n35.1\n19.4\n193\n4200\nmale\n\n\nAdelie\nDream\n37.3\n17.8\n191\n3350\nfemale\n\n\nAdelie\nDream\n41.3\n20.3\n194\n3550\nmale\n\n\nAdelie\nDream\n36.3\n19.5\n190\n3800\nmale\n\n\nAdelie\nDream\n36.9\n18.6\n189\n3500\nfemale\n\n\nAdelie\nDream\n38.3\n19.2\n189\n3950\nmale\n\n\nAdelie\nDream\n38.9\n18.8\n190\n3600\nfemale\n\n\nAdelie\nDream\n35.7\n18.0\n202\n3550\nfemale\n\n\nAdelie\nDream\n41.1\n18.1\n205\n4300\nmale\n\n\nAdelie\nDream\n34.0\n17.1\n185\n3400\nfemale\n\n\nAdelie\nDream\n39.6\n18.1\n186\n4450\nmale\n\n\nAdelie\nDream\n36.2\n17.3\n187\n3300\nfemale\n\n\nAdelie\nDream\n40.8\n18.9\n208\n4300\nmale\n\n\nAdelie\nDream\n38.1\n18.6\n190\n3700\nfemale\n\n\nAdelie\nDream\n40.3\n18.5\n196\n4350\nmale\n\n\nAdelie\nDream\n33.1\n16.1\n178\n2900\nfemale\n\n\nAdelie\nDream\n43.2\n18.5\n192\n4100\nmale\n\n\nAdelie\nBiscoe\n35.0\n17.9\n192\n3725\nfemale\n\n\nAdelie\nBiscoe\n41.0\n20.0\n203\n4725\nmale\n\n\nAdelie\nBiscoe\n37.7\n16.0\n183\n3075\nfemale\n\n\nAdelie\nBiscoe\n37.8\n20.0\n190\n4250\nmale\n\n\nAdelie\nBiscoe\n37.9\n18.6\n193\n2925\nfemale\n\n\nAdelie\nBiscoe\n39.7\n18.9\n184\n3550\nmale\n\n\nAdelie\nBiscoe\n38.6\n17.2\n199\n3750\nfemale\n\n\nAdelie\nBiscoe\n38.2\n20.0\n190\n3900\nmale\n\n\nAdelie\nBiscoe\n38.1\n17.0\n181\n3175\nfemale\n\n\nAdelie\nBiscoe\n43.2\n19.0\n197\n4775\nmale\n\n\nAdelie\nBiscoe\n38.1\n16.5\n198\n3825\nfemale\n\n\nAdelie\nBiscoe\n45.6\n20.3\n191\n4600\nmale\n\n\nAdelie\nBiscoe\n39.7\n17.7\n193\n3200\nfemale\n\n\nAdelie\nBiscoe\n42.2\n19.5\n197\n4275\nmale\n\n\nAdelie\nBiscoe\n39.6\n20.7\n191\n3900\nfemale\n\n\nAdelie\nBiscoe\n42.7\n18.3\n196\n4075\nmale\n\n\nAdelie\nTorgersen\n38.6\n17.0\n188\n2900\nfemale\n\n\nAdelie\nTorgersen\n37.3\n20.5\n199\n3775\nmale\n\n\nAdelie\nTorgersen\n35.7\n17.0\n189\n3350\nfemale\n\n\nAdelie\nTorgersen\n41.1\n18.6\n189\n3325\nmale\n\n\nAdelie\nTorgersen\n36.2\n17.2\n187\n3150\nfemale\n\n\nAdelie\nTorgersen\n37.7\n19.8\n198\n3500\nmale\n\n\nAdelie\nTorgersen\n40.2\n17.0\n176\n3450\nfemale\n\n\nAdelie\nTorgersen\n41.4\n18.5\n202\n3875\nmale\n\n\nAdelie\nTorgersen\n35.2\n15.9\n186\n3050\nfemale\n\n\nAdelie\nTorgersen\n40.6\n19.0\n199\n4000\nmale\n\n\nAdelie\nTorgersen\n38.8\n17.6\n191\n3275\nfemale\n\n\nAdelie\nTorgersen\n41.5\n18.3\n195\n4300\nmale\n\n\nAdelie\nTorgersen\n39.0\n17.1\n191\n3050\nfemale\n\n\nAdelie\nTorgersen\n44.1\n18.0\n210\n4000\nmale\n\n\nAdelie\nTorgersen\n38.5\n17.9\n190\n3325\nfemale\n\n\nAdelie\nTorgersen\n43.1\n19.2\n197\n3500\nmale\n\n\nAdelie\nDream\n36.8\n18.5\n193\n3500\nfemale\n\n\nAdelie\nDream\n37.5\n18.5\n199\n4475\nmale\n\n\nAdelie\nDream\n38.1\n17.6\n187\n3425\nfemale\n\n\nAdelie\nDream\n41.1\n17.5\n190\n3900\nmale\n\n\nAdelie\nDream\n35.6\n17.5\n191\n3175\nfemale\n\n\nAdelie\nDream\n40.2\n20.1\n200\n3975\nmale\n\n\nAdelie\nDream\n37.0\n16.5\n185\n3400\nfemale\n\n\nAdelie\nDream\n39.7\n17.9\n193\n4250\nmale\n\n\nAdelie\nDream\n40.2\n17.1\n193\n3400\nfemale\n\n\nAdelie\nDream\n40.6\n17.2\n187\n3475\nmale\n\n\nAdelie\nDream\n32.1\n15.5\n188\n3050\nfemale\n\n\nAdelie\nDream\n40.7\n17.0\n190\n3725\nmale\n\n\nAdelie\nDream\n37.3\n16.8\n192\n3000\nfemale\n\n\nAdelie\nDream\n39.0\n18.7\n185\n3650\nmale\n\n\nAdelie\nDream\n39.2\n18.6\n190\n4250\nmale\n\n\nAdelie\nDream\n36.6\n18.4\n184\n3475\nfemale\n\n\nAdelie\nDream\n36.0\n17.8\n195\n3450\nfemale\n\n\nAdelie\nDream\n37.8\n18.1\n193\n3750\nmale\n\n\nAdelie\nDream\n36.0\n17.1\n187\n3700\nfemale\n\n\nAdelie\nDream\n41.5\n18.5\n201\n4000\nmale\n\n\nGentoo\nBiscoe\n46.1\n13.2\n211\n4500\nfemale\n\n\nGentoo\nBiscoe\n50.0\n16.3\n230\n5700\nmale\n\n\nGentoo\nBiscoe\n48.7\n14.1\n210\n4450\nfemale\n\n\nGentoo\nBiscoe\n50.0\n15.2\n218\n5700\nmale\n\n\nGentoo\nBiscoe\n47.6\n14.5\n215\n5400\nmale\n\n\nGentoo\nBiscoe\n46.5\n13.5\n210\n4550\nfemale\n\n\nGentoo\nBiscoe\n45.4\n14.6\n211\n4800\nfemale\n\n\nGentoo\nBiscoe\n46.7\n15.3\n219\n5200\nmale\n\n\nGentoo\nBiscoe\n43.3\n13.4\n209\n4400\nfemale\n\n\nGentoo\nBiscoe\n46.8\n15.4\n215\n5150\nmale\n\n\nGentoo\nBiscoe\n40.9\n13.7\n214\n4650\nfemale\n\n\nGentoo\nBiscoe\n49.0\n16.1\n216\n5550\nmale\n\n\nGentoo\nBiscoe\n45.5\n13.7\n214\n4650\nfemale\n\n\nGentoo\nBiscoe\n48.4\n14.6\n213\n5850\nmale\n\n\nGentoo\nBiscoe\n45.8\n14.6\n210\n4200\nfemale\n\n\nGentoo\nBiscoe\n49.3\n15.7\n217\n5850\nmale\n\n\nGentoo\nBiscoe\n42.0\n13.5\n210\n4150\nfemale\n\n\nGentoo\nBiscoe\n49.2\n15.2\n221\n6300\nmale\n\n\nGentoo\nBiscoe\n46.2\n14.5\n209\n4800\nfemale\n\n\nGentoo\nBiscoe\n48.7\n15.1\n222\n5350\nmale\n\n\nGentoo\nBiscoe\n50.2\n14.3\n218\n5700\nmale\n\n\nGentoo\nBiscoe\n45.1\n14.5\n215\n5000\nfemale\n\n\nGentoo\nBiscoe\n46.5\n14.5\n213\n4400\nfemale\n\n\nGentoo\nBiscoe\n46.3\n15.8\n215\n5050\nmale\n\n\nGentoo\nBiscoe\n42.9\n13.1\n215\n5000\nfemale\n\n\nGentoo\nBiscoe\n46.1\n15.1\n215\n5100\nmale\n\n\nGentoo\nBiscoe\n47.8\n15.0\n215\n5650\nmale\n\n\nGentoo\nBiscoe\n48.2\n14.3\n210\n4600\nfemale\n\n\nGentoo\nBiscoe\n50.0\n15.3\n220\n5550\nmale\n\n\nGentoo\nBiscoe\n47.3\n15.3\n222\n5250\nmale\n\n\nGentoo\nBiscoe\n42.8\n14.2\n209\n4700\nfemale\n\n\nGentoo\nBiscoe\n45.1\n14.5\n207\n5050\nfemale\n\n\nGentoo\nBiscoe\n59.6\n17.0\n230\n6050\nmale\n\n\nGentoo\nBiscoe\n49.1\n14.8\n220\n5150\nfemale\n\n\nGentoo\nBiscoe\n48.4\n16.3\n220\n5400\nmale\n\n\nGentoo\nBiscoe\n42.6\n13.7\n213\n4950\nfemale\n\n\nGentoo\nBiscoe\n44.4\n17.3\n219\n5250\nmale\n\n\nGentoo\nBiscoe\n44.0\n13.6\n208\n4350\nfemale\n\n\nGentoo\nBiscoe\n48.7\n15.7\n208\n5350\nmale\n\n\nGentoo\nBiscoe\n42.7\n13.7\n208\n3950\nfemale\n\n\nGentoo\nBiscoe\n49.6\n16.0\n225\n5700\nmale\n\n\nGentoo\nBiscoe\n45.3\n13.7\n210\n4300\nfemale\n\n\nGentoo\nBiscoe\n49.6\n15.0\n216\n4750\nmale\n\n\nGentoo\nBiscoe\n50.5\n15.9\n222\n5550\nmale\n\n\nGentoo\nBiscoe\n43.6\n13.9\n217\n4900\nfemale\n\n\nGentoo\nBiscoe\n45.5\n13.9\n210\n4200\nfemale\n\n\nGentoo\nBiscoe\n50.5\n15.9\n225\n5400\nmale\n\n\nGentoo\nBiscoe\n44.9\n13.3\n213\n5100\nfemale\n\n\nGentoo\nBiscoe\n45.2\n15.8\n215\n5300\nmale\n\n\nGentoo\nBiscoe\n46.6\n14.2\n210\n4850\nfemale\n\n\nGentoo\nBiscoe\n48.5\n14.1\n220\n5300\nmale\n\n\nGentoo\nBiscoe\n45.1\n14.4\n210\n4400\nfemale\n\n\nGentoo\nBiscoe\n50.1\n15.0\n225\n5000\nmale\n\n\nGentoo\nBiscoe\n46.5\n14.4\n217\n4900\nfemale\n\n\nGentoo\nBiscoe\n45.0\n15.4\n220\n5050\nmale\n\n\nGentoo\nBiscoe\n43.8\n13.9\n208\n4300\nfemale\n\n\nGentoo\nBiscoe\n45.5\n15.0\n220\n5000\nmale\n\n\nGentoo\nBiscoe\n43.2\n14.5\n208\n4450\nfemale\n\n\nGentoo\nBiscoe\n50.4\n15.3\n224\n5550\nmale\n\n\nGentoo\nBiscoe\n45.3\n13.8\n208\n4200\nfemale\n\n\nGentoo\nBiscoe\n46.2\n14.9\n221\n5300\nmale\n\n\nGentoo\nBiscoe\n45.7\n13.9\n214\n4400\nfemale\n\n\nGentoo\nBiscoe\n54.3\n15.7\n231\n5650\nmale\n\n\nGentoo\nBiscoe\n45.8\n14.2\n219\n4700\nfemale\n\n\nGentoo\nBiscoe\n49.8\n16.8\n230\n5700\nmale\n\n\nGentoo\nBiscoe\n49.5\n16.2\n229\n5800\nmale\n\n\nGentoo\nBiscoe\n43.5\n14.2\n220\n4700\nfemale\n\n\nGentoo\nBiscoe\n50.7\n15.0\n223\n5550\nmale\n\n\nGentoo\nBiscoe\n47.7\n15.0\n216\n4750\nfemale\n\n\nGentoo\nBiscoe\n46.4\n15.6\n221\n5000\nmale\n\n\nGentoo\nBiscoe\n48.2\n15.6\n221\n5100\nmale\n\n\nGentoo\nBiscoe\n46.5\n14.8\n217\n5200\nfemale\n\n\nGentoo\nBiscoe\n46.4\n15.0\n216\n4700\nfemale\n\n\nGentoo\nBiscoe\n48.6\n16.0\n230\n5800\nmale\n\n\nGentoo\nBiscoe\n47.5\n14.2\n209\n4600\nfemale\n\n\nGentoo\nBiscoe\n51.1\n16.3\n220\n6000\nmale\n\n\nGentoo\nBiscoe\n45.2\n13.8\n215\n4750\nfemale\n\n\nGentoo\nBiscoe\n45.2\n16.4\n223\n5950\nmale\n\n\nGentoo\nBiscoe\n49.1\n14.5\n212\n4625\nfemale\n\n\nGentoo\nBiscoe\n52.5\n15.6\n221\n5450\nmale\n\n\nGentoo\nBiscoe\n47.4\n14.6\n212\n4725\nfemale\n\n\nGentoo\nBiscoe\n50.0\n15.9\n224\n5350\nmale\n\n\nGentoo\nBiscoe\n44.9\n13.8\n212\n4750\nfemale\n\n\nGentoo\nBiscoe\n50.8\n17.3\n228\n5600\nmale\n\n\nGentoo\nBiscoe\n43.4\n14.4\n218\n4600\nfemale\n\n\nGentoo\nBiscoe\n51.3\n14.2\n218\n5300\nmale\n\n\nGentoo\nBiscoe\n47.5\n14.0\n212\n4875\nfemale\n\n\nGentoo\nBiscoe\n52.1\n17.0\n230\n5550\nmale\n\n\nGentoo\nBiscoe\n47.5\n15.0\n218\n4950\nfemale\n\n\nGentoo\nBiscoe\n52.2\n17.1\n228\n5400\nmale\n\n\nGentoo\nBiscoe\n45.5\n14.5\n212\n4750\nfemale\n\n\nGentoo\nBiscoe\n49.5\n16.1\n224\n5650\nmale\n\n\nGentoo\nBiscoe\n44.5\n14.7\n214\n4850\nfemale\n\n\nGentoo\nBiscoe\n50.8\n15.7\n226\n5200\nmale\n\n\nGentoo\nBiscoe\n49.4\n15.8\n216\n4925\nmale\n\n\nGentoo\nBiscoe\n46.9\n14.6\n222\n4875\nfemale\n\n\nGentoo\nBiscoe\n48.4\n14.4\n203\n4625\nfemale\n\n\nGentoo\nBiscoe\n51.1\n16.5\n225\n5250\nmale\n\n\nGentoo\nBiscoe\n48.5\n15.0\n219\n4850\nfemale\n\n\nGentoo\nBiscoe\n55.9\n17.0\n228\n5600\nmale\n\n\nGentoo\nBiscoe\n47.2\n15.5\n215\n4975\nfemale\n\n\nGentoo\nBiscoe\n49.1\n15.0\n228\n5500\nmale\n\n\nGentoo\nBiscoe\n46.8\n16.1\n215\n5500\nmale\n\n\nGentoo\nBiscoe\n41.7\n14.7\n210\n4700\nfemale\n\n\nGentoo\nBiscoe\n53.4\n15.8\n219\n5500\nmale\n\n\nGentoo\nBiscoe\n43.3\n14.0\n208\n4575\nfemale\n\n\nGentoo\nBiscoe\n48.1\n15.1\n209\n5500\nmale\n\n\nGentoo\nBiscoe\n50.5\n15.2\n216\n5000\nfemale\n\n\nGentoo\nBiscoe\n49.8\n15.9\n229\n5950\nmale\n\n\nGentoo\nBiscoe\n43.5\n15.2\n213\n4650\nfemale\n\n\nGentoo\nBiscoe\n51.5\n16.3\n230\n5500\nmale\n\n\nGentoo\nBiscoe\n46.2\n14.1\n217\n4375\nfemale\n\n\nGentoo\nBiscoe\n55.1\n16.0\n230\n5850\nmale\n\n\nGentoo\nBiscoe\n48.8\n16.2\n222\n6000\nmale\n\n\nGentoo\nBiscoe\n47.2\n13.7\n214\n4925\nfemale\n\n\nGentoo\nBiscoe\n46.8\n14.3\n215\n4850\nfemale\n\n\nGentoo\nBiscoe\n50.4\n15.7\n222\n5750\nmale\n\n\nGentoo\nBiscoe\n45.2\n14.8\n212\n5200\nfemale\n\n\nGentoo\nBiscoe\n49.9\n16.1\n213\n5400\nmale\n\n\nChinstrap\nDream\n46.5\n17.9\n192\n3500\nfemale\n\n\nChinstrap\nDream\n50.0\n19.5\n196\n3900\nmale\n\n\nChinstrap\nDream\n51.3\n19.2\n193\n3650\nmale\n\n\nChinstrap\nDream\n45.4\n18.7\n188\n3525\nfemale\n\n\nChinstrap\nDream\n52.7\n19.8\n197\n3725\nmale\n\n\nChinstrap\nDream\n45.2\n17.8\n198\n3950\nfemale\n\n\nChinstrap\nDream\n46.1\n18.2\n178\n3250\nfemale\n\n\nChinstrap\nDream\n51.3\n18.2\n197\n3750\nmale\n\n\nChinstrap\nDream\n46.0\n18.9\n195\n4150\nfemale\n\n\nChinstrap\nDream\n51.3\n19.9\n198\n3700\nmale\n\n\nChinstrap\nDream\n46.6\n17.8\n193\n3800\nfemale\n\n\nChinstrap\nDream\n51.7\n20.3\n194\n3775\nmale\n\n\nChinstrap\nDream\n47.0\n17.3\n185\n3700\nfemale\n\n\nChinstrap\nDream\n52.0\n18.1\n201\n4050\nmale\n\n\nChinstrap\nDream\n45.9\n17.1\n190\n3575\nfemale\n\n\nChinstrap\nDream\n50.5\n19.6\n201\n4050\nmale\n\n\nChinstrap\nDream\n50.3\n20.0\n197\n3300\nmale\n\n\nChinstrap\nDream\n58.0\n17.8\n181\n3700\nfemale\n\n\nChinstrap\nDream\n46.4\n18.6\n190\n3450\nfemale\n\n\nChinstrap\nDream\n49.2\n18.2\n195\n4400\nmale\n\n\nChinstrap\nDream\n42.4\n17.3\n181\n3600\nfemale\n\n\nChinstrap\nDream\n48.5\n17.5\n191\n3400\nmale\n\n\nChinstrap\nDream\n43.2\n16.6\n187\n2900\nfemale\n\n\nChinstrap\nDream\n50.6\n19.4\n193\n3800\nmale\n\n\nChinstrap\nDream\n46.7\n17.9\n195\n3300\nfemale\n\n\nChinstrap\nDream\n52.0\n19.0\n197\n4150\nmale\n\n\nChinstrap\nDream\n50.5\n18.4\n200\n3400\nfemale\n\n\nChinstrap\nDream\n49.5\n19.0\n200\n3800\nmale\n\n\nChinstrap\nDream\n46.4\n17.8\n191\n3700\nfemale\n\n\nChinstrap\nDream\n52.8\n20.0\n205\n4550\nmale\n\n\nChinstrap\nDream\n40.9\n16.6\n187\n3200\nfemale\n\n\nChinstrap\nDream\n54.2\n20.8\n201\n4300\nmale\n\n\nChinstrap\nDream\n42.5\n16.7\n187\n3350\nfemale\n\n\nChinstrap\nDream\n51.0\n18.8\n203\n4100\nmale\n\n\nChinstrap\nDream\n49.7\n18.6\n195\n3600\nmale\n\n\nChinstrap\nDream\n47.5\n16.8\n199\n3900\nfemale\n\n\nChinstrap\nDream\n47.6\n18.3\n195\n3850\nfemale\n\n\nChinstrap\nDream\n52.0\n20.7\n210\n4800\nmale\n\n\nChinstrap\nDream\n46.9\n16.6\n192\n2700\nfemale\n\n\nChinstrap\nDream\n53.5\n19.9\n205\n4500\nmale\n\n\nChinstrap\nDream\n49.0\n19.5\n210\n3950\nmale\n\n\nChinstrap\nDream\n46.2\n17.5\n187\n3650\nfemale\n\n\nChinstrap\nDream\n50.9\n19.1\n196\n3550\nmale\n\n\nChinstrap\nDream\n45.5\n17.0\n196\n3500\nfemale\n\n\nChinstrap\nDream\n50.9\n17.9\n196\n3675\nfemale\n\n\nChinstrap\nDream\n50.8\n18.5\n201\n4450\nmale\n\n\nChinstrap\nDream\n50.1\n17.9\n190\n3400\nfemale\n\n\nChinstrap\nDream\n49.0\n19.6\n212\n4300\nmale\n\n\nChinstrap\nDream\n51.5\n18.7\n187\n3250\nmale\n\n\nChinstrap\nDream\n49.8\n17.3\n198\n3675\nfemale\n\n\nChinstrap\nDream\n48.1\n16.4\n199\n3325\nfemale\n\n\nChinstrap\nDream\n51.4\n19.0\n201\n3950\nmale\n\n\nChinstrap\nDream\n45.7\n17.3\n193\n3600\nfemale\n\n\nChinstrap\nDream\n50.7\n19.7\n203\n4050\nmale\n\n\nChinstrap\nDream\n42.5\n17.3\n187\n3350\nfemale\n\n\nChinstrap\nDream\n52.2\n18.8\n197\n3450\nmale\n\n\nChinstrap\nDream\n45.2\n16.6\n191\n3250\nfemale\n\n\nChinstrap\nDream\n49.3\n19.9\n203\n4050\nmale\n\n\nChinstrap\nDream\n50.2\n18.8\n202\n3800\nmale\n\n\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nfemale\n\n\nChinstrap\nDream\n51.9\n19.5\n206\n3950\nmale\n\n\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nfemale\n\n\nChinstrap\nDream\n45.7\n17.0\n195\n3650\nfemale\n\n\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmale\n\n\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nfemale\n\n\nChinstrap\nDream\n49.6\n18.2\n193\n3775\nmale\n\n\nChinstrap\nDream\n50.8\n19.0\n210\n4100\nmale\n\n\nChinstrap\nDream\n50.2\n18.7\n198\n3775\nfemale\n\n\n\n\n\n\n\n\nAfter dropping the year column and filtering for the missing values (NAs), we are left with 333 rows and 7 columns.\nNote: In machine learning, especially while using multiple predictors/features, it’s advised to transform all the numeric data so the values are on the same scale. However, since the three numeric features are on the same scale (i.e., milimeters) in this data set, it is not required."
  },
  {
    "objectID": "posts/013-linear-regression/index.html#exploratory-plots",
    "href": "posts/013-linear-regression/index.html#exploratory-plots",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "Exploratory plots",
    "text": "Exploratory plots\nIt’s always a good idea to visualize the data to get a better understanding of what we are working with. Figure 2 shows the per species count of penguins by sex. And as you will see that we will be regressing body mass in grams on the remaining variables/predictors, let’s visualize the relationship between body mass and the three numeric predictors per species in Figure 3.\n\nggplot(df_penguins, aes(x = sex, fill = species)) +\n  geom_bar(alpha = 0.8) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"), \n                    guide = \"none\") +\n  theme_minimal() +\n  facet_wrap(~species, ncol = 1) +\n  coord_flip()\n\n\n\n\nFigure 2: Per species penguin count.\n\n\n\n\n\nlong_data &lt;- df_penguins %&gt;% dplyr::select(species, bill_length_mm, bill_depth_mm,  flipper_length_mm,  body_mass_g) %&gt;%\n  gather(key = \"predictor\", value = \"value\", -c(species, body_mass_g))\n\nggplot(data = long_data, aes(x = value, y = body_mass_g, color = species)) +\n  geom_point(aes(shape = species),\n             size = 2) +\n      geom_smooth(method=\"lm\", se = FALSE) +\n  scale_color_manual(values = c(\"darkorange\",\"darkorchid\",\"cyan4\")) +\n  facet_wrap(~ predictor, scales = \"free_x\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure 3: Scatter plot showing the relationship between body mass (y-axis) and bill depth, bill length, and flipper length (x-axis) per species (color and shape)."
  },
  {
    "objectID": "posts/013-linear-regression/index.html#first-model",
    "href": "posts/013-linear-regression/index.html#first-model",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "First model",
    "text": "First model\nIn the first model, we will regress body mass in grams, i.e., the body_mass_g column in Table 1 on three numeric predictors bill_length_mm, bill_depth_mm, flipper_length_mm, and one categorical feature sex (female and male).\nWe will use the lm function from the base R to fit the linear regression model and the summary() function to summarize the output of the model fitting. The summary provides several useful statistics that can be used to interpret the model.\nTo execute the lm() function in R, we must provide a formula describing the model to fit and the input data. The formula for our model can be written as Equation 3\n\\[body\\_mass\\_g \\sim bill\\_length\\_mm + bill\\_depth\\_mm + flipper\\_length\\_mm + sex \\tag{3}\\]\nThe variable on the right side of the tilde symbol is the target, and the variables on the left, separated by plus signs, are the predictors.\n\n# Fit the linear model\nfit1 &lt;- lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + sex, data = df_penguins)\n\n# Get the summary of the model\nsummary(fit1)\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    sex, data = df_penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-927.81 -247.66    5.52  220.60  990.45 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -2288.465    631.580  -3.623 0.000337 ***\nbill_length_mm       -2.329      4.684  -0.497 0.619434    \nbill_depth_mm       -86.088     15.570  -5.529 6.58e-08 ***\nflipper_length_mm    38.826      2.448  15.862  &lt; 2e-16 ***\nsexmale             541.029     51.710  10.463  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 340.8 on 328 degrees of freedom\nMultiple R-squared:  0.823, Adjusted R-squared:  0.8208 \nF-statistic: 381.3 on 4 and 328 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s go through the summary of our fitted linear regression model which typically includes the following statistics (from top to bottom):\n\nCall: Call section shows the parameters that were passed to the lm() function.\nResiduals: Residuals represent the differences between the actual and predicted values of the dependent variable (body_mass_g). The summary shows statistics such as minimum, first quartile (1Q), median, third quartile (3Q), and maximum values of the residuals.\nCoefficients: The coefficients table presents the estimated coefficients for each predictor variable. The Estimate column shows the estimated effect of each variable on the dependent variable. The Std. Error column indicates the standard error of the estimate, which measures the variability of the coefficient. The t value column represents the t-statistic, which assesses the significance of each coefficient. The Pr(&gt;|t|) column provides the p-value, which indicates the probability of observing a t-statistic as extreme as the one calculated if the null hypothesis were true (null hypothesis: the coefficient is not significant).\n\nThe (Intercept) coefficient represents the estimated body mass when all the predictor variables are zero.\nThe bill_length_mm, bill_depth_mm, and flipper_length_mm coefficients indicate the estimated change in body mass associated with a one-unit increase in each respective predictor.\nThe sexmale coefficient represents the estimated difference in body mass between males and females (since it’s a binary variable). Here female is the reference value, which means that males are 541.029 g heavier than females.\n\nThe significance of each coefficient can be determined based on the corresponding p-value. In this case, the (Intercept), bill_depth_mm, flipper_length_mm, and sexmale coefficients are all highly significant (p &lt; 0.001), indicating that they have a significant effect on the body mass. However, the bill_length_mm coefficient is not statistically significant (p = 0.619), suggesting that it may not have a significant effect on body mass.\nResidual standard error: This value represents the standard deviation of the residuals (340.8), providing an estimate of the model’s prediction error. A lower residual standard error indicates a better fit.\nMultiple R-squared and Adjusted R-squared: The multiple R-squared value (0.823) indicates the proportion of variance in the dependent variable that can be explained by the predictor variables. The adjusted R-squared (0.8208) adjusts the R-squared value for the number of predictors in the model, penalizing the addition of irrelevant predictors. A higher R-squared value indicates a better fit.\nF-statistic and p-value: The F-statistic tests the overall significance of the model by comparing the fit of the current model with a null model (no predictors). The associated p-value (&lt; 2.2e-16) suggests that the model is highly significant and provides a better fit than the null model. A low p-value indicates that the model is statistically significant.\n\nIn conclusion, this linear regression model suggests that the numerical predictors bill_depth_mm, flipper_length_mm, and the categorical feature sex are significant predictors of the body mass of penguins. However, bill_length_mm does not appear to be a significant predictor in this model.\n\nDiagnostic plots\nIn addition to the summary statistics we can also generates a set of diagnostic plots that can help assess the assumptions and evaluate the performance of the model. Let’s interpret each of the common diagnostic plots typically produced by plot(fit) (Figure 4 to Figure 8).\n\nplot(fit1, which = 1)\n\n\n\n\nFigure 4: Residuals vs. Fitted. This plot shows the residuals (vertical axis) plotted against the predicted/fitted values (horizontal axis). It helps you examine the assumption of constant variance (homoscedasticity). In this plot, you would ideally want to see a random scatter of points with no discernible pattern or trend, indicating that the assumption of constant variance is reasonable.\n\n\n\n\n\nplot(fit1, which = 2)\n\n\n\n\nFigure 5: Normal Q-Q Plot. This plot assesses the assumption of normality of residuals. It compares the standardized residuals to the quantiles of a normal distribution. If the points lie approximately along a straight line, it suggests that the residuals are normally distributed. Deviations from the straight line indicate departures from normality.\n\n\n\n\n\nplot(fit1, which = 3)\n\n\n\n\nFigure 6: Scale-Location Plot. This plot, also known as the spread-location plot, is used to evaluate the assumption of constant variance (homoscedasticity) similarly to the Residuals vs. Fitted plot. However, it provides a different perspective by plotting the square root of the standardized residuals against the fitted values. A reasonably constant spread of points with no discernible pattern indicates homoscedasticity.\n\n\n\n\n\nplot(fit1, which = 4)\n\n\n\n\nFigure 7: Cook’s Distance. Cook’s distance is a measure of the influence of each observation on the model fit. The plot shows the Cook’s distance values for each observation. Points with high Cook’s distance may have a considerable impact on the model and should be further examined.\n\n\n\n\n\nplot(fit1, which = 5)\n\n\n\n\nFigure 8: Residuals vs. Leverage. This plot helps identify influential observations by plotting the standardized residuals against the leverage values. Leverage values measure how much an observation’s predictor values differ from the average predictor values. Points that have both high leverage and high residuals are worth investigating as they may have a disproportionate impact on the model.\n\n\n\n\nBy examining these diagnostic plots, you can gain insights into the assumptions of the linear regression model and detect any potential issues such as heteroscedasticity, non-linearity, or influential observations.\nNote that the interpretation of the diagnostic plots can vary depending on the specific characteristics of your dataset and the context of your analysis. It is important to carefully examine these plots to assess the validity and performance of your linear regression model."
  },
  {
    "objectID": "posts/013-linear-regression/index.html#second-model",
    "href": "posts/013-linear-regression/index.html#second-model",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "Second model",
    "text": "Second model\nIn the second model, we will regress body mass in grams, i.e., the body_mass_g column in Table 1 on three numeric predictors bill_length_mm, bill_depth_mm, flipper_length_mm, and three categorical predictors species (Adelie, Chinstrap, and Gentoo), island (Biscoe, Dream, and Torgersen), and sex (female and male). The formula for this second model will be Equation 4\n\\[body\\_mass\\_g \\sim species + island + bill\\_length\\_mm + \\\\\nbill\\_depth\\_mm + flipper\\_length\\_mm + sex \\tag{4}\\]\n\n# Fit the linear model\nfit2 &lt;- lm(body_mass_g ~ species + island + bill_length_mm + bill_depth_mm + flipper_length_mm + sex, data = df_penguins)\n\n# Get the summary of the model\nsummary(fit2)\n\n\nCall:\nlm(formula = body_mass_g ~ species + island + bill_length_mm + \n    bill_depth_mm + flipper_length_mm + sex, data = df_penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-779.20 -167.35   -3.16  179.37  914.27 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -1500.029    575.822  -2.605 0.009610 ** \nspeciesChinstrap   -260.306     88.551  -2.940 0.003522 ** \nspeciesGentoo       987.761    137.238   7.197 4.30e-12 ***\nislandDream         -13.103     58.541  -0.224 0.823032    \nislandTorgersen     -48.064     60.922  -0.789 0.430722    \nbill_length_mm       18.189      7.136   2.549 0.011270 *  \nbill_depth_mm        67.575     19.821   3.409 0.000734 ***\nflipper_length_mm    16.239      2.939   5.524 6.80e-08 ***\nsexmale             387.224     48.138   8.044 1.66e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 287.9 on 324 degrees of freedom\nMultiple R-squared:  0.8752,    Adjusted R-squared:  0.8721 \nF-statistic: 284.1 on 8 and 324 DF,  p-value: &lt; 2.2e-16\n\n\nSimilar to the model summary of fit1 , fit2 also have the same evaluation metrics; however, in the coefficient section now we have a few more rows related to the two additional predictors species and island.\n\nThe speciesChinstrap and speciesGentoo coefficients represent the estimated differences in body mass between the corresponding penguin species and the reference category. Here the reference category is Adélie. Reference category is picked in alphabetical order.\nThe islandDream and islandTorgersen coefficients represent the estimated differences in body mass between penguins from the corresponding islands and the reference island Biscoe.\n\nIn this model, the following predictors are significant:\n\nspeciesChinstrap, speciesGentoo, bill_depth_mm, flipper_length_mm, and sexmale have highly significant coefficients (p &lt; 0.01).\nbill_length_mm has a marginally significant coefficient (p = 0.011). Bill length was not significant when it was used without species and island predictors in the model.\n\nThe significance of islandDream and islandTorgersen is not supported by the data, as indicated by non-significant p-values."
  },
  {
    "objectID": "posts/013-linear-regression/index.html#comparing-the-two-models",
    "href": "posts/013-linear-regression/index.html#comparing-the-two-models",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "Comparing the two models",
    "text": "Comparing the two models\nWe can make two important observations by comparing the two models we built above. First, changing predictors may influence the coefficient/significance of other predictors. We see that in the first model, bill_length_mm was not significant, but in the second model, it is marginally significant. Second, different sets of predictors may represent different proportions of variance in the dependent variable that the predictor variables can explain. The second model shows that R-squared values have increased to 0.8752 and 0.8721 from 0.823 and 0.8208. This means that the two additional variables helped capture more variance. Therefore, choosing an appropriate set of predictors that best describes the complexity of the data is important."
  },
  {
    "objectID": "posts/013-linear-regression/index.html#first-model-1",
    "href": "posts/013-linear-regression/index.html#first-model-1",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "First model",
    "text": "First model\nIn the first model, similar to before, we will regress body_mass_g on three numeric predictors bill_length_mm, bill_depth_mm, flipper_length_mm, and one categorical feature, sex (female and male). However, this time we will first train the model only on the training data and then test the model on the test data. So let’s first build the model.\n\n# Fit the linear model\nfit1_train &lt;- lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + sex, data = train_data)\n\n# Get the summary of the model\nsummary(fit1_train)\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    sex, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-939.59 -248.45   -7.64  229.83  994.91 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -2027.1777   730.7280  -2.774  0.00593 ** \nbill_length_mm       -0.4218     5.4003  -0.078  0.93781    \nbill_depth_mm       -92.3046    18.2396  -5.061 7.89e-07 ***\nflipper_length_mm    37.5979     2.7917  13.468  &lt; 2e-16 ***\nsexmale             544.0535    60.4370   9.002  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 350.4 on 261 degrees of freedom\nMultiple R-squared:  0.8161,    Adjusted R-squared:  0.8133 \nF-statistic: 289.6 on 4 and 261 DF,  p-value: &lt; 2.2e-16\n\n\nNow let’s test the model on the test data. To test the model, we must first predict the values of our target using the predictors from the test dataset. Then the predicted target values are compared to the original target values from the test data set (the ground truth) to compute an accuracy metric.\n\n# Predict on the test data set.\npredict1 &lt;- predict(fit1_train, newdata = test_data)\n\n# The predicted values. They are in the same unit as the original target i.e. body_mass_g\ncat(predict1)\n\n3625.943 3534.313 3806.928 3785.672 3381.555 3723.386 3449.419 3963.758 3883.43 4038.404 3804.938 3348.516 4159.798 3502.267 3335.694 4161.442 3524.614 4285.875 3673.597 3192.804 4141.798 4133.245 4275.416 4027.821 4164.188 4074.142 3729.952 3410.047 5638.751 5289.111 4501.428 4829.097 5195.706 5263.446 4698.645 5478.621 5232.544 4830.154 5384.665 5505.299 4500.286 4716.475 4689.841 5262.308 4584.462 5450.085 4821.684 5573.252 5683.931 4482.669 4960.761 5347.704 3979.395 4102.772 3311.751 3379.997 4147.781 3491.439 4356.094 4131.268 3444.227 3595.172 3753.652 3668.3 4166.329 4440.224 3669.948\n\n\n\nMean Squared Error (MSE): MSE measures the average squared difference between the predicted and actual values. It provides a measure of the model’s overall prediction error.\n\n\nmse &lt;- mean((test_data$body_mass_g - predict1)^2)\n\nprint(mse)\n\n[1] 91057.22\n\n\n\nRoot Mean Squared Error (RMSE): RMSE is the square root of MSE and provides a more interpretable measure of the model’s prediction error.\n\n\nrmse &lt;- sqrt(mse)\n\nprint(rmse)\n\n[1] 301.7569\n\n\n\nMean Absolute Error (MAE): MAE measures the average absolute difference between the predicted and actual values. It represents the average magnitude of the prediction errors.\n\n\nmae &lt;- mean(abs(test_data$body_mass_g - predict1))\n\nprint(mae)\n\n[1] 240.685\n\n\nMSE, RMSE, and MAE give a measure of the prediction error, with lower values indicating better model performance and more accurate predictions. The RMSE is often preferred over MSE because it is in the same unit as the dependent variable, making it easier to interpret in the context of the problem. MAE does not square the prediction errors as the MSE does. Consequently, the MAE does not overly penalize larger errors, making it more robust to outliers in the data."
  },
  {
    "objectID": "posts/013-linear-regression/index.html#second-model-1",
    "href": "posts/013-linear-regression/index.html#second-model-1",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "Second model",
    "text": "Second model\nIn the second model, similar to before, we will regress body_mass_g` on three numeric predictors bill_length_mm, bill_depth_mm, flipper_length_mm, and three categorical predictors species (Adelie, Chinstrap, and Gentoo), island (Biscoe, Dream, and Torgersen), and sex (female and male). However, as we did in the first model above we will first train the model only on the training data and then test the model on the test data. So let’s first build the model.\n\n# Fit the linear model\nfit2_train &lt;- lm(body_mass_g ~ species + island + bill_length_mm + bill_depth_mm + flipper_length_mm + sex, data = train_data)\n\n# Get the summary of the model\nsummary(fit2_train)\n\n\nCall:\nlm(formula = body_mass_g ~ species + island + bill_length_mm + \n    bill_depth_mm + flipper_length_mm + sex, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-773.35 -159.93  -12.94  168.29  922.46 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -1601.876    636.617  -2.516 0.012472 *  \nspeciesChinstrap   -335.663     95.511  -3.514 0.000521 ***\nspeciesGentoo      1013.366    145.792   6.951 2.99e-11 ***\nislandDream          -2.507     65.309  -0.038 0.969413    \nislandTorgersen     -17.043     67.611  -0.252 0.801179    \nbill_length_mm       25.903      7.706   3.361 0.000894 ***\nbill_depth_mm        75.761     22.036   3.438 0.000683 ***\nflipper_length_mm    14.367      3.142   4.573 7.49e-06 ***\nsexmale             364.997     53.517   6.820 6.48e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 285.8 on 257 degrees of freedom\nMultiple R-squared:  0.8796,    Adjusted R-squared:  0.8758 \nF-statistic: 234.6 on 8 and 257 DF,  p-value: &lt; 2.2e-16\n\n\nNow let’s test the model on the test data.\n\n# Predict on the test data set.\npredict2 &lt;- predict(fit2_train, newdata = test_data)\n\n# The predicted values. They are in the same unit as the original target i.e. body_mass_g\ncat(predict2)\n\n3590.195 3624.511 4353.636 3781.747 3468.289 4206.503 3390.542 3988.718 4232.997 3965.01 3975.727 3417.981 4194.577 3225.112 3229.69 4021.976 3419.276 4256.824 3866.844 3273.355 4009.004 4147.367 3992.548 3880.737 4198.07 3832.495 3845.419 3310.229 5610.905 5355.166 4720.985 4604.059 5239.927 5425.792 4613.011 5505.981 5300.919 4711.534 5252.184 5459.304 4618.691 4722.829 4853.032 5495.73 4827.623 5494.399 4758.6 5718.334 5460.369 4582.038 5169.073 5457.311 3981.178 4106.045 3245.918 3400.699 3879.755 3354.457 4253.043 4292.449 3112.631 3480.889 3342.37 3550.431 4031.654 4206.217 3621.641\n\n\nAnd compute the accuracy metrics.\n\nMean Squared Error (MSE)\n\n\nmse &lt;- mean((test_data$body_mass_g - predict2)^2)\n\nprint(mse)\n\n[1] 90171.39\n\n\n\nRoot Mean Squared Error (RMSE)\n\n\nrmse &lt;- sqrt(mse)\n\nprint(rmse)\n\n[1] 300.2855\n\n\n\nMean Absolute Error (MAE)\n\n\nmae &lt;- mean(abs(test_data$body_mass_g - predict2))\n\nprint(mae)\n\n[1] 254.6932\n\n\nComparing the predictive performance of the two models, the difference between the metrics is minimal/negligible. Therefore, the inclusion or exclusion of species and island predictors does not have an appreciable effect on the predictive performance of the models based on the train test dataset used."
  },
  {
    "objectID": "posts/013-linear-regression/index.html#what-are-some-real-life-cases-with-linear-relationships-between-the-dependent-and-the-independent-variables",
    "href": "posts/013-linear-regression/index.html#what-are-some-real-life-cases-with-linear-relationships-between-the-dependent-and-the-independent-variables",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "What are some real-life cases with linear relationships between the dependent and the independent variables?",
    "text": "What are some real-life cases with linear relationships between the dependent and the independent variables?\nThere are many real-life cases in which a linear relationship exists between the dependent and independent variables. Here are a few examples:\nThe relationship between the weight of an object and the force needed to lift it. If we plot the weight of an object on the x-axis and the force required to lift it on the y-axis, we can use linear regression to model the relationship between the two variables.\nThe relationship between the number of hours a student studies and their test scores. If we plot the number of hours a student studies on the x-axis and their test scores on the y-axis, we can use linear regression to model the relationship between the two variables.\nThe relationship between the size of a company and its profitability. If we plot the size of a company on the x-axis and its profitability on the y-axis, we can use linear regression to model the relationship between the two variables.\nThe relationship between the age of a car and its fuel efficiency. If we plot the age of a car on the x-axis and its fuel efficiency on the y-axis, we can use linear regression to model the relationship between the two variables.\nIn each of these cases, there is a linear relationship between the dependent and independent variables, which can be modeled using linear regression."
  },
  {
    "objectID": "posts/013-linear-regression/index.html#what-are-the-real-life-cases-where-a-linear-regression-will-not-work",
    "href": "posts/013-linear-regression/index.html#what-are-the-real-life-cases-where-a-linear-regression-will-not-work",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "What are the real-life cases where a linear regression will not work?",
    "text": "What are the real-life cases where a linear regression will not work?\nLinear regression is not suitable for modeling relationships between variables that are not linear. Some examples of real-life cases where linear regression will not work include:\nThe relationship between the temperature of a substance and its volume. The volume of a substance changes non-linearly with temperature, so linear regression would not be appropriate for modeling this relationship.\nThe relationship between the price of a product and the demand for it. The demand for a product often follows an inverse relationship with price, so linear regression would not be appropriate for modeling this relationship.\nThe relationship between the weight of an object and the time it takes to fall to the ground. The time it takes for an object to fall to the ground increases non-linearly with its weight, so linear regression would not be appropriate for modeling this relationship.\nThe relationship between the age of a person and their risk of developing a certain disease. The risk of developing a disease often increases non-linearly with age, so linear regression would not be appropriate for modeling this relationship.\nIn each of these cases, there is a non-linear relationship between the variables, and linear regression would not be able to accurately model this relationship. Instead, other techniques such as polynomial regression or logistic regression may be more appropriate."
  },
  {
    "objectID": "posts/013-linear-regression/index.html#what-are-some-scientific-medical-or-engineering-breakthroughs-where-linear-regression-was-used",
    "href": "posts/013-linear-regression/index.html#what-are-some-scientific-medical-or-engineering-breakthroughs-where-linear-regression-was-used",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "What are some scientific, medical, or engineering breakthroughs where linear regression was used?",
    "text": "What are some scientific, medical, or engineering breakthroughs where linear regression was used?\nLinear regression is a widely used statistical technique that has been applied to a variety of scientific, medical, and engineering fields. Here are a few examples of famous breakthroughs where linear regression was used:\nIn the field of medicine, linear regression has been used to understand the relationship between various factors and the risk of developing certain diseases. For example, researchers have used linear regression to understand the relationship between diet and the risk of developing diabetes (Panagiotakos et al. 2005).\nIn the field of engineering, linear regression has been used to understand the relationship between various factors and the strength of materials. For example, researchers have used linear regression to understand the relationship between the composition of steel and its strength (Singh Tumrate, Roy Chowdhury, and Mishra 2021).\nIn the field of economics, linear regression has been used to understand the relationship between various factors and the performance of stocks. For example, researchers have used linear regression to understand the relationship between a company’s earnings and its stock price.\nIn the field of psychology, linear regression has been used to understand the relationship between various factors and human behavior. For example, researchers have used linear regression to understand the relationship between personality traits and job performance.\nThese are just a few examples of the many ways in which linear regression has been used to make scientific, medical, and engineering breakthroughs."
  },
  {
    "objectID": "posts/013-linear-regression/index.html#how-does-multicollinearity-affect-linear-regression-how-to-mitigate-and-interpret-models-built-with-multiple-colinear-variables.",
    "href": "posts/013-linear-regression/index.html#how-does-multicollinearity-affect-linear-regression-how-to-mitigate-and-interpret-models-built-with-multiple-colinear-variables.",
    "title": "Linear regression for inferential and predictive modeling",
    "section": "How does multicollinearity affect linear regression? How to mitigate and interpret models built with multiple colinear variables.",
    "text": "How does multicollinearity affect linear regression? How to mitigate and interpret models built with multiple colinear variables.\nMulticollinearity is a situation in which two or more independent variables are highly correlated with each other. This can be a problem in linear regression because it can lead to unstable and unreliable estimates of the regression coefficients.\nThere are a few ways that multicollinearity can affect linear regression:\nIt can cause the standard errors of the regression coefficients to be inflated, leading to a decrease in the statistical power of the model.\nIt can make it difficult to interpret the individual contributions of the independent variables to the dependent variable because the coefficients of the correlated variables will be correlated as well.\nIt can make the model more sensitive to small changes in the data, leading to unstable predictions.\nTo mitigate the effects of multicollinearity, you can do the following:\nRemove one or more of the correlated variables from the model.\nUse regularization techniques such as Lasso or Ridge regression, which can help to shrink the coefficients of correlated variables towards zero.\nTransform the correlated variables by taking their logarithm, square root, or other non-linear transformation.\nUse variable selection techniques such as backward elimination or forward selection to select a subset of uncorrelated variables for the model.\nTo interpret a model built with multiple collinear variables, you can look at the R-squared value and the p-values of the individual variables to assess their contribution to the model. However, it is important to bear in mind that the estimates of the regression coefficients may be unstable and unreliable due to the presence of multicollinearity."
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-11-21 Code on Kaggle\n2022-11-18 First draft"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#dataset",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#dataset",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Dataset",
    "text": "Dataset\nFor our example exercises, we will work with the “Global CO2 emissions from cement production” dataset (Andrew 2022). I have subsetted the data from 1928 onward and dropped any columns with all NAs or zeros. The table below shows all the data we will use in this tutorial.\nFigure 1 shows per country yearly (x-axis) emissions logged to base 10 (y-axis). The log is taken for visualization purposes. All the statistical calculations will be done on the original values.\n\n\n\n\n\n\nNote\n\n\n\nThe emissions from the use of fossil fuels in cement production are not included in this dataset since they are usually included elsewhere in global datasets of fossil CO2 emissions. The process emissions in this dataset, which result from the decomposition of carbonates in the production of cement clinker, amounted to ~1.7 Gt CO2 in 2021, while emissions from the combustion of fossil fuels to produce the heat required amounted to an additional ~1.0 Gt CO2 in 2021.\n\n\n\nsuppressMessages(library(DT))\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(kableExtra))\n\n\n# Download the data from Zenodo\ndat &lt;- readr::read_csv(\"https://zenodo.org/record/7081360/files/1.%20Cement_emissions_data.csv\", show_col_types = FALSE)\n\n# Filter the data and present it in a DT::datatable\ndat &lt;- dat %&gt;% dplyr::filter(Year &gt;= 1928) %&gt;%\n  select_if(function(x) all(!is.na(x))) %&gt;%\n  select_if(function(x) all(!x == 0))\nDT::datatable(dat)\n\n\n\n\n\n\n\ndat_gather &lt;- dat %&gt;% gather(key = \"Country\", value = \"Emission\", -Year)\nggplot(dat_gather, aes(x = Year, y = as.numeric(log10(Emission)), color = Country)) +\n  geom_line(aes(group = Country)) + \n  labs(x = \"Year\", y = \"log10(Emission)\", color = \"\") +theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nFigure 1: Global CO2 emissions from cement production"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "The method",
    "text": "The method\n\nFirst, we rank all of the values (from both groups) from the smallest to largest. Equal values are allocated the average of the ranks they would have if there was tiny differences between them.\nNext we sum the ranks for each group. You call the sum of the ranks for the larger group \\(R_1\\) and for the smaller sized group, \\(R_2\\). If both groups are equally sized then we can label them whichever way round we like.\nWe then input \\(R_1\\) and \\(R_2\\) and also \\(N_1\\) and \\(N_2\\), the respective sizes of each group, into the Equation 1.\n\n\\[\n\\begin{equation} U = (N_1 \\times N_2) + \\dfrac{N_1 \\times (N_1+1)}{2} - R_1 \\end{equation}\n\\tag{1}\\]\n\nThen we compare the value of \\(U\\) to significance tables. You find the intersection of the column with the value of \\(N_1\\) and the row with the value of \\(N_2\\). In this intersection there will be two ranges of values of \\(U\\) which are significant at the \\(5\\%\\) level. If our value is within one of these ranges, then we have a significant result and we reject the null hypothesis. If our value is not in the range then it is not significant and then the independent variable is unrelated to the dependent variable, we accept the \\(H_0\\).\nAs a check, we also need to examine the means of the two groups, to see which has the higher scores on the dependent variable."
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-two-tailed-test-in-r",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-two-tailed-test-in-r",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Example code for a two-tailed test in R",
    "text": "Example code for a two-tailed test in R\nIn this example we will do a two-tailed test to measure if there is a difference in emission between the USA and Canada. Our null hypothesis \\(H_0\\) is that there is no difference.\n\n(w_res &lt;- wilcox.test(dat$USA, dat$Canada, conf.int = TRUE))\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  dat$USA and dat$Canada\nW = 8763, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 26384 29797\nsample estimates:\ndifference in location \n              28155.27 \n\n\nWe can fetch results from w_res object like w_res$p.value. However, it’s easier to fetch all the values and convert them into a data frame using the boom::tidy() function from the tidyverse suite. As we see in Table 1 the p-value is \\(0\\), which means we can reject our null hypothesis and accept our alternative hypothesis that there is a significant difference in CO2 emissions between the USA and Canada.\n\nbroom::tidy(w_res) %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 1: Two-tailed Wilcoxon rank sum test between Co2 emissions from the USA and Canada\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n28155.27\n8763\n0\n26384\n29797\nWilcoxon rank sum test with continuity correction\ntwo.sided"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-one-tailed-test-in-r",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-one-tailed-test-in-r",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Example code for a one-tailed test in R",
    "text": "Example code for a one-tailed test in R\nIn this example we will do a one-tailed test to measure if emissions from the USA is greater than Canada. Our null hypothesis \\(H_0\\) is that the emissions from the USA is not greater than Canada.\n\n(w_res &lt;- wilcox.test(dat$USA, dat$Canada, conf.int = TRUE, alternative = \"greater\"))\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  dat$USA and dat$Canada\nW = 8763, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n 26696   Inf\nsample estimates:\ndifference in location \n              28155.27 \n\n\nAs we see in Table 2 the p-value is \\(0\\), which means we can reject our null hypothesis and accept our alternative hypothesis that the CO2 emissions are in the USA than Canada.\n\nbroom::tidy(w_res) %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 2: One-tailed Wilcoxon rank sum test between Co2 emissions from the USA and Canada\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n28155.27\n8763\n0\n26696\nInf\nWilcoxon rank sum test with continuity correction\ngreater"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method-1",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method-1",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "The method",
    "text": "The method\n\nCalculate the difference values between your two samples of data. We then remove difference values of zero.\nRank them. If values are tied then you use the same method as in the Mann-Whitney tests. You assign the difference scores the average rank if it was possible to separate the tied difference scores.\nThe ranks of the differences can now have the sign of the difference reattached.\nThe sum of the positive ranks are calculated.\nThe sum of the negative ranks are calculated.\nYou then choose the smaller sum of ranks and we call this our \\(T\\) value, which we compare with significance tables. You choose the row which has the number of pairs of values in your sample.\nReport your findings and make your conclusion."
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-paired-two-tailed-test-in-r",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-paired-two-tailed-test-in-r",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Example code for a paired two-tailed test in R",
    "text": "Example code for a paired two-tailed test in R\nSince this a paired test we will test if there is difference in emission between two time periods say 2000 and 2020 across all the countries in our dataset. Our null hypothesis \\(H_0\\) is that there is no difference.\n\ndat_m &lt;- dat %&gt;% dplyr::select(-Year) %&gt;% as.matrix()\nrownames(dat_m) &lt;- dat$Year\ndat_t &lt;- t(dat_m)\nx &lt;- as.numeric(dat_t[,\"2000\"])\ny &lt;- as.numeric(dat_t[,\"2020\"])\n\n(w_res &lt;- wilcox.test(x, y, conf.int = TRUE, paired = TRUE))\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x and y\nV = 119, p-value = 0.5803\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -3848.5   621.5\nsample estimates:\n(pseudo)median \n          -297 \n\n\nAs we can see in the Table 3, the p.value is 0.5, which is above our alpha level of 0.05; therefore, we can accept our null hypothesis that there is indeed no significant difference in CO2 emissions between 2000 and 2020.\n\nbroom::tidy(w_res) %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 3: Two-tailed Wilcoxon signed rank test between Co2 emissions from 2000 and 2020\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-297\n119\n0.580338\n-3848.5\n621.5\nWilcoxon signed rank exact test\ntwo.sided"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method-2",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method-2",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "The method",
    "text": "The method\n\nRank all data from all groups together; i.e., rank the data from 1 to \\(N\\) ignoring group membership. Assign any tied values the average of the ranks they would have received had they not been tied.\nThe test statistic is given by Equation 2.\n\n\\[\nH = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i}-3(N+1)\n\\tag{2}\\]\nWhere \\(N\\) is the total sample size, \\(k\\) is the number of groups we are comparing, \\(R_i\\) is the sum of ranks for group \\(i\\), and \\(n_i\\) is the sample size of group \\(i\\).\n\nThe decision to reject or not the null hypothesis is made by comparing \\(H\\) to a critical value \\(H_c\\) obtained from a table or a software for a given significance or alpha level. If \\(H\\) is bigger than \\(H_c\\), the null hypothesis is rejected.\nIf the statistic is not significant, then there is no evidence of stochastic dominance between the samples. However, if the test is significant then at least one sample stochastically dominates another sample."
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-in-r",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-in-r",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Example code in R",
    "text": "Example code in R\nWe will use the same long form of data that we used in the Figure 1.\n\n(k_res &lt;- kruskal.test(dat_gather$Emission, as.factor(dat_gather$Country)))\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  dat_gather$Emission and as.factor(dat_gather$Country)\nKruskal-Wallis chi-squared = 1253.6, df = 22, p-value &lt; 2.2e-16\n\n\n\nbroom::tidy(k_res) %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 4: Kruskal-Wallis rank sum test between Co2 emissions from 23 countries from 1928 to 2021\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n1253.576\n0\n22\nKruskal-Wallis rank sum test"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#websites",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#websites",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Website(s)",
    "text": "Website(s)\n\nNon-parametric Hypothesis Tests (Psychology)"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-12-22 Section for ANOVA.\n2022-11-11 Added example code for one-tailed t-test; mentioned Welch’s t-test in a note callout.\n2022-11-10 First draft with live code on Kaggle."
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#paired-vs.-unpaired-tests",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#paired-vs.-unpaired-tests",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Paired vs. unpaired tests",
    "text": "Paired vs. unpaired tests\nPaired tests are used to compare two related data groups, such as before and after measurements. Unpaired tests are used to compare two unrelated data groups, such as men and women."
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#one-sample-vs.-two-sample-tests",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#one-sample-vs.-two-sample-tests",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "One sample vs. two sample tests",
    "text": "One sample vs. two sample tests\nThere are key differences between one-sample and two-sample hypothesis testing. Firstly, in one sample hypothesis testing, we are testing the mean of a single sample against a known population mean. In contrast, two-sample hypothesis testing involves comparing the means of two independent samples. Secondly, one-sample hypothesis testing only requires a single sample size, while two-sample hypothesis testing requires two. Finally, one-sample hypothesis testing is typically used when the population standard deviation is known, while two-sample hypothesis testing is used when the population standard deviation is unknown."
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#one-vs.-two-tailed-tests",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#one-vs.-two-tailed-tests",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "One vs. two-tailed tests",
    "text": "One vs. two-tailed tests\n\n\n\n\n\n\nNote\n\n\n\nAll the example codes in this tutorial are for two-tailed tests except one in the T-test Section 3.\n\n\nIn a hypothesis test, the null and alternate hypotheses are stated in terms of population parameters. These hypotheses are:\n\nNull hypothesis (\\(H_0\\)): The value of the population parameter is equal to the hypothesized value.\nThe alternate hypothesis (\\(H_1\\)): The value of the population parameter is not equal to the hypothesized value.\n\nThe hypothesis test is based on a sample from the population. This sample is used to test the hypotheses by deriving a test statistic. Finally, the value of the test statistic is compared to a critical value. The critical value depends on the alpha level, which is the likelihood of rejecting the null hypothesis when it is true.\nThe null and alternate hypotheses determine the direction of the test. There are two types of hypothesis tests: one-tailed and two-tailed tests.\n\nOne-tailed Test\nA one-tailed test is conducted when the null and alternate hypotheses are stated in terms of “greater than” or “less than”.\nFor example, let’s say that a company wants to test a new advertising campaign. The null hypothesis (\\(H_0\\)) is that the new campaign will have no effect on sales. The alternate hypothesis (\\(H_1\\)) is that the new campaign will increase sales.\nThe null hypothesis is stated as:\n\\(H_0\\) : The population mean is less than or equal to 10%.\nThe alternate hypothesis is stated as:\n\\(H_1\\) : The population mean is greater than 10%.\nThe test is conducted by taking a sample of data and calculating the mean. Then, the mean is compared to the critical value. The null hypothesis is rejected if the mean is greater than the critical value.\n\n\nTwo-tailed Test\nA two-tailed test is conducted when the null and alternate hypotheses are stated in terms of “not equal to”.\nTaking our advertising campaign example. The null hypothesis (\\(H_0\\)) is that the new campaign will have no effect on sales. The alternate hypothesis (\\(H_1\\)) is that the new campaign will increase or decrease sales.\nThe null hypothesis is stated as:\n\\(H_0\\) : The population mean is equal to 10%.\nThe alternate hypothesis is stated as:\n\\(H_1\\) : The population mean is not equal to 10%.\nThe test is conducted by taking a sample of data and calculating the mean. Then, the mean is compared to the critical value. The null hypothesis is rejected if the mean is not equal to the critical value."
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#the-method",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#the-method",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "The method",
    "text": "The method\nNo matter which hypothesis testing method is selected following are the steps that are executed:\n\nFirstly, identify the null hypothesis \\(H_0:\\mu = \\mu_0\\)\nThen identify the alternative hypothesis \\(H_1\\) and decide if it is of the form \\(H_1 : \\mu \\neq \\mu_0\\) (a two-tailed test) or if there is a specific direction for how the mean changes \\(H_1 : \\mu &gt; \\mu_0\\) or \\(H_1 : \\mu &lt; \\mu_0\\), (a one-tailed test).\nNext, calculate the test statistic. Compare the test statistic to the critical values and obtain a range for the p-value which is the probability that the difference between the two groups is due to chance. The test is usually used with a significance level of 0.05, which means that there is a 5% chance that the difference between the two groups is due to chance. However, per recommendations from the American Statistical Association we need to be careful when we state statements like “statistically significant”.\nForm conclusions. If your test statistic is greater than the critical values in the table, it is significant. You can reject the null hypothesis at that level, otherwise you accept it."
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#dataset",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#dataset",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Dataset",
    "text": "Dataset\nFor our example exercises, we will use the dataset from Open Case Studies “exploring global patterns of obesity across rural and urban regions” (Wright et al. 2020) . Body mass index (BMI) is often used as a proxy for adiposity (the condition of having excess body fat) and is measured as an individual’s weight in kilograms (\\(kg\\)) or pounds (\\(lbs\\)) divided by the individual’s height in meters (\\(m^2\\)) squared. Recently, an article published in Nature evaluated and compared the BMI of populations in rural and urban communities around the world (“Rising Rural Body-Mass Index Is the Main Driver of the Global Obesity Epidemic in Adults” 2019). The article challenged the widely-held view that increased urbanization was one of the major reasons for increased global obesity rates. This view came about because many countries around the world have shown increased urbanization levels in parallel with increased obesity rates. In this article, however, the NCD-RisC argued that this might not be the case and that in fact for most regions around the world, BMI measurements are increasing in rural populations just as much, if not more so, than urban populations. Furthermore, this study suggested that obesity has particularly increased in female populations in rural regions:\n\n“We noted a persistently higher rural BMI, especially for women.”\n\nWe will fetch the cleaned version of the data from the the Open Case Studies GitHub repository as data wrangling is out of the scope of this tutorial.\n\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(DT))\nsuppressMessages(library(kableExtra))\ndat &lt;- readr::read_csv(\"https://raw.githubusercontent.com/opencasestudies/ocs-bp-rural-and-urban-obesity/master/data/wrangled/BMI_long.csv\", show_col_types = FALSE)\n\nDT::datatable(dat)"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-unpaired-z-test",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-unpaired-z-test",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Two sample unpaired z-test",
    "text": "Two sample unpaired z-test\n\\[\n\\begin{equation} z = \\dfrac{\\bar{x_1} - \\bar{x_2}}{\\sqrt{\\dfrac{\\sigma_1^2}{n_1} +\\dfrac{\\sigma_2^2}{n_2}}} \\end{equation}\n\\tag{2}\\]\nWhere \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) are the sample means, \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are the population variances, and \\({n_1}\\) and \\({n_2}\\) are the number of samples.\n\nExample code in R\n\nsuppressMessages(library(PASWR2))\n\n# Calculate population standard deviations\nsig_x &lt;- dplyr::filter(dat, Sex == \"Women\", Year == 1985) %&gt;%\n  dplyr::pull(BMI) %&gt;% na.omit() %&gt;% sd()\nsig_y &lt;- dplyr::filter(dat, Sex == \"Women\", Year == 2017) %&gt;%\n  dplyr::pull(BMI) %&gt;% na.omit() %&gt;% sd()\n\n# Fetch a random sample of BMI data for women in the year 1985 and 2017\nx1 &lt;- dplyr::filter(dat, Sex == \"Women\", Year == 1985) %&gt;%\n  dplyr::pull(BMI) %&gt;% na.omit() %&gt;%\n  sample(.,300)\nx2 &lt;- dplyr::filter(dat, Sex == \"Women\", Year == 2017) %&gt;%\n  dplyr::pull(BMI) %&gt;% na.omit() %&gt;%\n  sample(.,300)\n\n# Perform a two sample (unpaired) t-test between x1 and x2\n(z_res &lt;- PASWR2::z.test(x1, x2, mu = 0, sigma.x = sig_x, sigma.y = sig_y))\n\n\n    Two Sample z-test\n\ndata:  x1 and x2\nz = -11.071, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.885719 -2.017614\nsample estimates:\nmean of x mean of y \n 24.08067  26.53233 \n\n\n\n# Fetch z-test result metrics and present them in a tidy table\nbroom::tidy(z_res) %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n\nestimate1\nestimate2\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n24.08067\n26.53233\n-11.0705\n0\n-2.885719\n-2.017614\nTwo Sample z-test\ntwo.sided"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-paired-z-test",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-paired-z-test",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Two sample paired z-test",
    "text": "Two sample paired z-test\n\\[\n\\begin{equation} z= \\dfrac{\\bar{d}- D}{\\sqrt{\\dfrac{\\sigma_d^2}{n}}} \\end{equation}\n\\tag{3}\\]\nWhere \\(\\bar{d}\\) is the mean of the differences between the samples, \\(D\\) is the hypothesised mean of the differences (usually this is zero), \\(n\\) is the sample size and \\(\\sigma_d^2\\) is the population variance of the differences.\n\nExample code in R\n\n# Fetch a first 300 samples of BMI data for women in the year 1985 and 2017\nx1 &lt;- dplyr::filter(dat, Sex == \"Women\", Year == 1985) %&gt;%\n  dplyr::pull(BMI) %&gt;% na.omit() \nx1 &lt;- x1[1:300]\nx2 &lt;- dplyr::filter(dat, Sex == \"Women\", Year == 2017) %&gt;%\n  dplyr::pull(BMI) %&gt;% na.omit()\nx2 &lt;- x2[1:300]\n\n# Perform a two sample (unpaired) t-test between x1 and x2\n(z_res &lt;- PASWR2::z.test(x1, x2, sigma.x = sig_x, sigma.y = sig_y, sigma.d = abs(sig_y - sig_x), paired = TRUE))\n\n\n    Paired z-test\n\ndata:  x1 and x2\nz = -393.33, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.36171 -2.33829\nsample estimates:\nmean of the differences \n                  -2.35 \n\n\n\n# Fetch z-test result metrics and present them in a tidy table\nbroom::tidy(z_res) %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-2.35\n-393.3333\n0\n-2.36171\n-2.33829\nPaired z-test\ntwo.sided"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-unpaired-independent-t-test",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-unpaired-independent-t-test",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Two sample unpaired (independent) t-test",
    "text": "Two sample unpaired (independent) t-test\n\\[\n\\begin{equation} t = \\dfrac{\\bar{x_1} - \\bar{x_2}}{\\sqrt{s^2\\bigg(\\dfrac{1}{n_1} + \\dfrac{1}{n_2}\\bigg)}} \\end{equation}\n\\tag{5}\\]\nWhere the pooled standard deviation \\(s\\) is:\n\\[\n\\begin{equation} s =\\sqrt{\\dfrac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 +n_2 -2}} \\end{equation}\n\\tag{6}\\]\nWhere \\(\\bar x_1\\) and \\(\\bar x_2\\) are the means from the two samples, likewise \\(n_1\\) and \\(n_2\\) are the sample sizes and \\(s_1^2\\) and \\(s_2^2\\) are the sample variances. This test statistic you will compare to t-tables on \\((n1+n2−2)\\) degrees of freedom.\n\nExample code for a two-tailed test in R\n\n# Fetch the BMI data for women from rural and urban areas in the year 1985\nx1 &lt;- dplyr::filter(dat, Sex == \"Women\", Region == \"Rural\", Year == 1985) %&gt;%\n  dplyr::pull(BMI) \nx2 &lt;- dplyr::filter(dat, Sex == \"Women\", Region == \"Urban\", Year == 1985) %&gt;%\n  dplyr::pull(BMI) \n\n# Perform a two sample (unpaired) t-test between x1 and x2\n(t_res &lt;- t.test(x1, x2, var.equal = TRUE))\n\n\n    Two Sample t-test\n\ndata:  x1 and x2\nt = -3.8952, df = 394, p-value = 0.0001152\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.5744694 -0.5182378\nsample estimates:\nmean of x mean of y \n 23.58782  24.63417 \n\n\nWe use the var.equal = TRUE option here to use the pooled standard deviation \\(s\\) Equation 6.\n\n\n\n\n\n\nNote\n\n\n\nFor the pooled variance t-test to be appropriate you rely on the assumption that the two samples come from the same population and have equal variance. However, a modification of the t-test known as Welch’s test is said to correct for this problem by estimating the variances, and adjusting the degrees of freedom to use in the test. This correction is performed by default, but can be shut off by using the var.equal=TRUE argument as used in the example above.\n\n\n\n# Fetch t-test result metrics and present them in a tidy table\nbroom::tidy(t_res) %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-1.046354\n23.58782\n24.63417\n-3.895234\n0.0001152\n394\n-1.574469\n-0.5182378\nTwo Sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\nExample code for a one-tailed test in R\nHere we will test if women in rural areas had a higher BMI than those in urban areas in 1985.\n\n# Perform a one-tailed two sample (unpaired) t-test between x1 and x2\n(t_res &lt;- t.test(x1, x2, var.equal = TRUE, alternative = \"greater\"))\n\n\n    Two Sample t-test\n\ndata:  x1 and x2\nt = -3.8952, df = 394, p-value = 0.9999\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -1.489242       Inf\nsample estimates:\nmean of x mean of y \n 23.58782  24.63417 \n\n\n\n# Fetch t-test result metrics and present them in a tidy table\nbroom::tidy(t_res) %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-1.046354\n23.58782\n24.63417\n-3.895234\n0.9999424\n394\n-1.489242\nInf\nTwo Sample t-test\ngreater"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-paired-dependent-t-test",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-paired-dependent-t-test",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Two sample paired (dependent) t-test",
    "text": "Two sample paired (dependent) t-test\n\\[\n\\begin{equation} t = \\dfrac{\\bar{d}}{\\sqrt{\\dfrac{s^2}{n}}} \\end{equation}\n\\tag{7}\\]\nWhere \\(\\bar d\\) is the mean of the differences between the samples. You will compare the t-statistic to the critical values in a t-table on \\((n−1)\\) degrees of freedom. Here \\(s\\) is the standard deviation of the differences.\n\nExample code in R\n\n# Perform a two sample (paired) t-test between x1 and x2\n(t_res &lt;- t.test(x1, x2, paired = TRUE))\n\n\n    Paired t-test\n\ndata:  x1 and x2\nt = -14.095, df = 195, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.1870263 -0.8956268\nsample estimates:\nmean difference \n      -1.041327 \n\n\n\n# Fetch t-test result metrics and present them in a tidy table\nbroom::tidy(t_res) %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-1.041327\n-14.09549\n0\n195\n-1.187026\n-0.8956268\nPaired t-test\ntwo.sided"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#example-code-in-r-4",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#example-code-in-r-4",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Example code in R",
    "text": "Example code in R\n\n(f_res &lt;- var.test(x1, x2))\n\n\n    F test to compare two variances\n\ndata:  x1 and x2\nF = 1.3238, num df = 196, denom df = 198, p-value = 0.04957\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 1.000525 1.751931\nsample estimates:\nratio of variances \n          1.323819 \n\n\n\n# Fetch f-test result metrics and present them in a tidy table\nbroom::tidy(f_res) %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\nMultiple parameters; naming those columns num.df, den.df\n\n\n\n\n\nestimate\nnum.df\nden.df\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n1.323819\n196\n198\n1.323819\n0.0495733\n1.000525\n1.751931\nF test to compare two variances\ntwo.sided"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#websites",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#websites",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Website(s)",
    "text": "Website(s)\n\nParametric Hypothesis Tests (Psychology)"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-11-17 First draft"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#import-packages",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#import-packages",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "Import packages",
    "text": "Import packages\n\nimport Pkg\nPkg.activate(\".\")\nusing CSV\nusing DataFrames\nusing Statistics\nusing HypothesisTests\n\n  Activating project at `~/sandbox/dataalltheway/posts/010-01-parametric-hypothesis-tests-julia`"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-unpaired-z-test",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-unpaired-z-test",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "Two sample unpaired z-test",
    "text": "Two sample unpaired z-test\n\nuneqvarztest = let\n    # Fetch a random sample of BMI data for women in the year 1985 and 2017\n    x1 = filter([:Sex, :Year] =&gt; (s, y) -&gt; s==\"Women\" && y==1985 , data) |&gt;\n        x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    x2 = filter([:Sex, :Year] =&gt; (s, y) -&gt; s==\"Women\" && y==2017 , data) |&gt;\n        x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    UnequalVarianceZTest(x1, x2)\nend\n\nTwo sample z-test (unequal variance)\n------------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          -2.26\n    95% confidence interval: (-2.679, -1.841)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           &lt;1e-25\n\nDetails:\n    number of observations:   [300,300]\n    z-statistic:              -10.560590588866509\n    population standard error: 0.21400318296427412"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-paired-z-test",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-paired-z-test",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "Two sample paired z-test",
    "text": "Two sample paired z-test\n\neqvarztest = let\n    # Fetch a random sample of BMI data for women in the year 1985 and 2017\n    x1 = filter([:Sex, :Year] =&gt; (s, y) -&gt; s==\"Women\" && y==1985 , data) |&gt;\n        x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    x2 = filter([:Sex, :Year] =&gt; (s, y) -&gt; s==\"Women\" && y==2017 , data) |&gt;\n        x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    EqualVarianceZTest(x1, x2)\nend\n\nTwo sample z-test (equal variance)\n----------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          -2.173\n    95% confidence interval: (-2.611, -1.735)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           &lt;1e-21\n\nDetails:\n    number of observations:   [300,300]\n    z-statistic:              -9.724414586039652\n    population standard error: 0.22345818154642977"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#one-sample-t-test",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#one-sample-t-test",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "One sample t-test",
    "text": "One sample t-test\n\nonesamplettest = let \n    x1 = filter(\n        [:Sex, :Region, :Year] =&gt; \n            (s, r, y) -&gt; s==\"Men\" && r==\"Rural\" && y == 2017,\n        data\n    ) |&gt;\n    x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    OneSampleTTest(x1, 24.5)\nend\n\nOne sample t-test\n-----------------\nPopulation details:\n    parameter of interest:   Mean\n    value under h_0:         24.5\n    point estimate:          25.466\n    95% confidence interval: (25.16, 25.77)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           &lt;1e-08\n\nDetails:\n    number of observations:   300\n    t-statistic:              6.280721563263261\n    degrees of freedom:       299\n    empirical standard error: 0.15380398418714467"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-unpaired-independent-t-test",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-unpaired-independent-t-test",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "Two sample unpaired (independent) t-test",
    "text": "Two sample unpaired (independent) t-test\n\nunpairedtwosamplettest = let \n    x1 = filter([:Sex, :Region, :Year] =&gt; \n            (s, r, y) -&gt; s==\"Women\" && r==\"Rural\" && y == 1985,\n        data) |&gt;\n        x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    x2 = filter([:Sex, :Region, :Year] =&gt; \n            (s, r, y) -&gt; s==\"Women\" && r==\"Urban\" && y == 1985,\n        data) |&gt;\n        x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    UnequalVarianceTTest(x1, x2)\nend\n\nTwo sample t-test (unequal variance)\n------------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          -1.05867\n    95% confidence interval: (-1.512, -0.6054)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           &lt;1e-05\n\nDetails:\n    number of observations:   [300,300]\n    t-statistic:              -4.587387795167387\n    degrees of freedom:       575.968012373301\n    empirical standard error: 0.2307776699807073\n\n\n\n\n\n\n\n\nWelch’s Test\n\n\n\nThis test uses the Welch correction, and there is no way to turn it off in HypothesisTests.jl.\n\n\n\nOnly considering right tailed (one-tailed)\n\nunpairedtwosamplettest = let \n    x1 = filter([:Sex, :Region, :Year] =&gt; \n            (s, r, y) -&gt; s==\"Women\" && r==\"Rural\" && y == 1985,\n        data) |&gt;\n        x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    x2 = filter([:Sex, :Region, :Year] =&gt; \n            (s, r, y) -&gt; s==\"Women\" && r==\"Urban\" && y == 1985,\n        data) |&gt;\n        x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    UnequalVarianceTTest(x1, x2)\nend\npvalue(unpairedtwosamplettest, tail=:right)\n\n0.9999999445762"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-paired-dependent-t-test",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-paired-dependent-t-test",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "Two sample paired (dependent) t-test",
    "text": "Two sample paired (dependent) t-test\n\npairedtwosamplettest = let \n    x1 = filter([:Sex, :Region, :Year] =&gt; \n            (s, r, y) -&gt; s==\"Women\" && r==\"Rural\" && y == 1985,\n        data) |&gt;\n        x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    x2 = filter([:Sex, :Region, :Year] =&gt; \n            (s, r, y) -&gt; s==\"Women\" && r==\"Urban\" && y == 1985,\n        data) |&gt;\n        x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    x -&gt; x[!, :BMI] |&gt; skipmissing |&gt; collect |&gt; x-&gt;rand(x, 300)\n    EqualVarianceTTest(x1, x2)\nend\n\nTwo sample t-test (equal variance)\n----------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          -1.01167\n    95% confidence interval: (-1.44, -0.5838)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           &lt;1e-05\n\nDetails:\n    number of observations:   [300,300]\n    t-statistic:              -4.64337449574737\n    degrees of freedom:       598\n    empirical standard error: 0.2178731583233696"
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html",
    "href": "posts/008-build-a-singularity-container/index.html",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "",
    "text": "Singularity is a free and open-source container platform foroperating-system-level virtualization. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Singularity on your laptop (preferably Linux) and then run it on your local computer or a High-Performance Computer (HPC). A Singularity container is a single file that is easy to ship to an HPC or a friend.\nI heavily rely on Singularity for my work as I write arbitrary code that runs on an HPC. I often require a specific set of libraries, compilers, or other supporting software that are often hard to manage on an HPC. Also, from a reproducibility point of view, it’s easier to build a container with a fixed library and software version packaged in a single file than scattered in multiple modules on the HPC. And when the time comes for the publication, I can easily share the container on Zenodo, etc.\nThe title of this post emphasizes data science, machine learning, and chemistry because all the software we will install is related to these disciplines. However, this procedure applies to building containers for any field of application.\n\n\n\nBuild a Linux based Singularity container.\n\nFirst build a writable sandbox with essential elements.\nInspect the container.\nInstall additional software.\nConvert the sandbox to a read-only SquashFS container image.\n\nInstall software & packages from multiple sources.\n\nUsing apt-get package management system.\nCompiling from source code.\nUsing Python pip.\nUsing install.packages() function in R.\n\nSoftware highlight.\n\nJupyter notebook.\nTensorflow GPU version.\nOpenMPI.\nPopular datascience packages in Python and R.\nChemistry/chemoinformatics software: RDkit, OpenBabel, Pybel, & Mordred.\n\nTest the container.\n\nTest the GPU version of Tensorflow."
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html#learning-objectives",
    "href": "posts/008-build-a-singularity-container/index.html#learning-objectives",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "",
    "text": "Build a Linux based Singularity container.\n\nFirst build a writable sandbox with essential elements.\nInspect the container.\nInstall additional software.\nConvert the sandbox to a read-only SquashFS container image.\n\nInstall software & packages from multiple sources.\n\nUsing apt-get package management system.\nCompiling from source code.\nUsing Python pip.\nUsing install.packages() function in R.\n\nSoftware highlight.\n\nJupyter notebook.\nTensorflow GPU version.\nOpenMPI.\nPopular datascience packages in Python and R.\nChemistry/chemoinformatics software: RDkit, OpenBabel, Pybel, & Mordred.\n\nTest the container.\n\nTest the GPU version of Tensorflow."
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html#inspect-container",
    "href": "posts/008-build-a-singularity-container/index.html#inspect-container",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "Inspect Container",
    "text": "Inspect Container\nTo get a list of the labels defined for the container singularity inspect --labels container/\nTo print the container’s help section singularity inspect --helpfile container/\nTo show container’s environment singularity inspect --environment container/\nTo retrieve the definition file used to build the container singularity inspect --deffile container/"
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html#install-data-science-and-chemistry-packages",
    "href": "posts/008-build-a-singularity-container/index.html#install-data-science-and-chemistry-packages",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "Install Data Science and Chemistry Packages",
    "text": "Install Data Science and Chemistry Packages\nOnce the core writable sandbox is built we will install the additional data science and chemistry packages.\nTo do that execute:\nsudo singularity shell --writable container/\nThen execute the following lines in the shell environment.\n    # Install Python packages.\n        python3 -m pip --no-cache-dir install numpy pandas h5py pyarrow sklearn statsmodels matplotlib seaborn plotly \n\n    # Install Tensorflow.\n        python3 -m pip --no-cache-dir install tensorflow==2.2.0 \n\n    # Install R packages.\n        R --quiet --slave -e 'install.packages(\"tidyverse\", version = \"1.3.0\", repos=\"https://cloud.r-project.org/\")'\n        R --quiet --slave -e 'install.packages(\"tidymodels\", version = \"0.1.0\", repos=\"https://cloud.r-project.org/\")'\n        R --quiet --slave -e 'install.packages(c(\"lme4\", \"glmnet\", \"yaml\", \"jsonlite\", \"rlang\"), repos=\"https://cloud.r-project.org/\")'\n\n    # Install RDKit\n        export RDBASE=/usr/local/share/rdkit\n        export LD_LIBRARY_PATH=\"$RDBASE/lib:$LD_LIBRARY_PATH\"\n        export PYTHONPATH=\"$RDBASE:$PYTHONPATH\"\n        mkdir -p /tmp/rdkit\n        cd /tmp/rdkit\n        wget https://github.com/rdkit/rdkit/archive/2020_03_3.tar.gz\n        tar zxf 2020_03_3.tar.gz\n        mv rdkit-2020_03_3 $RDBASE\n        mkdir $RDBASE/build\n        cd $RDBASE/build\n        cmake -DPYTHON_EXECUTABLE=/usr/bin/python3 ..\n        make -j $(nproc)\n        make install\n\n        ln -s /usr/local/share/rdkit/rdkit /usr/local/lib/python3.6/dist-packages/\n\n    # Install OpenBabel.\n        apt-get -qq -y update\n        apt-get -qq install -y --no-install-recommends openbabel python-openbabel\n\n    # Install Mordred Molecular Descriptor Calculator.\n        python3 -m pip --no-cache-dir install mordred\n\n    # Cleanup\n        rm -rf /tmp/rdkit"
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html#convert-a-writable-sandbox-to-a-read-only-compressed-container",
    "href": "posts/008-build-a-singularity-container/index.html#convert-a-writable-sandbox-to-a-read-only-compressed-container",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "Convert a Writable Sandbox to a Read Only Compressed Container",
    "text": "Convert a Writable Sandbox to a Read Only Compressed Container\nOnce you are satisfied that you have installed all the required packages you can convert the writable sandbox to a read only squashfs filesystem. Squashfs is a compressed read-only file system for Linux.\nsudo singularity build container.sif container/"
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html#install-kernel-spces-for-jupyter-notebook-for-r",
    "href": "posts/008-build-a-singularity-container/index.html#install-kernel-spces-for-jupyter-notebook-for-r",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "Install Kernel Spces for Jupyter Notebook for R",
    "text": "Install Kernel Spces for Jupyter Notebook for R\nKernel specs are installed from outside the container in the host’s home environment.\nsingularity exec container.sif R --quiet --slave -e 'IRkernel::installspec()'\nNOTE: You only have to do it once per host to install kernelspec."
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html#tensorflow-gpu",
    "href": "posts/008-build-a-singularity-container/index.html#tensorflow-gpu",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "Tensorflow GPU",
    "text": "Tensorflow GPU\nimport tensorflow as tf\n\ntf.debugging.set_log_device_placement(True)\ngpus = tf.config.list_physical_devices('GPU')\n\nif gpus:\n    with tf.device('/GPU:0'):\n        tf.random.set_seed(123)\n        a = tf.random.normal([10000,20000], 0, 1, tf.float32, seed=1)\n        b = tf.random.normal([20000,10000], 0, 1, tf.float32, seed=1)\n        c = tf.matmul(a, b)\n        print(c)\nelse:\n    print(\"No GPUs found.\")\n\nprint(\"Num GPUs:\", len(gpus))\nTo execute the script singularity exec --nv container.sif python3 tf_gpu.py\nTo monitor NVIDIA GPU usage nvidia-smi"
  },
  {
    "objectID": "posts/006-google-colab-for-scientific-software/index.html",
    "href": "posts/006-google-colab-for-scientific-software/index.html",
    "title": "A case for using Google Colab notebooks as an alternative to web servers for scientific software",
    "section": "",
    "text": "Updates\n\n\n\n\n\n2022-10-18 Typo correction and included a list of links to learn more about Google Colab.\n2022-10-20 The title and description changed. A PDF version of the article is uploaded to Zenodo at https://doi.org/10.5281/zenodo.7232109\n\n\n\nI recently came across ColabFold (Mirdita et al. 2022), a slimmer and faster implementation of AlphaFold2 (Jumper et al. 2021) (the famous protein structure prediction software from DeepMind) implemented on Google Colab in the form of a Jupyter notebook, giving it an easy-to-use web server-like interface. I found this idea intriguing as it removes the overhead of maintaining a webserver while providing a web-based graphical user interface.\nGoogle Colab is a free (with options for pro subscriptions) Jupyter notebook environment for Python (R indirectly) provided by Google that runs on unoccupied Google servers. This free resource also includes access to GPU and TPU making it attractive to various machine learning and data science tasks. For the most part, Google Colab is utilized in machine learning and data science education. However, following the example of ColabFold and my implementation of ColabHDStIM, I want to make a case that it can also be used for providing an easy-to-use interface or live demo for scientific software without maintaining the complex infrastructure of a web server.\nComing from a bioinformatics/computational biology background, I know there is a craze for developing web servers worldwide. However, although many web servers are created yearly, many groups, especially in developing countries, lack the resources to build one. On the flip side, many of these initially well-funded web servers are either of low quality, are not kept updated, or go offline soon after the publication, thus squandering the resources (Veretnik, Fink, and Bourne 2008; Schultheiss et al. 2011; Kern, Fehlmann, and Keller 2020). Therefore, there is a need for an alternative where scientists can distribute their software in an easy-to-use interface like interactive notebooks. Even if the notebook environments are limited in executing production-scale software, they can still be utilized to provide a live demo on a minimal dataset. In my opinion, it is better than the vignettes accompanying software. \nBelow are some pros and cons of using Google Colab.\nPros\n\nEasy to implement\nFree hardware resources from Google, including GPU and TPU\nOption to buy more resources from Google as per need\nWhile hosted on Google’s server, the same notebook can be executed using a local runtime to take advantage of local hardware resources.\nForkable and hackable if the original maintainer stops the development.\n\nCons\n\nFree hardware resources can be limiting\nUploading and downloading data to a Colab is slow and require a workaround\nAll the instances are transient; therefore, on every restart, all the required software is re-installed, which takes time.\nColab notebooks are meant to run interactively; therefore, maintaining a long background session is hard or impossible.\nColab primarily supports Python and requires workarounds to support other languages.\n\nLearn more about Google Colab\n\nGoogle Colab frequently asked questions\nWelcome to Colab!\nPractical introduction to Google Colab for data science (YouTube video)\n\nReferences\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nKern, Fabian, Tobias Fehlmann, and Andreas Keller. 2020. “On the Lifetime of Bioinformatics Web Services.” Nucleic Acids Research 48 (22): 12523–33. https://doi.org/10.1093/nar/gkaa1125.\n\n\nMirdita, Milot, Konstantin Schütze, Yoshitaka Moriwaki, Lim Heo, Sergey Ovchinnikov, and Martin Steinegger. 2022. “ColabFold: Making Protein Folding Accessible to All.” Nature Methods 19 (6): 679–82. https://doi.org/10.1038/s41592-022-01488-1.\n\n\nSchultheiss, Sebastian J., Marc-Christian Münch, Gergana D. Andreeva, and Gunnar Rätsch. 2011. “Persistence and Availability of Web Services in Computational Biology.” Edited by Dongxiao Zhu. PLoS ONE 6 (9): e24914. https://doi.org/10.1371/journal.pone.0024914.\n\n\nVeretnik, Stella, J. Lynn Fink, and Philip E. Bourne. 2008. “Computational Biology Resources Lack Persistence and Usability.” Edited by Barbara Bryant. PLoS Computational Biology 4 (7): e1000136. https://doi.org/10.1371/journal.pcbi.1000136.\n\n\n\n\n\nCitationBibTeX citation:@misc{farmer2022,\n  author = {Farmer, Rohit},\n  publisher = {Zenodo},\n  title = {A Case for Using {Google} {Colab} Notebooks as an Alternative\n    to Web Servers for Scientific Software},\n  date = {2022-10-17},\n  url = {https://doi.org/10.5281/zenodo.7232109},\n  doi = {10.5281/zenodo.7232109},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFarmer, Rohit. 2022. “A Case for Using Google Colab Notebooks as\nan Alternative to Web Servers for Scientific Software.” Zenodo.\nhttps://doi.org/10.5281/zenodo.7232109."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "",
    "text": "Note: This tutorial is written for Linux based systems."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#requirements",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#requirements",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Requirements",
    "text": "Requirements\n\nR &gt;= 3.0.0\nTo install the latest version of R please follow the download and install instructions at https://cloud.r-project.org/\n\n\nNeovim &gt;= 0.2.0\nNeovim (nvim) is the continuation and extension of Vim editor with the aim to keep the good parts of Vim and add more features. In this tutorial I will be using Neovim (nvim), however, most of the steps are equally applicable to Vim also. Please follow download and installation instructions on nvim’s GitHub wiki https://github.com/neovim/neovim/wiki/Installing-Neovim.\nOR\n\n\nVim &gt;= 8.1\nVim usually comes installed in most of the Linux based operating system. However, it may not be the latest one. Therefore, to install the latest version please download and install it from Vim’s GitHub repository as mentioned below or a method that is more confortable to you.\ngit clone https://github.com/vim/vim.git\nmake -C vim/\nsudo make install -C vim/\n\n\nPlugin Manager\nThere are more than one plugin manager’s available for Vim that can be used to install the required plugins. In this tutorial I will be using vim-plug pluggin manager.\n\n\nPlugins\nIn the end below are the plugins that we would need to convert Vim editor into a fully functional IDE for R.\n\nNvim-R: https://github.com/jalvesaq/Nvim-R\n\nNvim-R is the main plugin that will add the functionality to execute R code from within the Vim editor.\n\nNcm-R: https://github.com/gaalcaras/ncm-R\n\nNcm-R adds synchronous auto completion features for R.\nIt is based on ncm2 and nvim-yarp plugins.\n\nNerd Tree: https://github.com/preservim/nerdtree\n\nNerd Tree will be used to toggle file explorer in the side panel.\n\nDelimitMate: https://github.com/Raimondi/delimitMate\n\nThis plug-in provides automatic closing of quotes, parenthesis, brackets, etc.\n\nVim-monokai-tasty: https://github.com/patstockwell/vim-monokai-tasty\n\nMonokai color scheme inspired by Sublime Text’s interpretation of monokai.\n\nLightline.vim: https://github.com/itchyny/lightline.vim\n\nLineline.vim adds asthetic enhancements to Vim’s statusline/tabline."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#procedure",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#procedure",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Procedure",
    "text": "Procedure\n\nMake sure that you have R &gt;=3.0.0 installed.\nMake sure that you have Neovim &gt;= 0.2.0 installed.\nInstall the vim-plug plugin manager.\n\ncurl -fLo ~/.local/share/nvim/site/autoload/plug.vim --create-dirs \\\n    https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n\nInstall the required plugins.\n\nFirst, create an init.vim file in ~/.config/nvim folder (create the folder if it doesn’t exist). This file is equivalent to a .vimrc file in the traditional Vim environment. To init.vim file start adding:\n\" Specify a directory for plugins\n\" - Avoid using standard Vim directory names like 'plugin'\ncall plug#begin('~/.vim/plugged')\n\n\" List of plugins.\n\" Make sure you use single quotes\n\n\" Shorthand notation\nPlug 'jalvesaq/Nvim-R'\nPlug 'ncm2/ncm2'\nPlug 'roxma/nvim-yarp'\nPlug 'gaalcaras/ncm-R'\nPlug 'preservim/nerdtree'\nPlug 'Raimondi/delimitMate'\nPlug 'patstockwell/vim-monokai-tasty'\nPlug 'itchyny/lightline.vim'\n\n\" Initialize plugin system\ncall plug#end()\n\nUpdate and add more features to the init.vim file.\n\n\" Set a Local Leader\n\n\" With a map leader it's possible to do extra key combinations\n\" like &lt;leader&gt;w saves the current file\nlet mapleader = \",\"\nlet g:mapleader = \",\"\n\n\n\" Plugin Related Settings\n\n\" NCM2\nautocmd BufEnter * call ncm2#enable_for_buffer()    \" To enable ncm2 for all buffers.\nset completeopt=noinsert,menuone,noselect           \" :help Ncm2PopupOpen for more\n                                                    \" information.\n\n\" NERD Tree\nmap &lt;leader&gt;nn :NERDTreeToggle&lt;CR&gt;                  \" Toggle NERD tree.\n\n\" Monokai-tasty\nlet g:vim_monokai_tasty_italic = 1                  \" Allow italics.\ncolorscheme vim-monokai-tasty                       \" Enable monokai theme.\n\n\" LightLine.vim \nset laststatus=2              \" To tell Vim we want to see the statusline.\nlet g:lightline = {\n   \\ 'colorscheme':'monokai_tasty',\n   \\ }\n\n\n\" General NVIM/VIM Settings\n\n\" Mouse Integration\nset mouse=i                   \" Enable mouse support in insert mode.\n\n\" Tabs & Navigation\nmap &lt;leader&gt;nt :tabnew&lt;cr&gt;    \" To create a new tab.\nmap &lt;leader&gt;to :tabonly&lt;cr&gt;     \" To close all other tabs (show only the current tab).\nmap &lt;leader&gt;tc :tabclose&lt;cr&gt;    \" To close the current tab.\nmap &lt;leader&gt;tm :tabmove&lt;cr&gt;     \" To move the current tab to next position.\nmap &lt;leader&gt;tn :tabn&lt;cr&gt;        \" To swtich to next tab.\nmap &lt;leader&gt;tp :tabp&lt;cr&gt;        \" To switch to previous tab.\n\n\n\" Line Numbers & Indentation\nset backspace=indent,eol,start  \" To make backscape work in all conditions.\nset ma                          \" To set mark a at current cursor location.\nset number                      \" To switch the line numbers on.\nset expandtab                   \" To enter spaces when tab is pressed.\nset smarttab                    \" To use smart tabs.\nset autoindent                  \" To copy indentation from current line \n                                \" when starting a new line.\nset si                          \" To switch on smart indentation.\n\n\n\" Search\nset ignorecase                  \" To ignore case when searching.\nset smartcase                   \" When searching try to be smart about cases.\nset hlsearch                    \" To highlight search results.\nset incsearch                   \" To make search act like search in modern browsers.\nset magic                       \" For regular expressions turn magic on.\n\n\n\" Brackets\nset showmatch                   \" To show matching brackets when text indicator \n                                \" is over them.\nset mat=2                       \" How many tenths of a second to blink \n                                \" when matching brackets.\n\n\n\" Errors\nset noerrorbells                \" No annoying sound on errors.\n\n\n\" Color & Fonts\nsyntax enable                   \" Enable syntax highlighting.\nset encoding=utf8                \" Set utf8 as standard encoding and \n                                 \" en_US as the standard language.\n\n\" Enable 256 colors palette in Gnome Terminal.\nif $COLORTERM == 'gnome-terminal'\n    set t_Co=256\nendif\n\ntry\n    colorscheme desert\ncatch\nendtry\n\n\n\" Files & Backup\nset nobackup                     \" Turn off backup.\nset nowb                         \" Don't backup before overwriting a file.\nset noswapfile                   \" Don't create a swap file.\nset ffs=unix,dos,mac             \" Use Unix as the standard file type.\n\n\n\" Return to last edit position when opening files\nau BufReadPost * if line(\"'\\\"\") &gt; 1 && line(\"'\\\"\") &lt;= line(\"$\") | exe \"normal! g'\\\"\" | endif"
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#frequently-used-keyboard-shortcutscommands",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#frequently-used-keyboard-shortcutscommands",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Frequently Used Keyboard Shortcuts/Commands",
    "text": "Frequently Used Keyboard Shortcuts/Commands\nNote: The commands below are according to the init.vim settings mentioned in this Gist.\n# Nvim-R\n\\rf               \" Connect to R console.\n\\rq               \" Quit R console.\n\\ro               \" Open object bowser.\n\\d                \" Execute current line of code and move to the next line.\n\\ss               \" Execute a block of selected code.\n\\aa               \" Execute the entire script. This is equivalent to source().\n\\xx               \" Toggle comment in an R script.\n\n# NERDTree\n,nn               \" Toggle NERDTree."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#example-code",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#example-code",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Example Code",
    "text": "Example Code\nlibrary(tidyverse)\n# \\rf               \" Connect to R console.\n# \\rq               \" Quit R console.\n# \\ro               \" Open object bowser.\n# \\d \\ss \\aa        \" Execution modes. \n# ?help\n# ,nn               \" NERDTree.\n# ,nt, tp, tn       \" Tab navigation.\n\ntheme_set(theme_bw())\ndata(\"midwest\", package = \"ggplot2\")\n\ngg  &lt;- ggplot(midwest, aes(x=area, y = poptotal)) +\n        geom_point(aes(col = state, size = popdensity)) +\n        geom_smooth(method = \"loess\", se = F) +\n        xlim(c(0, 0.1)) +\n        ylim(c(0, 500000)) +\n        labs(subtitle = \"Area Vs Population\",\n             y = \"Population\",\n             x = \"Area\",\n             title = \"Scatterplot\",\n             caption = \"Source: midwest\")\n\nplot(gg) # Opens an external window with the plot.\n\nmidwest$county # To show synchronous auto completion. \n\nView(midwest) # Opens an external window to display a portion of the tibble."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#add-colour-etc.-to-vim-in-a-screen-session-optional",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#add-colour-etc.-to-vim-in-a-screen-session-optional",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Add Colour etc. to VIM in a Screen Session (optional)",
    "text": "Add Colour etc. to VIM in a Screen Session (optional)\nAdd these lines to ~/.screenrc file.\n# Use 256 colors\nattrcolor b \".I\"    # allow bold colors - necessary for some reason\ntermcapinfo xterm 'Co#256:AB=\\E[48;5;%dm:AF=\\E[38;5;%dm'   # tell screen how to set colors. AB = background, AF=foreground\ndefbce on    # use current bg color for erased chars]]'\n\n# Informative statusbar\nhardstatus off\nhardstatus alwayslastline\nhardstatus string '%{= kG}[ %{G}%H %{g}][%= %{= kw}%?%-Lw%?%{r}(%{W}%n*%f%t%?(%u)%?%{r})%{w}%?%+Lw%?%?%= %{g}][%{B} %m-%d %{W} %c %{g}]'\n\n# Use X scrolling mechanism\ntermcapinfo xterm* ti@:te@\n\n# Fix for residual editor text\naltscreen on"
  },
  {
    "objectID": "posts/002-tweets-from-heads-of-governments-and-states/index.html",
    "href": "posts/002-tweets-from-heads-of-governments-and-states/index.html",
    "title": "Tweets from heads of governments and states",
    "section": "",
    "text": "Since October 2018, I have been maintaining a bot written in Python and running on a Raspberry Pi 3B+ that collects tweets from heads of governments (worldwide) followed by https://twitter.com/headoffice. It was an excellent exercise learning Python, Twitter API, SQLite database, and using a Raspberry Pi for hobby projects. I have now released the data on Kaggle at https://doi.org/10.34740/KAGGLE/DSV/4208877 for the community to use.\nThe dataset contains an Excel workbook per year with data points on the rows and features on the columns. Features include the timestamp (UTC), language in which the tweet is written, user id, user name, tweet id, and tweet text. The first version includes the data from October 2018 until September 15, 2022. After that, future releases will be quarterly. It is a textual dataset and is primarily useful for analyses related to natural language processing.\nIn the Kaggle submission, I have also included a notebook (https://www.kaggle.com/code/rohitfarmer/dont-run-tweet-collection-and-preprocessing) with the Python code that collected the tweets and the additional code that I used to pre-process the data before submission. After releasing the first data set, I updated the code and moved the bot from Python to R using the rtweet library instead of tweepy. I found rtweet to perform better, especially in filtering out duplicated tweets.\nIn the current setup (https://github.com/rohitfarmer/government-tweets) that is still running on my Raspberry Pi 3B+, the main bot script runs every fifteen minutes via crontab and fetches data that is more recent than the latest tweet collected in the previous run. The data is stored in an SQLite database which is backed up to MEGA cloud storage via Rclone once every midnight ET.\nI enjoyed the process of creating the bot and being able to run it for a couple of years, and I hope I will soon find some time to look into the data and fetch some exciting insights. But, until then, the data is available to the data science community to utilize as they please. So, please open a discussion on the Kaggle page for questions, comments, or collaborations.\n\n\n\nCitationBibTeX citation:@dataset{farmer2022,\n  author = {Farmer, Rohit},\n  publisher = {Kaggle},\n  title = {Tweets from Heads of Governments and States},\n  date = {2022-10-05},\n  url = {https://www.kaggle.com/dsv/4208877},\n  doi = {10.34740/KAGGLE/DSV/4208877},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFarmer, Rohit. 2022. “Tweets from Heads of Governments and\nStates.” Kaggle. https://doi.org/10.34740/KAGGLE/DSV/4208877."
  },
  {
    "objectID": "bookmarks/index.html",
    "href": "bookmarks/index.html",
    "title": "Bookmarks",
    "section": "",
    "text": "Below are some of the articles, blog posts, and stack exchange threads that I have found helpful. Please comment if you know or found something interesting that I should list here.\n\nCheat sheets\n\nChoosing the right estimator\nMachine learning cheat sheet\nMachine learning glossary\nThe neural network zoo\n\n\n\nData processing\n\nNormalize data before or after split of training and testing data?\n\n\n\nR\n\nBook: Advanced R by Hadley Wickham\nTeaching R in a Kinder, Gentler, More Effective Manner: Teach Base-R, Not Just the Tidyverse Author: Prof. Norm Matloff, University of California, Davis\nR Workflow - An overview of R Workflow, which covers how to use R effectively all the way from importing data to analysis, and making use of Quarto for reproducible reporting.\n\n\n\nStatistics\n\nThe ASA Statement on p-Values: Context, Process, and Purpose\n\n\n\n\n\n\n\nStatistical significance is not equivalent to scientific, human, or economic significance\n\n\n\n\n\nStatistical significance is not equivalent to scientific, human, or economic significance. Smaller p-values do not necessarily imply the presence of larger or more important effects, and larger p-values do not imply alack of importance or even lack of effect… No single index should substitute for scientific reasoning.\n\n\n\n\nStatistical Inference in the 21st Century: A World Beyond p &lt; 0.05\n\n\n\n\n\n\n\nDon’t Say “Statistically Significant”\n\n\n\n\n\nThe ASA Statement on P-Values and Statistical Significance stopped just short of recommending that declarations of “statistical significance” be abandoned. We take that step here. We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term “statistically significant” entirely. Nor should variants such as “significantly different,” “p &lt; 0.05,” and “nonsignificant” survive, whether expressed in words, by asterisks in a table, or in some other way.\nRegardless of whether it was ever useful, a declaration of “statistical significance” has today become meaningless. Made broadly known by Fisher’s use of the phrase (1925), Edgeworth’s (1885) original intention for statistical significance was simply as a tool to indicate when a result warrants further scrutiny. But that idea has been irretrievably lost. Statistical significance was never meant to imply scientific importance, and the confusion of the two was decried soon after its widespread use (Boring 1919). Yet a full century later the confusion persists.\n\n\n\n\nBayesian and frequentist reasoning in plain English\nUltimate Guide to Statistics for Data Science\nBook: Improving Your Statistical Inferences\nBook/course: Online Statistics Education: An Interactive Multimedia Course of Study\nBook: Bayesian Data Analysis Third edition by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin.\nBook: Regression and Other Stories by Andrew Gelman, Jennifer Hill, Aki Vehtari\n\n\n\nMachine learning\n\nA free deep learning course"
  },
  {
    "objectID": "contact/index.html",
    "href": "contact/index.html",
    "title": "Contact",
    "section": "",
    "text": "The best way to contact me is by email at rohit [dot] farmer [at] dataalltheway [dot] com.\nYou can also leave a comment at the bottom of this page and I will respond as soon as possible.\nI am also active on Mastodon and Twitter and would be happy to chat there.\n\n\n\n\n\n\nNote\n\n\n\nThis is a not-for-profit website without advertisements and a paywall, and I intend to support it fully in the foreseeable future. Therefore, I am not looking for any affiliate marketing or paid promotions. However, I appreciate your time and expertise in providing constructive criticism of the content on this website, suggesting corrections, contributing an article, or writing a comment on a post."
  },
  {
    "objectID": "posts/001-data-transformation/index.html",
    "href": "posts/001-data-transformation/index.html",
    "title": "Data Transformation",
    "section": "",
    "text": "Data transformation is a process of performing a mathematical function on each data point used in a statistical or machine learning analysis to either satisfy the underlying assumptions of a statistical test (e.g., normal distribution for a t-test), help a machine-learning algorithm to converge faster and or make a visualization interpretable. In addition to statistical analyses and modeling, data transformation can also be helpful in data visualization, for example, performing a log transformation on a skewed data set to plot it in a relatively unskewed and visually appealing scatter plot. Most of the data transformation methods are invertible and original values of a data set can be recovered by implementing a counter mathematical function. In mathematical form it can be expressed as:\n\\[x' = f(x)\\]\nWhere \\(x\\) is the original data, \\(x'\\) is the transformed data, and \\(f(x)\\) is a mathematical function performed on \\(x\\).\nIn data science, data transformation is also sometimes combined with the data cleaning step. In addition to performing a mathematical function to the data points, they are also checked for quality, for example, checking for missing values. I will discuss data cleaning procedures elsewhere. Data transformation can be considered as an umbrella term for both data scaling and data normalization. They are frequently used interchangeably, sometimes referring to the same mathematical operation. Although data scaling and normalization are used to achieve a similar result, it is better to understand them as two different operations that are happening under the hood.\nAlthough every data transformation method performs a mathematical operation on every data point (e.i. element wise), for some, this operation is not influenced if data points are either removed or added to the data set. Let’s consider a data set in the form of a two-dimensional data table with samples on the row and features on the column. Now take two methods to compare 1) log transformation 2) min-max scaling. In log transformation \\(log(x)\\), a log is taken for every data point individually, and the result will not change if some rows or columns are dropped or added in our example data table. However, in min-max scaling\n\\[x' = x-min(x)/max(x)-min(x)\\]\nthat is performed feature-wise (columns); if the data point that was selected as a min or max in a previous transformation is removed, then re-doing the transformation will change the result. The removal of a data point may happen; for example, if the min or max value selected in the first iteration was an outlier or that a particular sample had multiple missing values, and therefore, it had to be removed, amongst others. Min-max scaling will also influence if more data points are added to our data set. It may bring a new min or max data point and hence will change the scaling. Therefore while selecting a data transformation method, it must be noted if data points are dropped in the subsequent analysis, then should you perform the transformation again as a result of data point loss or it will be indifferent."
  },
  {
    "objectID": "posts/001-data-transformation/index.html#log-transformation",
    "href": "posts/001-data-transformation/index.html#log-transformation",
    "title": "Data Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\nIn a log transformation, logarithm is calculated for every value in the data set. Traditionally, log transformation is carried out to reduce the skewness of data or to bring data closer to a normal distribution. Usually the base to the log doesn’t matter unless it is a domain specific requirement. However, every feature of the data set should be transformed with the same base. Most of the programming languages have a core function to calculate the log of a number. In programming languages that support vector operation, for example, R, the same log function can be performed on both a single value or on all the values within a data frame, vector or matrix.\nFor example, let’s visualize the effect of log transformation on a synthetically generated dummy data. To generated figures Figure 1 and Figure 2, I have randomly sampled 10,000 positive real numbers from a skewed (positive and negative) normal distribution and performed a log transformation on every data point. The left sub-panel shows a histogram of the non-transformed data, and the right sub-panel shows a histogram of the log-transformed data. Although log transformation is known for reducing the skewness of the data and making the distribution more symmetric around the mean, it holds only for the positively skewed data. If the data are negatively skewed a log transformation will skew it further. In case of a negatively skewed data doing a power transformation may help to reduce the skewness (figure Figure 3). Usually raising the data to a power of 2 has slight effect on the skewness; a higher number may be required. In addition to the visual inspection, we can also numerically quantify the skewness of the data; that is mentioned in the figure caption.\nLog Transformation: \\[x' = log(x)\\]\nPower Transformation: \\[x' = x^n\\]\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import skewnorm\nfrom scipy.stats import skew \nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Generate random data points from a skewed normal distribution\ndata_pos = np.round(skewnorm.rvs(10, size=10000, loc=1, random_state = 101), decimals = 2)\n#print('Skewness for the positively (right) skewed data before transformation : ', round(skew(data_pos), 2)) \ndata_neg = np.round(skewnorm.rvs(-10, size=10000, loc=10, random_state = 101), decimals = 2)\n#print('Skewness for the negatively (left) skewed data before transformation : ', round(skew(data_neg), 2)) \n\n# Log transform the data\nlog_data_pos = np.log(data_pos)\n#print('Skewness for the positively skewed data after transformation : ', round(skew(log_data_pos), 2)) \nlog_data_neg = np.log(data_neg)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(log_data_neg), 2)) \n\n\n\n\nCode\nfig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n\n# We can set the number of bins with the *bins* keyword argument.\naxs[0].hist(data_pos, bins=20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(log_data_pos, bins=20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Log-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 1: Histogram of the positively skewed data and its log transformation. The skewness for the non-transformed data (left) is 0.9 and for the log-transformed data (right) is 0.2.\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\naxs[0].hist(data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(log_data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Log-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 2: Histogram of the negatively skewed data and its log transformation. The skewness for the non-transformed data (left) is -0.9 and for the log-transformed data (right) is -1.2.\n\n\n\n\n\n\nCode\n# Square data.\npow_data_neg = np.power(data_neg, 6)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(pow_data_neg), 2)) \nfig, axs = plt.subplots(ncols=2, sharey = \"all\", tight_layout=True)\naxs[0].hist(data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(pow_data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Power-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 3: Histogram of the negatively skewed data and its power transformation. Data is raised to the power ot 6. The skewness for the non-transformed data (left) is -0.9 and for the power-transformed data (right) is -0.3.\n\n\n\n\nNote: Since the data used in these figures are sampled from a skewed normal distribution the skewness calculated here are below 2. For a non-normally distributed skewed data it would be higher than 2. Log transformation is often used to bring a non-normal distribution closer to a normal distribution.\nLog transformation can only be performed on positive values. Mathematics principles doesn’t allow log calculation on negative values. In case our input data contains negative values and a log like transformation is desired inverse hyperbolic sin (arcsinh) transformation method can be used."
  },
  {
    "objectID": "posts/001-data-transformation/index.html#arcsinh-transformation",
    "href": "posts/001-data-transformation/index.html#arcsinh-transformation",
    "title": "Data Transformation",
    "section": "Arcsinh Transformation",
    "text": "Arcsinh Transformation\nInverse hyperbolic sin transformation is a non-linear transformation that is often used in situations where a log transformations can’t be used; such as in the presence of negative values. Flow and mass cytometry are popular examples where arcsinh transformation is a almost always a method of choice. Reason being older flow cytometry machines produced positive values that were displayed on a log scale. However, newer machines can produce both negative and positive values that can’t be displayed on a log scale. Therefore, to keep the data resemble a log transformation arcsinh transformation is used.\nArcsinh transformation can also be tweaked by using a cofactor to behave differently around zero. For both negative and positive values starting from zero to cofactor are presented in a linear fashion along the lines of raw data values and values beyond he cofactor are presented in a log like fashion. In flow and mass cytometry a cofactor of 150 and 5 are used respectively.\nFor all real x: \\[arcsinh(x) = log(x + \\sqrt{x^2 + 1})\\]\nLet’s use similar positively skewed data as in the log transformation to visualize how an arcsinh transformation affects the shape of the distribution. The only change that I would want to do in this data set is to add few negative values. As I mentioned earlier that our mathematical laws doesn’t allow us to take log on negative numbers arcsinh transformation is capable of transforming small negative values closer to zero. Figures Figure 4 and Figure 5 show the histograms comparing the original and the arcsinh transformed data for positive and negatively skewed data respectively. From the figures it’s evident that unlike log, arcsinh transformation works on both positively and negatively skewed data equally well.\n\n\nCode\n# Generate random data points from a skewed normal distribution\ndata_pos = np.round(skewnorm.rvs(10, size=10000, loc=0, random_state = 101), decimals = 2)\n#print('Skewness for the positively (right) skewed data before transformation : ', round(skew(data_pos), 2))\ndata_neg = np.round(skewnorm.rvs(-10, size=10000, loc=0, random_state = 101), decimals = 2)\n#print('Skewness for the negatively (left) skewed data before transformation : ', round(skew(data_neg), 2)) \n\n# Arcsinh transform the data\narcsinh_data_pos = np.arcsinh(data_pos)\n#print('Skewness for the positively skewed data after transformation : ', round(skew(arcsinh_data_pos), 2)) \narcsinh_data_neg = np.arcsinh(data_neg)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(arcsinh_data_neg), 2)) \n\n\n\n\nCode\nfig, axs = plt.subplots(ncols=2, sharey = \"all\", tight_layout=True)\naxs[0].hist(data_pos, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(arcsinh_data_pos, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Arcsinh-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 4: Histogram of the positively skewed data and its arcsinh transformation. The skewness for the non-transformed data (left) is 0.9 and for the arcsinh-transformed data (right) is 0.3.\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(ncols=2, sharey = \"all\", tight_layout=True)\naxs[0].hist(data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(arcsinh_data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Arcsinh-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 5: Histogram of the negatively skewed data and its arcsinh transformation. The skewness for the non-transformed data (left) is -0.9 and for the arcsinh-transformed data (right) is -0.3."
  },
  {
    "objectID": "posts/001-data-transformation/index.html#min-max-scaling",
    "href": "posts/001-data-transformation/index.html#min-max-scaling",
    "title": "Data Transformation",
    "section": "Min-Max Scaling",
    "text": "Min-Max Scaling\nIn min-max scaling for a given feature, we subtract the minimum value from each value and divide the residual by the difference between the maximum and the minimum value. The resulting transformed data is scaled between 0 and 1.\n\\[minmax(x) = x - min(x) / max(x) - min(x)\\]\nMin-max scaling can also be modified to scale the values to the desired range, for example, between -1 and 1.\n\\[minmax(x) = ((b - a) * (x - min(x)) / max(x) - min(x)) +  a\\]\nWhere \\(a\\) and \\(b\\) are the minimum and maximum range respectively.\n\nApplication(s)\n\nNeural networks"
  },
  {
    "objectID": "posts/001-data-transformation/index.html#standardization",
    "href": "posts/001-data-transformation/index.html#standardization",
    "title": "Data Transformation",
    "section": "Standardization",
    "text": "Standardization\nStandardization is also known as z-scaling, mean removal, or variance scaling. In standardization, the goal is to scale the data with a mean of zero and a standard deviation of one.\n\\[z = (x - \\mu)/\\sigma\\]\nWhere \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of a given feature. Then, the distribution of the transformed data is called the z-distribution.\n\nApplication(s)\n\nPrincipal Component Analysis (PCA)\nIn heatmaps to compare data among samples"
  },
  {
    "objectID": "posts/001-data-transformation/index.html#quantile-normalization",
    "href": "posts/001-data-transformation/index.html#quantile-normalization",
    "title": "Data Transformation",
    "section": "Quantile Normalization",
    "text": "Quantile Normalization\nQuantile normalization (QN) is a technique to make two distribution identical in statistical properties. QN involves first ranking the feature of each sample by magnitude, calculating the average value for genes occupying the same rank, and then substituting the values of all genes occupying that particular rank with this average value. The next step is to reorder the features of each sample in their original order."
  },
  {
    "objectID": "posts/003-how-to-download-a-shared-file-from-googledrive-in-r/index.html",
    "href": "posts/003-how-to-download-a-shared-file-from-googledrive-in-r/index.html",
    "title": "How to download a shared file from Google Drive in R",
    "section": "",
    "text": "To download a shared file with “anyone with the link” access rights from Google Drive in R, we can utilize the googledrive library from the tidyverse package. The method described here will utilize the file ID copied from the shared link. Typically googledrive package is used to work with a Google Drive of an authenticated user. However, since we are downloading a publicly shared file in this tutorial, we will work without user authentication. So, please follow the steps below.\n\n\nBelow is a share link from my Google Drive pointing to an R data frame.\nhttps://drive.google.com/file/d/1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4/view?usp=sharing\nThis string 1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4 is the file ID that we will use to download.\n\n\n\n\nif(!require(googledrive)) install.packages(\"googledrive\")\nlibrary(googledrive)\n\ndrive_deauth()\ndrive_user()\npublic_file &lt;-  drive_get(as_id(\"1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4\"))\ndrive_download(public_file, overwrite = TRUE)\n\nFile downloaded:\n• hdstim-example-data.rds &lt;id: 1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4&gt;\nSaved locally as:\n• hdstim-example-data.rds\nThe downloaded data frame.\n\nlibrary(DT)\ndatatable(head(readRDS(\"hdstim-example-data.rds\")))"
  },
  {
    "objectID": "posts/003-how-to-download-a-shared-file-from-googledrive-in-r/index.html#step-1-copy-the-file-id-from-the-share-link.",
    "href": "posts/003-how-to-download-a-shared-file-from-googledrive-in-r/index.html#step-1-copy-the-file-id-from-the-share-link.",
    "title": "How to download a shared file from Google Drive in R",
    "section": "",
    "text": "Below is a share link from my Google Drive pointing to an R data frame.\nhttps://drive.google.com/file/d/1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4/view?usp=sharing\nThis string 1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4 is the file ID that we will use to download."
  },
  {
    "objectID": "posts/003-how-to-download-a-shared-file-from-googledrive-in-r/index.html#step-2-download-the-file-using-an-authenticated-connection.",
    "href": "posts/003-how-to-download-a-shared-file-from-googledrive-in-r/index.html#step-2-download-the-file-using-an-authenticated-connection.",
    "title": "How to download a shared file from Google Drive in R",
    "section": "",
    "text": "if(!require(googledrive)) install.packages(\"googledrive\")\nlibrary(googledrive)\n\ndrive_deauth()\ndrive_user()\npublic_file &lt;-  drive_get(as_id(\"1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4\"))\ndrive_download(public_file, overwrite = TRUE)\n\nFile downloaded:\n• hdstim-example-data.rds &lt;id: 1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4&gt;\nSaved locally as:\n• hdstim-example-data.rds\nThe downloaded data frame.\n\nlibrary(DT)\ndatatable(head(readRDS(\"hdstim-example-data.rds\")))"
  },
  {
    "objectID": "posts/005-classify-the-bitter-or-sweet-taste-of-compounds/index.html",
    "href": "posts/005-classify-the-bitter-or-sweet-taste-of-compounds/index.html",
    "title": "Classify the bitter or sweet taste of compounds",
    "section": "",
    "text": "Original Post\n\n\n\nThis post is an identical copy of “About Dataset” at Kaggle: https://www.kaggle.com/dsv/4234193\n\n\n\nContext\nThroughout human evolution, we have been drawn toward sweet-tasting foods and averted from bitter tastes - sweet is good or desirable, bitter is undesirable, ear wax or medicinal. Therefore, a better understanding of molecular features that determine the bitter-sweet taste of substances is crucial for identifying natural and synthetic compounds for various purposes.\n\n\nSources\nThis dataset is adapted from https://github.com/cosylabiiit/bittersweet, https://www.nature.com/articles/s41598-019-43664-y. In chemoinformatics, molecules are often represented as compact SMILES strings. In this dataset, SMILES structures, along with their names and targets (bitter, sweet, tasteless, and non-bitter), were obtained from the original study. Subsequently, SMILES were converted into canonical SMILES using RDKit, and the features (molecular descriptors, both 2D and 3D) were calculated using Mordred. Secondly, tasteless and non-bitter categories were merged into a single category of non-bitter-sweet. Finally, since many of the compounds were missing names, IUPAC names were fetched using PubChemPy for all the compounds, and for still missing names, a generic compound + incrementor name was assigned.\n\n\nInspiration\nThis is a classification dataset with the first three columns carrying names, SMILES, and canonical SMILES. Any of these columns can be used to refer to a molecule. The fourth column is the target (taste category). And all numeric features are from the 5th column until the end of the file. Many features have cells with string annotations due to errors produced by Mordred. Therefore, the following data science techniques can be learned while working on this dataset:\n\nData cleanup\nFeatures selection (since the number of features is quite large in proportion to the data points)\nFeature scaling/transformation/normalization\nDimensionality reduction\nBinomial classification (bitter vs. sweet) - utilize non-bitter-sweet as a negative class.\nMultinomial classification (bitter vs. sweet vs. non-bitter-sweet)\nSince SMILES can be converted into molecular graphs, graph-based modeling should also be possible.\n\n\n\nInitial data preparation\nA copy of the original dataset and the scripts and notebooks used to convert SMILES to canonical SMILES, generate features, fetch names, and export the final TSV file for Kaggle is loosely maintained at https://github.com/rohitfarmer/bittersweet.\n\n\n\n\nCitationBibTeX citation:@dataset{farmer2022,\n  author = {Farmer, Rohit},\n  publisher = {Kaggle},\n  title = {Classify the Bitter or Sweet Taste of Compounds},\n  date = {2022-10-15},\n  url = {https://www.kaggle.com/dsv/4234193},\n  doi = {10.34740/KAGGLE/DSV/4234193},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFarmer, Rohit. 2022. “Classify the Bitter or Sweet Taste of\nCompounds.” Kaggle. https://doi.org/10.34740/KAGGLE/DSV/4234193."
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html",
    "href": "posts/007-open-data-for-datascience/index.html",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "",
    "text": "Updates\n\n\n\n\n\n2022-10-26 Added sections for Kaggle and other platforms\n2022-10-25 Initial uncomplete post and Kaggle notebook"
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html#default-datasets-in-r",
    "href": "posts/007-open-data-for-datascience/index.html#default-datasets-in-r",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "Default datasets in R",
    "text": "Default datasets in R\nIn R (v4.1.3), there are 104 datasets for various statistical and machine-learning tasks. The commands in the cell below list all the datasets available by default (Table 1) and across all the installed packages, respectively. This article summarizes some of R’s popular datasets, namely mtcars, iris, etc.\n# Default datasets\ndata()\n\n# Datasets across all the installed packages\ndata(package = .packages(all.available = TRUE))\n\n\nCode\ndat &lt;- data()\ndat &lt;- as_tibble(dat$results) %&gt;% dplyr::select(-LibPath) %&gt;%\n  dplyr::filter(Package == \"datasets\")\nknitr::kable(dat) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n    scroll_box(width = \"100%\", height = \"300px\")\n\n\n\n\n\nTable 1: Default datasets in R\n\n\nPackage\nItem\nTitle\n\n\n\n\ndatasets\nAirPassengers\nMonthly Airline Passenger Numbers 1949-1960\n\n\ndatasets\nBJsales\nSales Data with Leading Indicator\n\n\ndatasets\nBJsales.lead (BJsales)\nSales Data with Leading Indicator\n\n\ndatasets\nBOD\nBiochemical Oxygen Demand\n\n\ndatasets\nCO2\nCarbon Dioxide Uptake in Grass Plants\n\n\ndatasets\nChickWeight\nWeight versus age of chicks on different diets\n\n\ndatasets\nDNase\nElisa assay of DNase\n\n\ndatasets\nEuStockMarkets\nDaily Closing Prices of Major European Stock Indices, 1991-1998\n\n\ndatasets\nFormaldehyde\nDetermination of Formaldehyde\n\n\ndatasets\nHairEyeColor\nHair and Eye Color of Statistics Students\n\n\ndatasets\nHarman23.cor\nHarman Example 2.3\n\n\ndatasets\nHarman74.cor\nHarman Example 7.4\n\n\ndatasets\nIndometh\nPharmacokinetics of Indomethacin\n\n\ndatasets\nInsectSprays\nEffectiveness of Insect Sprays\n\n\ndatasets\nJohnsonJohnson\nQuarterly Earnings per Johnson & Johnson Share\n\n\ndatasets\nLakeHuron\nLevel of Lake Huron 1875-1972\n\n\ndatasets\nLifeCycleSavings\nIntercountry Life-Cycle Savings Data\n\n\ndatasets\nLoblolly\nGrowth of Loblolly pine trees\n\n\ndatasets\nNile\nFlow of the River Nile\n\n\ndatasets\nOrange\nGrowth of Orange Trees\n\n\ndatasets\nOrchardSprays\nPotency of Orchard Sprays\n\n\ndatasets\nPlantGrowth\nResults from an Experiment on Plant Growth\n\n\ndatasets\nPuromycin\nReaction Velocity of an Enzymatic Reaction\n\n\ndatasets\nSeatbelts\nRoad Casualties in Great Britain 1969-84\n\n\ndatasets\nTheoph\nPharmacokinetics of Theophylline\n\n\ndatasets\nTitanic\nSurvival of passengers on the Titanic\n\n\ndatasets\nToothGrowth\nThe Effect of Vitamin C on Tooth Growth in Guinea Pigs\n\n\ndatasets\nUCBAdmissions\nStudent Admissions at UC Berkeley\n\n\ndatasets\nUKDriverDeaths\nRoad Casualties in Great Britain 1969-84\n\n\ndatasets\nUKgas\nUK Quarterly Gas Consumption\n\n\ndatasets\nUSAccDeaths\nAccidental Deaths in the US 1973-1978\n\n\ndatasets\nUSArrests\nViolent Crime Rates by US State\n\n\ndatasets\nUSJudgeRatings\nLawyers' Ratings of State Judges in the US Superior Court\n\n\ndatasets\nUSPersonalExpenditure\nPersonal Expenditure Data\n\n\ndatasets\nUScitiesD\nDistances Between European Cities and Between US Cities\n\n\ndatasets\nVADeaths\nDeath Rates in Virginia (1940)\n\n\ndatasets\nWWWusage\nInternet Usage per Minute\n\n\ndatasets\nWorldPhones\nThe World's Telephones\n\n\ndatasets\nability.cov\nAbility and Intelligence Tests\n\n\ndatasets\nairmiles\nPassenger Miles on Commercial US Airlines, 1937-1960\n\n\ndatasets\nairquality\nNew York Air Quality Measurements\n\n\ndatasets\nanscombe\nAnscombe's Quartet of 'Identical' Simple Linear Regressions\n\n\ndatasets\nattenu\nThe Joyner-Boore Attenuation Data\n\n\ndatasets\nattitude\nThe Chatterjee-Price Attitude Data\n\n\ndatasets\naustres\nQuarterly Time Series of the Number of Australian Residents\n\n\ndatasets\nbeaver1 (beavers)\nBody Temperature Series of Two Beavers\n\n\ndatasets\nbeaver2 (beavers)\nBody Temperature Series of Two Beavers\n\n\ndatasets\ncars\nSpeed and Stopping Distances of Cars\n\n\ndatasets\nchickwts\nChicken Weights by Feed Type\n\n\ndatasets\nco2\nMauna Loa Atmospheric CO2 Concentration\n\n\ndatasets\ncrimtab\nStudent's 3000 Criminals Data\n\n\ndatasets\ndiscoveries\nYearly Numbers of Important Discoveries\n\n\ndatasets\nesoph\nSmoking, Alcohol and (O)esophageal Cancer\n\n\ndatasets\neuro\nConversion Rates of Euro Currencies\n\n\ndatasets\neuro.cross (euro)\nConversion Rates of Euro Currencies\n\n\ndatasets\neurodist\nDistances Between European Cities and Between US Cities\n\n\ndatasets\nfaithful\nOld Faithful Geyser Data\n\n\ndatasets\nfdeaths (UKLungDeaths)\nMonthly Deaths from Lung Diseases in the UK\n\n\ndatasets\nfreeny\nFreeny's Revenue Data\n\n\ndatasets\nfreeny.x (freeny)\nFreeny's Revenue Data\n\n\ndatasets\nfreeny.y (freeny)\nFreeny's Revenue Data\n\n\ndatasets\ninfert\nInfertility after Spontaneous and Induced Abortion\n\n\ndatasets\niris\nEdgar Anderson's Iris Data\n\n\ndatasets\niris3\nEdgar Anderson's Iris Data\n\n\ndatasets\nislands\nAreas of the World's Major Landmasses\n\n\ndatasets\nldeaths (UKLungDeaths)\nMonthly Deaths from Lung Diseases in the UK\n\n\ndatasets\nlh\nLuteinizing Hormone in Blood Samples\n\n\ndatasets\nlongley\nLongley's Economic Regression Data\n\n\ndatasets\nlynx\nAnnual Canadian Lynx trappings 1821-1934\n\n\ndatasets\nmdeaths (UKLungDeaths)\nMonthly Deaths from Lung Diseases in the UK\n\n\ndatasets\nmorley\nMichelson Speed of Light Data\n\n\ndatasets\nmtcars\nMotor Trend Car Road Tests\n\n\ndatasets\nnhtemp\nAverage Yearly Temperatures in New Haven\n\n\ndatasets\nnottem\nAverage Monthly Temperatures at Nottingham, 1920-1939\n\n\ndatasets\nnpk\nClassical N, P, K Factorial Experiment\n\n\ndatasets\noccupationalStatus\nOccupational Status of Fathers and their Sons\n\n\ndatasets\nprecip\nAnnual Precipitation in US Cities\n\n\ndatasets\npresidents\nQuarterly Approval Ratings of US Presidents\n\n\ndatasets\npressure\nVapor Pressure of Mercury as a Function of Temperature\n\n\ndatasets\nquakes\nLocations of Earthquakes off Fiji\n\n\ndatasets\nrandu\nRandom Numbers from Congruential Generator RANDU\n\n\ndatasets\nrivers\nLengths of Major North American Rivers\n\n\ndatasets\nrock\nMeasurements on Petroleum Rock Samples\n\n\ndatasets\nsleep\nStudent's Sleep Data\n\n\ndatasets\nstack.loss (stackloss)\nBrownlee's Stack Loss Plant Data\n\n\ndatasets\nstack.x (stackloss)\nBrownlee's Stack Loss Plant Data\n\n\ndatasets\nstackloss\nBrownlee's Stack Loss Plant Data\n\n\ndatasets\nstate.abb (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.area (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.center (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.division (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.name (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.region (state)\nUS State Facts and Figures\n\n\ndatasets\nstate.x77 (state)\nUS State Facts and Figures\n\n\ndatasets\nsunspot.month\nMonthly Sunspot Data, from 1749 to \"Present\"\n\n\ndatasets\nsunspot.year\nYearly Sunspot Data, 1700-1988\n\n\ndatasets\nsunspots\nMonthly Sunspot Numbers, 1749-1983\n\n\ndatasets\nswiss\nSwiss Fertility and Socioeconomic Indicators (1888) Data\n\n\ndatasets\ntreering\nYearly Treering Data, -6000-1979\n\n\ndatasets\ntrees\nDiameter, Height and Volume for Black Cherry Trees\n\n\ndatasets\nuspop\nPopulations Recorded by the US Census\n\n\ndatasets\nvolcano\nTopographic Information on Auckland's Maunga Whau Volcano\n\n\ndatasets\nwarpbreaks\nThe Number of Breaks in Yarn during Weaving\n\n\ndatasets\nwomen\nAverage Heights and Weights for American Women"
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html#us-cities-and-counties",
    "href": "posts/007-open-data-for-datascience/index.html#us-cities-and-counties",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "US cities and counties",
    "text": "US cities and counties\n\n\nCode\ncity_county &lt;- dplyr::filter(open_gov, Type == \"US City or County\")\nDT::datatable(city_county, options = list(pageLength = 5))"
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html#us-states",
    "href": "posts/007-open-data-for-datascience/index.html#us-states",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "US states",
    "text": "US states\n\n\nCode\nus_state &lt;- dplyr::filter(open_gov, Type %in% c(\"US State\", \"Other State Related\"))\nDT::datatable(us_state, options = list(pageLength = 5))"
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html#international-countries-and-regions",
    "href": "posts/007-open-data-for-datascience/index.html#international-countries-and-regions",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "International countries and regions",
    "text": "International countries and regions\n\n\nCode\nint_count &lt;- dplyr::filter(open_gov, Type %in% c(\"International Country\", \"International Regional\"))\nDT::datatable(int_count, options = list(pageLength = 5))"
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html#sec-marylandapi",
    "href": "posts/007-open-data-for-datascience/index.html#sec-marylandapi",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "An example of using Maryland state open data via an API",
    "text": "An example of using Maryland state open data via an API\nSince I live and work in Maryland, I want to see how wages in Maryland and its counties have changed over time. I also want to test if Montgomery county (where I live) has different wages compared to Frederick, Howard, and Prince George’s counties which borders Montgomery on the north, east, and south sides. Therefore, in this example, I will fetch Maryland Average Wage Per Job (Current Dollars): 2010-2020 data via API using RSocrata library in R and carry out some analysis.\n\n\n\n\n\n\nNote\n\n\n\nSee https://dev.socrata.com/ to learn more about how to work with open data APIs in various programming languages.\n\n\nIn Table 2, each row has an average wage for a year for Maryland, and each of its counties (columns) from 2010-2020 and Figure 1 shows the same data as a line graph depicting the change in wages (y-axis) over time (x-axis).\nTable 3 lists the results of an unpaired two-sample t-test between wages from Montgomery and Frederick, Howard, and Prince George’s counties. As you can see from the t-test results, wages differ between Montgomery and Frederick, Howard, and Prince George’s counties, with Montogomery county residents earning higher than all its three bordering counties.\n\n\nCode\nlibrary(RSocrata)\n# Fetch the data using the API endpoint\nmaw &lt;- read.socrata(\"https://opendata.maryland.gov/resource/mk5a-nf44.json\")\nknitr::kable(dplyr::select(maw, -date_created)) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;%\n    scroll_box(width = \"100%\", height = \"400px\")\n\n\n\n\n\n\nTable 2: Maryland Average Wage Per Job (Current Dollars): 2010-2020\n\n\nyear\nmaryland\nallegany_county\nanne_arundel_county\nbaltimore_city\nbaltimore_county\ncalvert_county\ncaroline_county\ncarroll_county\ncecil_county\ncharles_county\ndorchester_county\nfrederick_county\ngarrett_county\nharford_county\nhoward_county\nkent_county\nmontgomery_county\nprince_george_s_county\nqueen_anne_s_county\nsomerset_county\nst_mary_s_county\ntalbot_county\nwashington_county\nwicomico_county\nworcester_county\n\n\n\n\n2010\n53096\n35771\n56745\n55640\n49986\n42726\n34616\n38027\n42027\n41290\n35489\n48018\n31591\n46741\n58130\n36334\n65178\n51808\n36018\n38228\n60032\n37845\n38228\n38472\n30799\n\n\n2011\n54517\n36677\n58011\n57027\n50914\n43431\n35981\n39039\n42465\n42200\n35718\n48794\n32484\n48558\n60448\n36815\n67247\n52844\n36437\n39652\n63057\n38462\n39420\n38915\n31438\n\n\n2012\n55466\n36983\n58706\n58876\n51722\n44239\n37506\n39919\n43260\n42888\n37172\n49972\n32506\n49772\n62371\n36622\n68159\n53292\n37258\n39853\n63698\n39807\n39564\n39066\n31641\n\n\n2013\n55555\n37827\n59384\n59318\n51778\n44126\n38404\n40736\n44214\n42909\n37773\n49570\n33477\n49624\n62271\n37572\n67437\n53441\n36848\n40744\n63501\n39901\n40032\n39714\n32384\n\n\n2014\n56924\n38449\n60551\n61112\n52961\n45162\n39383\n41607\n45051\n44260\n39094\n50747\n34195\n50205\n64784\n38411\n68731\n54985\n37932\n41802\n64691\n40118\n41018\n40863\n33635\n\n\n2015\n58729\n39888\n62195\n63389\n54248\n48825\n41043\n43325\n46776\n44919\n40022\n51510\n35067\n52418\n66677\n38741\n71480\n56456\n38970\n43397\n65497\n41313\n42270\n42599\n34524\n\n\n2016\n59710\n40708\n63147\n64481\n55159\n53657\n40832\n43815\n47300\n46958\n40431\n51630\n34925\n52862\n67621\n39504\n72904\n57251\n39941\n43575\n65937\n41740\n42725\n43875\n35260\n\n\n2017\n61298\n42143\n64629\n66365\n56887\n55922\n42034\n45576\n48662\n47673\n41711\n52270\n35971\n53775\n68958\n40446\n74709\n58829\n42099\n45988\n67622\n43105\n44039\n45491\n35802\n\n\n2018\n62836\n43197\n66458\n67005\n58793\n53557\n43190\n45690\n49981\n48225\n41987\n53624\n37575\n54921\n71300\n42422\n76867\n60383\n43582\n45381\n68887\n44670\n45846\n45567\n37231\n\n\n2019\n64690\n44692\n68586\n69930\n60116\n51598\n45190\n47189\n52177\n49193\n43271\n55621\n38290\n57349\n74136\n42575\n78386\n62096\n44011\n49234\n70807\n45115\n46965\n46620\n38234\n\n\n2020\n70446\n48294\n74533\n74483\n65743\n55903\n49336\n51470\n55854\n53404\n47182\n60646\n40690\n62395\n82780\n45891\n86138\n66777\n48385\n53880\n77490\n48338\n50743\n50556\n41605\n\n\n\n\n\n\n\n\n\n\n\nCode\nmaw_gather &lt;- maw %&gt;% dplyr::select(-date_created) %&gt;%\n  gather(key = \"county\", value = \"wage\", -year ) %&gt;% as_tibble()\nggplot(maw_gather, aes(x = year, y = as.numeric(wage), color = county)) +\n  geom_line(aes(group = county)) + \n  labs(x = \"Year\", y = \"Wage\", color = \"\") +theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\nFigure 1: Maryland Average Wage Per Job (Current Dollars): 2010-2020\n\n\n\n\n\n\nCode\nmft &lt;- broom::tidy(t.test(as.numeric(maw$montgomery_county), \n                   as.numeric(maw$frederick_county))) %&gt;% \n                   dplyr::mutate(\"test\" = \"Montgomery vs. Frederick\")\n\nmht &lt;- broom::tidy(t.test(as.numeric(maw$montgomery_county), \n                   as.numeric(maw$howard_county))) %&gt;% \n                   dplyr::mutate(\"test\" = \"Montgomery vs. Howard\")\n\nmpgt &lt;- broom::tidy(t.test(as.numeric(maw$montgomery_county), \n                    as.numeric(maw$prince_george_s_county))) %&gt;% \n                    dplyr::mutate(\"test\" = \"Montgomery vs. Prince George's\")\n\nall_t &lt;- dplyr::bind_rows(mft, mht, mpgt) %&gt;%\n  dplyr::select(all_of(c(\"test\", \"estimate\", \"estimate1\", \n  \"estimate2\", \"statistic\", \"p.value\")))\n\nknitr::kable(all_t) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\nTable 3: T-test results between wages from Montgomery and Frederick, Howard, and Prince George’s county\n\n\ntest\nestimate\nestimate1\nestimate2\nstatistic\np.value\n\n\n\n\nMontgomery vs. Frederick\n20439.455\n72476\n52036.55\n9.452393\n0.0000001\n\n\nMontgomery vs. Howard\n5250.909\n72476\n67225.09\n1.858408\n0.0781124\n\n\nMontgomery vs. Prince George's\n15370.364\n72476\n57105.64\n6.597893\n0.0000030"
  },
  {
    "objectID": "posts/009-rules-for-naming-files/index.html",
    "href": "posts/009-rules-for-naming-files/index.html",
    "title": "Rules for naming files and folders that are cross platform and helpful in datascience",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-11-03 Corrections made as pointed by https://fosstodon.org/@rudolf read this thread https://fosstodon.org/@swatantra/109281773145979327"
  },
  {
    "objectID": "posts/009-rules-for-naming-files/index.html#some-examples",
    "href": "posts/009-rules-for-naming-files/index.html#some-examples",
    "title": "Rules for naming files and folders that are cross platform and helpful in datascience",
    "section": "Some Examples",
    "text": "Some Examples\n2022-08-31-labnotebook-for-hdstim.docx\n\nfigure-01.png\nfigure-02.png\nfigure-03.png\n\n/path/to/folder/exploring-flow"
  },
  {
    "objectID": "posts/010-02-parametric-hypothesis-tests-python/index.html",
    "href": "posts/010-02-parametric-hypothesis-tests-python/index.html",
    "title": "Parametric hypothesis tests with examples in Python",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2023-01-09 First draft"
  },
  {
    "objectID": "posts/010-02-parametric-hypothesis-tests-python/index.html#import-packages",
    "href": "posts/010-02-parametric-hypothesis-tests-python/index.html#import-packages",
    "title": "Parametric hypothesis tests with examples in Python",
    "section": "Import packages",
    "text": "Import packages\n\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\n\ndat = pd.read_csv(\"https://raw.githubusercontent.com/opencasestudies/ocs-bp-rural-and-urban-obesity/master/data/wrangled/BMI_long.csv\")"
  },
  {
    "objectID": "posts/010-02-parametric-hypothesis-tests-python/index.html#example-code-for-a-two-sample-unpaired-z-test",
    "href": "posts/010-02-parametric-hypothesis-tests-python/index.html#example-code-for-a-two-sample-unpaired-z-test",
    "title": "Parametric hypothesis tests with examples in Python",
    "section": "Example code for a two sample unpaired z-test",
    "text": "Example code for a two sample unpaired z-test\n\nfrom statsmodels.stats.weightstats import ztest as ztest\nimport random\n\nmask1 = (dat['Sex'] == \"Women\") & (dat['Year'] == 1985)\nx1 = dat[mask1]['BMI']\nx1 = x1.array.dropna()\nx1 = random.sample(x1.tolist(), k = 300)\n\nmask2 = (dat['Sex'] == \"Women\") & (dat['Year'] == 2017)\nx2 = dat[mask2]['BMI']\nx2 = x2.array.dropna()\nx2 = random.sample(x2.tolist(), k = 300)\n\nz_statistics, p_value = ztest(x1, x2, value=0) \n\nprint(\"z-statistic:\", z_statistics)\nprint(\"p-value:\", p_value)\n\nz-statistic: -9.201889936608346\np-value: 3.517084717411295e-20"
  },
  {
    "objectID": "posts/010-02-parametric-hypothesis-tests-python/index.html#example-code-for-a-two-tailed-t-test",
    "href": "posts/010-02-parametric-hypothesis-tests-python/index.html#example-code-for-a-two-tailed-t-test",
    "title": "Parametric hypothesis tests with examples in Python",
    "section": "Example code for a two-tailed t-test",
    "text": "Example code for a two-tailed t-test\n\nmask1 = (dat['Sex'] == \"Women\") & (dat['Region'] == \"Rural\") & (dat['Year'] == 1985)\nx1 = dat[mask1]['BMI']\n\nmask2 = (dat['Sex'] == \"Women\") & (dat['Region'] == \"Urban\") & (dat['Year'] == 1985)\nx2 = dat[mask2]['BMI']\n\nt_statistic, p_value = stats.ttest_ind(x1, x2, equal_var = True, nan_policy = \"omit\")\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\nt-statistic: -3.8952336023562912\np-value: 0.00011523146459551333"
  },
  {
    "objectID": "posts/010-02-parametric-hypothesis-tests-python/index.html#example-code-for-a-one-tailed-t-test",
    "href": "posts/010-02-parametric-hypothesis-tests-python/index.html#example-code-for-a-one-tailed-t-test",
    "title": "Parametric hypothesis tests with examples in Python",
    "section": "Example code for a one-tailed t-test",
    "text": "Example code for a one-tailed t-test\n\nt_statistic, p_value = stats.ttest_ind(x1, x2, equal_var = True, nan_policy = \"omit\", alternative = \"greater\")\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\nt-statistic: -3.8952336023562912\np-value: 0.9999423842677022"
  },
  {
    "objectID": "posts/010-02-parametric-hypothesis-tests-python/index.html#two-sample-paired-dependent-t-test",
    "href": "posts/010-02-parametric-hypothesis-tests-python/index.html#two-sample-paired-dependent-t-test",
    "title": "Parametric hypothesis tests with examples in Python",
    "section": "Two sample paired (dependent) t-test",
    "text": "Two sample paired (dependent) t-test\n\nt_statistic, p_value = stats.ttest_rel(x1, x2, nan_policy = \"omit\")\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\nt-statistic: -14.095486243034763\np-value: 1.426675846865914e-31"
  },
  {
    "objectID": "posts/010-02-parametric-hypothesis-tests-python/index.html#example-code-for-a-oneway-anova",
    "href": "posts/010-02-parametric-hypothesis-tests-python/index.html#example-code-for-a-oneway-anova",
    "title": "Parametric hypothesis tests with examples in Python",
    "section": "Example code for a oneway ANOVA",
    "text": "Example code for a oneway ANOVA\n\nmask1 = (dat['Sex'] == \"Men\") & (dat['Region'] == \"Rural\") & (dat['Year'] == 2017)\nx1 = dat[mask1]['BMI']\n\nmask2 = (dat['Sex'] == \"Men\") & (dat['Region'] == \"Urban\") & (dat['Year'] == 2017)\nx2 = dat[mask2]['BMI']\n\nmask3 = (dat['Sex'] == \"Men\") & (dat['Region'] == \"National\") & (dat['Year'] == 2017)\nx3 = dat[mask3]['BMI']\n\nf_value, p_value = stats.f_oneway(x1.array.dropna(), x2.array.dropna(), x3.array.dropna())\n\nprint(\"f-value statistic: \",f_value)\nprint(\"p-value: \", p_value)\n\nf-value statistic:  3.4215235158825905\np-value:  0.033309935710150805"
  },
  {
    "objectID": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html",
    "href": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html",
    "title": "Non-parametric hypothesis tests with examples in Julia",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-11-30 First draft"
  },
  {
    "objectID": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html#import-packages",
    "href": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html#import-packages",
    "title": "Non-parametric hypothesis tests with examples in Julia",
    "section": "Import packages",
    "text": "Import packages\n\nimport Pkg\nPkg.activate(\".\")\nusing CSV\nusing Plots\nusing HypothesisTests\nusing DataFrames\n\n  Activating project at `~/sandbox/dataalltheway/posts/011-01-non-parametric-hypothesis-tests-julia`"
  },
  {
    "objectID": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html#right-tailed-test",
    "href": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html#right-tailed-test",
    "title": "Non-parametric hypothesis tests with examples in Julia",
    "section": "Right tailed test",
    "text": "Right tailed test\n\npvalue(mwut_results, tail=:right)\n\n1.2040605143479147e-31"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html",
    "title": "Type inference in readr and arrow",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-11-23 This article is cross-posted from https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/ with permission."
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#bit-integers",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#bit-integers",
    "title": "Type inference in readr and arrow",
    "section": "32-bit integers",
    "text": "32-bit integers\nAnother difference between readr and arrow is the difference between how integers larger than 32 bits are read in. Natively, R can only support 32-bit integers, though it can support 64-bit integers via the bit64 package. If we create a CSV with one column containing the largest integer that R can natively support, and then another column containing that value plus 1, we get different behaviour when we import this data with readr and arrow. In readr, when we enable integer guessing, the smaller value is read in as an integer, and the larger value is read in as a double. However, once we move over to manually specifying column types, we can use vroom::col_big_integer() to use bit64 and get us a large integer column. The arrow package also uses bit64, and its integer guessing results in 64-bit integer via inference.\n\nsixty_four &lt;- data.frame(x = 2^31 - 1, y = 2^31)\n\nreadr::write_csv(sixty_four, \"sixty_four.csv\")\n\n# doubles by default\nreadr::read_csv(\"sixty_four.csv\")\n\nRows: 1 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 2\n           x          y\n       &lt;dbl&gt;      &lt;dbl&gt;\n1 2147483647 2147483648\n\n\n\n# 32 bit integer or double depending on value size\nreadr::read_csv(\"sixty_four.csv\", col_types = list(.default = col_character())) %&gt;%\n  type_convert(guess_integer = TRUE)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  x = col_integer(),\n  y = col_double()\n)\n\n\n# A tibble: 1 × 2\n           x          y\n       &lt;int&gt;      &lt;dbl&gt;\n1 2147483647 2147483648\n\n\n\n# integers by specification\nreadr::read_csv(\n  \"sixty_four.csv\",\n  col_types = list(x = col_integer(), y = vroom::col_big_integer())\n)\n\n# A tibble: 1 × 2\n           x          y\n       &lt;int&gt;    &lt;int64&gt;\n1 2147483647 2147483648\n\n\n\n# integers by inference\narrow::read_csv_arrow(\"sixty_four.csv\")\n\n# A tibble: 1 × 2\n           x          y\n       &lt;int&gt;    &lt;int64&gt;\n1 2147483647 2147483648"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#the-number-parsing-strategy",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#the-number-parsing-strategy",
    "title": "Type inference in readr and arrow",
    "section": "The “number” parsing strategy",
    "text": "The “number” parsing strategy\nOne really cool feature in readr is the “number” parsing strategy. This allows values which have been stored as character data with commas to separate the thousands to be read in as doubles. This is not supported in arrow.\n\nnumber_type &lt;- data.frame(\n  x = c(\"1,000\", \"1,250\")\n)\n\nreadr::write_csv(number_type, \"number_type.csv\")\n\n# double type, but parsed in as number in column spec shown below\nreadr::read_csv(\"number_type.csv\")\n\nRows: 2 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nnum (1): x\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 1\n      x\n  &lt;dbl&gt;\n1  1000\n2  1250\n\n\n\n# read in as character data in Arrow\narrow::read_csv_arrow(\"number_type.csv\")\n\n# A tibble: 2 × 1\n  x    \n  &lt;chr&gt;\n1 1,000\n2 1,250"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#dictionariesfactors",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#dictionariesfactors",
    "title": "Type inference in readr and arrow",
    "section": "Dictionaries/Factors",
    "text": "Dictionaries/Factors\nAnyone who’s been around long enough might remember that R’s native CSV reading function read.csv() had a default setting of importing character columns as factors (I definitely have read.csv(..., stringAsFactors=FALSE) carved into a groove in some dark corner of my memory). This default was changed in version 4.0.0, released in April 2020, reflecting the fact that in most cases users want their string data to be imported as characters unless otherwise specified. Still, some datasets contain character data which users do want to import as factors. In readr, this can be controlled by manually specifying the column as a factor\nIn arrow, if you don’t want to individually specify column types, you can set up an option to import character columns as dictionaries (the Arrow equivalent of factors), which are converted into factors.\n\ndict_type &lt;- data.frame(\n  x = c(\"yes\", \"no\", \"yes\", \"no\")\n)\n\nreadr::write_csv(dict_type, \"dict_type.csv\")\n\n# character data\nreadr::read_csv(\"dict_type.csv\")\n\nRows: 4 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): x\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 4 × 1\n  x    \n  &lt;chr&gt;\n1 yes  \n2 no   \n3 yes  \n4 no   \n\n\n\n# factor data\nreadr::read_csv(\"dict_type.csv\", col_types = list(x = col_factor()))\n\n# A tibble: 4 × 1\n  x    \n  &lt;fct&gt;\n1 yes  \n2 no   \n3 yes  \n4 no   \n\n\n\n# set up the option. there's an open ticket to make this code a bit nicer to read.\nauto_dict_option &lt;- arrow::CsvConvertOptions$create(auto_dict_encode = TRUE)\narrow::read_csv_arrow(\"dict_type.csv\", convert_options = auto_dict_option)\n\n# A tibble: 4 × 1\n  x    \n  &lt;fct&gt;\n1 yes  \n2 no   \n3 yes  \n4 no"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#custom-logicalboolean-values",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#custom-logicalboolean-values",
    "title": "Type inference in readr and arrow",
    "section": "Custom logical/boolean values",
    "text": "Custom logical/boolean values\nAnother slightly niche but potentially useful piece of functionality available in arrow is the ability to customise which values can be parsed as logical/boolean type and how they translate to TRUE/FALSE. This can be achieved by setting some custom conversion options.\n\nalternative_true_false &lt;- arrow::CsvConvertOptions$create(\n  false_values = \"no\", true_values = \"yes\"\n)\narrow::read_csv_arrow(\"dict_type.csv\", convert_options = alternative_true_false)\n\n# A tibble: 4 × 1\n  x    \n  &lt;lgl&gt;\n1 TRUE \n2 FALSE\n3 TRUE \n4 FALSE"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#using-schemas-for-manual-control-of-data-types",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#using-schemas-for-manual-control-of-data-types",
    "title": "Type inference in readr and arrow",
    "section": "Using schemas for manual control of data types",
    "text": "Using schemas for manual control of data types\nAlthough relying on the reader itself to guess your column types can work well, what if you want more precise control?\nIn readr, you can use the col_types parameter to specify column types. You can use the same parameter in arrow to use R type specifications.\n\ngiven_types &lt;- data.frame(x = c(1, 2, 3), y = c(4, 5, 6))\n\nreadr::write_csv(given_types, \"given_types.csv\")\n\nreadr::read_csv(\"given_types.csv\", col_types = list(col_integer(), col_double()))\n\n# A tibble: 3 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1     4\n2     2     5\n3     3     6\n\n\nYou can also use this shortcode specification. Here, “i” means integer and “d” means double.\n\nreadr::read_csv(\"given_types.csv\", col_types = \"id\")\n\n# A tibble: 3 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1     4\n2     2     5\n3     3     6\n\n\nIn arrow you can use the shortcodes (though not the col_*() functions), but you must specify the column names.\nWe skip the first row as our data has a header row - this is the same behaviour as when we use both names and types in readr::read_csv() which then assumes that the header row is data if we don’t skip it.\n\narrow::read_csv_arrow(\"given_types.csv\", col_names = c(\"x\", \"y\"), col_types = \"id\", skip = 1)\n\n# A tibble: 3 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1     4\n2     2     5\n3     3     6\n\n\nWhat if you want to use Arrow types instead of R types though? In this case, you need to use a schema. I won’t go into detail here, but in short, schemas are lists of fields, each of which contain a field name and a data type. You can specify a schema like this:\n\n# this gives the same result as before - because our Arrow data has been converted to the relevant R type\narrow::read_csv_arrow(\"given_types.csv\", schema = schema(x = int8(), y = float32()), skip = 1)\n\n# A tibble: 3 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1     4\n2     2     5\n3     3     6\n\n\n\n# BUT, if you don't read it in as a data frame you'll see the Arrow type\narrow::read_csv_arrow(\"given_types.csv\", schema = schema(x = int8(), y = float32()), skip = 1, as_data_frame = FALSE)\n\nTable\n3 rows x 2 columns\n$x &lt;int8&gt;\n$y &lt;float&gt;"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#further-reading",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#further-reading",
    "title": "Type inference in readr and arrow",
    "section": "Further Reading",
    "text": "Further Reading\nIf you want a much more detailed discussion of Arrow data types, see this excellent blog post by Danielle Navarro."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 13, 2023\n\n\nLinear regression for inferential and predictive modeling\n\n\nRohit Farmer\n\n\n\n\nJan 9, 2023\n\n\nParametric hypothesis tests with examples in Python\n\n\nRohit Farmer\n\n\n\n\nNov 30, 2022\n\n\nNon-parametric hypothesis tests with examples in Julia\n\n\nDhruva Sambrani\n\n\n\n\nNov 21, 2022\n\n\nType inference in readr and arrow\n\n\nNic Crane\n\n\n\n\nNov 18, 2022\n\n\nNon-parametric hypothesis tests with examples in R\n\n\nRohit Farmer\n\n\n\n\nNov 17, 2022\n\n\nParametric hypothesis tests with examples in Julia\n\n\nDhruva Sambrani\n\n\n\n\nNov 10, 2022\n\n\nParametric hypothesis tests with examples in R\n\n\nRohit Farmer\n\n\n\n\nNov 3, 2022\n\n\nRules for naming files and folders that are cross platform and helpful in datascience\n\n\nRohit Farmer\n\n\n\n\nOct 31, 2022\n\n\nHow to build a Singularity container for machine learning, data science, and chemistry\n\n\nRohit Farmer\n\n\n\n\nOct 25, 2022\n\n\nSources of open data for statistics, data science, and machine learning\n\n\nRohit Farmer\n\n\n\n\nOct 17, 2022\n\n\nA case for using Google Colab notebooks as an alternative to web servers for scientific software\n\n\nRohit Farmer\n\n\n\n\nOct 15, 2022\n\n\nHow to use Neovim or VIM editor as an IDE for R\n\n\nRohit Farmer\n\n\n\n\nOct 15, 2022\n\n\nClassify the bitter or sweet taste of compounds\n\n\nRohit Farmer\n\n\n\n\nOct 14, 2022\n\n\nHow to download a shared file from Google Drive in R\n\n\nRohit Farmer\n\n\n\n\nOct 5, 2022\n\n\nData Transformation\n\n\nRohit Farmer\n\n\n\n\nOct 5, 2022\n\n\nTweets from heads of governments and states\n\n\nRohit Farmer\n\n\n\n\n\n\nNo matching items"
  }
]