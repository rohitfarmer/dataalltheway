[
  {
    "objectID": "posts/how-to-download-a-shared-file-from-googledrive-in-r/index.html",
    "href": "posts/how-to-download-a-shared-file-from-googledrive-in-r/index.html",
    "title": "How to download a shared file from Google Drive in R",
    "section": "",
    "text": "To download a shared file with “anyone with the link” access rights from Google Drive in R, we can utilize the googledrive library from the tidyverse package. The method described here will utilize the file ID copied from the shared link. Typically googledrive package is used to work with a Google Drive of an authenticated user. However, since we are downloading a publicly shared file in this tutorial, we will work without user authentication. So, please follow the steps below.\n\n\nBelow is a share link from my Google Drive pointing to an R data frame.\nhttps://drive.google.com/file/d/1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4/view?usp=sharing\nThis string 1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4 is the file ID that we will use to download.\n\n\n\n\nif(!require(googledrive)) install.packages(\"googledrive\")\nlibrary(googledrive)\n\ndrive_deauth()\ndrive_user()\npublic_file <-  drive_get(as_id(\"1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4\"))\ndrive_download(public_file, overwrite = TRUE)\n\nThe downloaded data frame.\n\nlibrary(DT)\ndatatable(readRDS(\"hdstim-example-data.rds\"))"
  },
  {
    "objectID": "posts/tweets-from-heads-of-governments-and-states/index.html",
    "href": "posts/tweets-from-heads-of-governments-and-states/index.html",
    "title": "Tweets from heads of governments and states",
    "section": "",
    "text": "The dataset contains an Excel workbook per year with data points on the rows and features on the columns. Features include the timestamp (UTC), language in which the tweet is written, user id, user name, tweet id, and tweet text. The first version includes the data from October 2018 until September 15, 2022. After that, future releases will be quarterly. It is a textual dataset and is primarily useful for analyses related to natural language processing.\nIn the Kaggle submission, I have also included a notebook (https://www.kaggle.com/code/rohitfarmer/dont-run-tweet-collection-and-preprocessing) with the Python code that collected the tweets and the additional code that I used to pre-process the data before submission. After releasing the first data set, I updated the code and moved the bot from Python to R using the `rtweet` library instead of `tweepy`. I found `rtweet` to perform better, especially in filtering out duplicated tweets.\nIn the current setup (https://github.com/rohitfarmer/government-tweets) that is still running on my Raspberry Pi 3B+, the main bot script runs every fifteen minutes via `crontab` and fetches data that is more recent than the latest tweet collected in the previous run. The data is stored in an SQLite database which is backed up to MEGA cloud storage via Rclone once every midnight ET.\nI enjoyed the process of creating the bot and being able to run it for a couple of years, and I hope I will soon find some time to look into the data and fetch some exciting insights. But, until then, the data is available to the data science community to utilize as they please. So, please open a discussion on the Kaggle page for questions, comments, or collaborations."
  },
  {
    "objectID": "posts/data-transformation/index.html",
    "href": "posts/data-transformation/index.html",
    "title": "Data Transformation",
    "section": "",
    "text": "Data transformation is a process of performing a mathematical function on each data point used in a statistical or machine learning analysis to either satisfy the underlying assumptions of a statistical test (e.g., normal distribution for a t-test), help a machine-learning algorithm to converge faster and or make a visualization interpretable. In addition to statistical analyses and modeling, data transformation can also be helpful in data visualization, for example, performing a log transformation on a skewed data set to plot it in a relatively unskewed and visually appealing scatter plot. Most of the data transformation methods are invertible and original values of a data set can be recovered by implementing a counter mathematical function. In mathematical form it can be expressed as:\n\\[x' = f(x)\\]\nWhere \\(x\\) is the original data, \\(x'\\) is the transformed data, and \\(f(x)\\) is a mathematical function performed on \\(x\\).\nIn data science, data transformation is also sometimes combined with the data cleaning step. In addition to performing a mathematical function to the data points, they are also checked for quality, for example, checking for missing values. I will discuss data cleaning procedures elsewhere. Data transformation can be considered as an umbrella term for both data scaling and data normalization. They are frequently used interchangeably, sometimes referring to the same mathematical operation. Although data scaling and normalization are used to achieve a similar result, it is better to understand them as two different operations that are happening under the hood.\nAlthough every data transformation method performs a mathematical operation on every data point (e.i. element wise), for some, this operation is not influenced if data points are either removed or added to the data set. Let’s consider a data set in the form of a two-dimensional data table with samples on the row and features on the column. Now take two methods to compare 1) log transformation 2) min-max scaling. In log transformation \\(log(x)\\), a log is taken for every data point individually, and the result will not change if some rows or columns are dropped or added in our example data table. However, in min-max scaling\n\\[x' = x-min(x)/max(x)-min(x)\\]\nthat is performed feature-wise (columns); if the data point that was selected as a min or max in a previous transformation is removed, then re-doing the transformation will change the result. The removal of a data point may happen; for example, if the min or max value selected in the first iteration was an outlier or that a particular sample had multiple missing values, and therefore, it had to be removed, amongst others. Min-max scaling will also influence if more data points are added to our data set. It may bring a new min or max data point and hence will change the scaling. Therefore while selecting a data transformation method, it must be noted if data points are dropped in the subsequent analysis, then should you perform the transformation again as a result of data point loss or it will be indifferent."
  },
  {
    "objectID": "posts/data-transformation/index.html#log-transformation",
    "href": "posts/data-transformation/index.html#log-transformation",
    "title": "Data Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\nIn a log transformation, logarithm is calculated for every value in the data set. Traditionally, log transformation is carried out to reduce the skewness of data or to bring data closer to a normal distribution. Usually the base to the log doesn’t matter unless it is a domain specific requirement. However, every feature of the data set should be transformed with the same base. Most of the programming languages have a core function to calculate the log of a number. In programming languages that support vector operation, for example, R, the same log function can be performed on both a single value or on all the values within a data frame, vector or matrix.\nFor example, let’s visualize the effect of log transformation on a synthetically generated dummy data. To generated figures Figure 1 and Figure 2, I have randomly sampled 10,000 positive real numbers from a skewed (positive and negative) normal distribution and performed a log transformation on every data point. The left sub-panel shows a histogram of the non-transformed data, and the right sub-panel shows a histogram of the log-transformed data. Although log transformation is known for reducing the skewness of the data and making the distribution more symmetric around the mean, it holds only for the positively skewed data. If the data are negatively skewed a log transformation will skew it further. In case of a negatively skewed data doing a power transformation may help to reduce the skewness (figure Figure 3). Usually raising the data to a power of 2 has slight effect on the skewness; a higher number may be required. In addition to the visual inspection, we can also numerically quantify the skewness of the data; that is mentioned in the figure caption.\nLog Transformation: \\[x' = log(x)\\]\nPower Transformation: \\[x' = x^n\\]\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import skewnorm\nfrom scipy.stats import skew \nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\n\n# Generate random data points from a skewed normal distribution\ndata_pos = np.round(skewnorm.rvs(10, size=10000, loc=1, random_state = 101), decimals = 2)\n#print('Skewness for the positively (right) skewed data before transformation : ', round(skew(data_pos), 2)) \ndata_neg = np.round(skewnorm.rvs(-10, size=10000, loc=10, random_state = 101), decimals = 2)\n#print('Skewness for the negatively (left) skewed data before transformation : ', round(skew(data_neg), 2)) \n\n# Log transform the data\nlog_data_pos = np.log(data_pos)\n#print('Skewness for the positively skewed data after transformation : ', round(skew(log_data_pos), 2)) \nlog_data_neg = np.log(data_neg)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(log_data_neg), 2)) \n\n\n\n\nCode\nfig, axs = plt.subplots(ncols=2, sharey = \"all\")\nfig.set_size_inches(12,4)\nsns.set_context(context = \"paper\", font_scale = 1.5)\nsns.set_palette('Dark2')\nh1 = sns.histplot(data_pos, bins = 20, ax=axs[0], kde = False)\nh1.set_title(\"Non-Transformed Data\")\nh1.set_xlabel(\"Feature\")\nh1.xaxis.set_major_locator(ticker.MultipleLocator(0.5))\nh1.set_ylabel(\"Frequency\")\nh2 = sns.histplot(log_data_pos, bins = 20, ax=axs[1], kde = False)\nh2.set_title(\"Log-Transformed Data\")\nh2.xaxis.set_major_locator(ticker.MultipleLocator(0.4))\nh2.set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 1: Histogram of the positively skewed data and its log transformation. The skewness for the non-transformed data (left) is 0.9 and for the log-transformed data (right) is 0.2.\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(ncols=2, sharey = \"all\")\nfig.set_size_inches(12,4)\nsns.set_context(context = \"paper\", font_scale = 1.5)\nsns.set_palette('Dark2')\nh1 = sns.histplot(data_neg, bins = 20, ax=axs[0], kde = False)\nh1.set_title(\"Non-Transformed Data\")\nh1.set_xlabel(\"Feature\")\nh1.xaxis.set_major_locator(ticker.MultipleLocator(0.5))\nh1.set_ylabel(\"Frequency\")\nh2 = sns.histplot(log_data_neg, bins = 20, ax=axs[1], kde = False)\nh2.set_title(\"Log-Transformed Data\")\nh2.set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 2: Histogram of the negatively skewed data and its log transformation. The skewness for the non-transformed data (left) is -0.9 and for the log-transformed data (right) is -1.2.\n\n\n\n\n\n\nCode\n# Square data.\npow_data_neg = np.power(data_neg, 6)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(pow_data_neg), 2)) \nfig, axs = plt.subplots(ncols=2, sharey = \"all\")\nfig.set_size_inches(12,4)\nsns.set_context(context = \"paper\", font_scale = 1.5)\nsns.set_palette('Dark2')\nh1 = sns.histplot(data_neg, bins = 20, ax=axs[0], kde = False)\nh1.set_title(\"Non-Transformed Data\")\nh1.set_xlabel(\"Feature\")\nh1.xaxis.set_major_locator(ticker.MultipleLocator(0.5))\nh1.set_ylabel(\"Frequency\")\nh2 = sns.histplot(pow_data_neg, bins = 20, ax=axs[1], kde = False)\nh2.set_title(\"Power-Transformed Data\")\nh2.set_xlabel(\"Feature\")\nh2.xaxis.set_major_locator(ticker.MultipleLocator(400000))\nplt.show()\n\n\n\n\n\nFigure 3: Histogram of the negatively skewed data and its power transformation. Data is raised to the power ot 6. The skewness for the non-transformed data (left) is -0.9 and for the power-transformed data (right) is -0.3.\n\n\n\n\nNote: Since the data used in these figures are sampled from a skewed normal distribution the skewness calculated here are below 2. For a non-normally distributed skewed data it would be higher than 2. Log transformation is often used to bring a non-normal distribution closer to a normal distribution.\nLog transformation can only be performed on positive values. Mathematics principles doesn’t allow log calculation on negative values. In case our input data contains negative values and a log like transformation is desired inverse hyperbolic sin (arcsinh) transformation method can be used."
  },
  {
    "objectID": "posts/data-transformation/index.html#arcsinh-transformation",
    "href": "posts/data-transformation/index.html#arcsinh-transformation",
    "title": "Data Transformation",
    "section": "Arcsinh Transformation",
    "text": "Arcsinh Transformation\nInverse hyperbolic sin transformation is a non-linear transformation that is often used in situations where a log transformations can’t be used; such as in the presence of negative values. Flow and mass cytometry are popular examples where arcsinh transformation is a almost always a method of choice. Reason being older flow cytometry machines produced positive values that were displayed on a log scale. However, newer machines can produce both negative and positive values that can’t be displayed on a log scale. Therefore, to keep the data resemble a log transformation arcsinh transformation is used.\nArcsinh transformation can also be tweaked by using a cofactor to behave differently around zero. For both negative and positive values starting from zero to cofactor are presented in a linear fashion along the lines of raw data values and values beyond he cofactor are presented in a log like fashion. In flow and mass cytometry a cofactor of 150 and 5 are used respectively.\nFor all real x: \\[arcsinh(x) = log(x + \\sqrt{x^2 + 1})\\]\nLet’s use similar positively skewed data as in the log transformation to visualize how an arcsinh transformation affects the shape of the distribution. The only change that I would want to do in this data set is to add few negative values. As I mentioned earlier that our mathematical laws doesn’t allow us to take log on negative numbers arcsinh transformation is capable of transforming small negative values closer to zero. Figures Figure 4 and Figure 5 show the histograms comparing the original and the arcsinh transformed data for positive and negatively skewed data respectively. From the figures it’s evident that unlike log, arcsinh transformation works on both positively and negatively skewed data equally well.\n\n\nCode\n# Generate random data points from a skewed normal distribution\ndata_pos = np.round(skewnorm.rvs(10, size=10000, loc=0, random_state = 101), decimals = 2)\n#print('Skewness for the positively (right) skewed data before transformation : ', round(skew(data_pos), 2))\ndata_neg = np.round(skewnorm.rvs(-10, size=10000, loc=0, random_state = 101), decimals = 2)\n#print('Skewness for the negatively (left) skewed data before transformation : ', round(skew(data_neg), 2)) \n\n# Arcsinh transform the data\narcsinh_data_pos = np.arcsinh(data_pos)\n#print('Skewness for the positively skewed data after transformation : ', round(skew(arcsinh_data_pos), 2)) \narcsinh_data_neg = np.arcsinh(data_neg)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(arcsinh_data_neg), 2)) \n\n\n\n\nCode\nfig, axs = plt.subplots(ncols=2, sharey = \"all\")\nfig.set_size_inches(12,4)\nsns.set_context(context = \"paper\", font_scale = 1.5)\nsns.set_palette('Dark2')\nh1 = sns.histplot(data_pos, bins = 20, ax=axs[0], kde = False)\nh1.set_title(\"Non-Transformed Data\")\nh1.set_xlabel(\"Feature\")\nh1.xaxis.set_major_locator(ticker.MultipleLocator(0.5))\nh1.set_ylabel(\"Frequency\")\nh2 = sns.histplot(arcsinh_data_pos, bins = 20, ax=axs[1], kde = False)\nh2.set_title(\"Arcsinh-Transformed Data\")\nh2.xaxis.set_major_locator(ticker.MultipleLocator(0.4))\nh2.set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 4: Histogram of the positively skewed data and its arcsinh transformation. The skewness for the non-transformed data (left) is 0.9 and for the arcsinh-transformed data (right) is 0.3.\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(ncols=2, sharey = \"all\")\nfig.set_size_inches(12,4)\nsns.set_context(context = \"paper\", font_scale = 1.5)\nsns.set_palette('Dark2')\nh1 = sns.histplot(data_neg, bins = 20, ax=axs[0], kde = False)\nh1.set_title(\"Non-Transformed Data\")\nh1.set_xlabel(\"Feature\")\nh1.xaxis.set_major_locator(ticker.MultipleLocator(0.5))\nh1.set_ylabel(\"Frequency\")\nh2 = sns.histplot(arcsinh_data_neg, bins = 20, ax=axs[1], kde = False)\nh2.set_title(\"Arcsinh-Transformed Data\")\nh2.set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 5: Histogram of the negatively skewed data and its arcsinh transformation. The skewness for the non-transformed data (left) is -0.9 and for the arcsinh-transformed data (right) is -0.3."
  },
  {
    "objectID": "posts/data-transformation/index.html#min-max-scaling",
    "href": "posts/data-transformation/index.html#min-max-scaling",
    "title": "Data Transformation",
    "section": "Min-Max Scaling",
    "text": "Min-Max Scaling\nIn min-max scaling for a given feature, we subtract the minimum value from each value and divide the residual by the difference between the maximum and the minimum value. The resulting transformed data is scaled between 0 and 1.\n\\[minmax(x) = x - min(x) / max(x) - min(x)\\]\nMin-max scaling can also be modified to scale the values to the desired range, for example, between -1 and 1.\n\\[minmax(x) = ((b - a) * (x - min(x)) / max(x) - min(x)) +  a\\]\nWhere \\(a\\) and \\(b\\) are the minimum and maximum range respectively.\n\nApplication(s)\n\nNeural networks"
  },
  {
    "objectID": "posts/data-transformation/index.html#standardization",
    "href": "posts/data-transformation/index.html#standardization",
    "title": "Data Transformation",
    "section": "Standardization",
    "text": "Standardization\nStandardization is also known as z-scaling, mean removal, or variance scaling. In standardization, the goal is to scale the data with a mean of zero and a standard deviation of one.\n\\[z = (x - \\mu)/\\sigma\\]\nWhere \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of a given feature. Then, the distribution of the transformed data is called the z-distribution.\n\nApplication(s)\n\nPrincipal Component Analysis (PCA)\nIn heatmaps to compare data among samples"
  },
  {
    "objectID": "posts/data-transformation/index.html#quantile-normalization",
    "href": "posts/data-transformation/index.html#quantile-normalization",
    "title": "Data Transformation",
    "section": "Quantile Normalization",
    "text": "Quantile Normalization\nQuantile normalization (QN) is a technique to make two distribution identical in statistical properties. QN involves first ranking the feature of each sample by magnitude, calculating the average value for genes occupying the same rank, and then substituting the values of all genes occupying that particular rank with this average value. The next step is to reorder the features of each sample in their original order."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data All The Way",
    "section": "",
    "text": "This is a tutorial website with concepts and ready code (R or Python) on various topics related to data science, statistics, and machine learning. I started this website to document the code I saved in GitHub gists. However, after a while, I realized that too many GitHub gists are harder to manage, and also search engines might not index them for others to use. Therefore, a website like this would serve both purposes.\nI intend to write posts as complete and explanatory as possible. However, many posts could just be code with a minimal description. If you want to get involved and help improve an article, please leave a comment or open a pull request at https://github.com/rohitfarmer/dataalltheway.\nI hope you will find this resource helpful.\nTo know more about my other projects and research, please visit at www.rohitfarmer.com."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nOct 14, 2022\n\n\nHow to download a shared file from Google Drive in R\n\n\nRohit Farmer\n\n\n\n\nOct 5, 2022\n\n\nTweets from heads of governments and states\n\n\nRohit Farmer\n\n\n\n\nOct 5, 2022\n\n\nData Transformation\n\n\nRohit Farmer\n\n\n\n\n\n\nNo matching items"
  }
]