[
  {
    "objectID": "posts/002-tweets-from-heads-of-governments-and-states/index.html",
    "href": "posts/002-tweets-from-heads-of-governments-and-states/index.html",
    "title": "Tweets from heads of governments and states",
    "section": "",
    "text": "Since October 2018, I have been maintaining a bot written in Python and running on a Raspberry Pi 3B+ that collects tweets from heads of governments (worldwide) followed by https://twitter.com/headoffice. It was an excellent exercise learning Python, Twitter API, SQLite database, and using a Raspberry Pi for hobby projects. I have now released the data on Kaggle at https://doi.org/10.34740/KAGGLE/DSV/4208877for the community to use.\nThe dataset contains an Excel workbook per year with data points on the rows and features on the columns. Features include the timestamp (UTC), language in which the tweet is written, user id, user name, tweet id, and tweet text. The first version includes the data from October 2018 until September 15, 2022. After that, future releases will be quarterly. It is a textual dataset and is primarily useful for analyses related to natural language processing.\nIn the Kaggle submission, I have also included a notebook (https://www.kaggle.com/code/rohitfarmer/dont-run-tweet-collection-and-preprocessing) with the Python code that collected the tweets and the additional code that I used to pre-process the data before submission. After releasing the first data set, I updated the code and moved the bot from Python to R using the rtweet library instead of tweepy. I found rtweet to perform better, especially in filtering out duplicated tweets.\nIn the current setup (https://github.com/rohitfarmer/government-tweets) that is still running on my Raspberry Pi 3B+, the main bot script runs every fifteen minutes via crontab and fetches data that is more recent than the latest tweet collected in the previous run. The data is stored in an SQLite database which is backed up to MEGA cloud storage via Rclone once every midnight ET.\nI enjoyed the process of creating the bot and being able to run it for a couple of years, and I hope I will soon find some time to look into the data and fetch some exciting insights. But, until then, the data is available to the data science community to utilize as they please. So, please open a discussion on the Kaggle page for questions, comments, or collaborations.\n\n\n\nCitationBibTeX citation:@online{farmer2022,\n  author = {Rohit Farmer},\n  title = {Tweets from Heads of Governments and States},\n  date = {2022-10-05},\n  url = {https://www.dataalltheway.com/posts/002-tweets-from-heads-of-governments-and-states},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRohit Farmer. 2022. “Tweets from Heads of Governments and\nStates.” October 5, 2022. https://www.dataalltheway.com/posts/002-tweets-from-heads-of-governments-and-states."
  },
  {
    "objectID": "posts/001-data-transformation/index.html",
    "href": "posts/001-data-transformation/index.html",
    "title": "Data Transformation",
    "section": "",
    "text": "Data transformation is a process of performing a mathematical function on each data point used in a statistical or machine learning analysis to either satisfy the underlying assumptions of a statistical test (e.g., normal distribution for a t-test), help a machine-learning algorithm to converge faster and or make a visualization interpretable. In addition to statistical analyses and modeling, data transformation can also be helpful in data visualization, for example, performing a log transformation on a skewed data set to plot it in a relatively unskewed and visually appealing scatter plot. Most of the data transformation methods are invertible and original values of a data set can be recovered by implementing a counter mathematical function. In mathematical form it can be expressed as:\n\\[x' = f(x)\\]\nWhere \\(x\\) is the original data, \\(x'\\) is the transformed data, and \\(f(x)\\) is a mathematical function performed on \\(x\\).\nIn data science, data transformation is also sometimes combined with the data cleaning step. In addition to performing a mathematical function to the data points, they are also checked for quality, for example, checking for missing values. I will discuss data cleaning procedures elsewhere. Data transformation can be considered as an umbrella term for both data scaling and data normalization. They are frequently used interchangeably, sometimes referring to the same mathematical operation. Although data scaling and normalization are used to achieve a similar result, it is better to understand them as two different operations that are happening under the hood.\nAlthough every data transformation method performs a mathematical operation on every data point (e.i. element wise), for some, this operation is not influenced if data points are either removed or added to the data set. Let’s consider a data set in the form of a two-dimensional data table with samples on the row and features on the column. Now take two methods to compare 1) log transformation 2) min-max scaling. In log transformation \\(log(x)\\), a log is taken for every data point individually, and the result will not change if some rows or columns are dropped or added in our example data table. However, in min-max scaling\n\\[x' = x-min(x)/max(x)-min(x)\\]\nthat is performed feature-wise (columns); if the data point that was selected as a min or max in a previous transformation is removed, then re-doing the transformation will change the result. The removal of a data point may happen; for example, if the min or max value selected in the first iteration was an outlier or that a particular sample had multiple missing values, and therefore, it had to be removed, amongst others. Min-max scaling will also influence if more data points are added to our data set. It may bring a new min or max data point and hence will change the scaling. Therefore while selecting a data transformation method, it must be noted if data points are dropped in the subsequent analysis, then should you perform the transformation again as a result of data point loss or it will be indifferent."
  },
  {
    "objectID": "posts/001-data-transformation/index.html#log-transformation",
    "href": "posts/001-data-transformation/index.html#log-transformation",
    "title": "Data Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\nIn a log transformation, logarithm is calculated for every value in the data set. Traditionally, log transformation is carried out to reduce the skewness of data or to bring data closer to a normal distribution. Usually the base to the log doesn’t matter unless it is a domain specific requirement. However, every feature of the data set should be transformed with the same base. Most of the programming languages have a core function to calculate the log of a number. In programming languages that support vector operation, for example, R, the same log function can be performed on both a single value or on all the values within a data frame, vector or matrix.\nFor example, let’s visualize the effect of log transformation on a synthetically generated dummy data. To generated figures Figure 1 and Figure 2, I have randomly sampled 10,000 positive real numbers from a skewed (positive and negative) normal distribution and performed a log transformation on every data point. The left sub-panel shows a histogram of the non-transformed data, and the right sub-panel shows a histogram of the log-transformed data. Although log transformation is known for reducing the skewness of the data and making the distribution more symmetric around the mean, it holds only for the positively skewed data. If the data are negatively skewed a log transformation will skew it further. In case of a negatively skewed data doing a power transformation may help to reduce the skewness (figure Figure 3). Usually raising the data to a power of 2 has slight effect on the skewness; a higher number may be required. In addition to the visual inspection, we can also numerically quantify the skewness of the data; that is mentioned in the figure caption.\nLog Transformation: \\[x' = log(x)\\]\nPower Transformation: \\[x' = x^n\\]\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import skewnorm\nfrom scipy.stats import skew \nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Generate random data points from a skewed normal distribution\ndata_pos = np.round(skewnorm.rvs(10, size=10000, loc=1, random_state = 101), decimals = 2)\n#print('Skewness for the positively (right) skewed data before transformation : ', round(skew(data_pos), 2)) \ndata_neg = np.round(skewnorm.rvs(-10, size=10000, loc=10, random_state = 101), decimals = 2)\n#print('Skewness for the negatively (left) skewed data before transformation : ', round(skew(data_neg), 2)) \n\n# Log transform the data\nlog_data_pos = np.log(data_pos)\n#print('Skewness for the positively skewed data after transformation : ', round(skew(log_data_pos), 2)) \nlog_data_neg = np.log(data_neg)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(log_data_neg), 2)) \n\n\n\n\nCode\nfig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n\n# We can set the number of bins with the *bins* keyword argument.\naxs[0].hist(data_pos, bins=20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(log_data_pos, bins=20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Log-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 1: Histogram of the positively skewed data and its log transformation. The skewness for the non-transformed data (left) is 0.9 and for the log-transformed data (right) is 0.2.\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\naxs[0].hist(data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(log_data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Log-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 2: Histogram of the negatively skewed data and its log transformation. The skewness for the non-transformed data (left) is -0.9 and for the log-transformed data (right) is -1.2.\n\n\n\n\n\n\nCode\n# Square data.\npow_data_neg = np.power(data_neg, 6)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(pow_data_neg), 2)) \nfig, axs = plt.subplots(ncols=2, sharey = \"all\", tight_layout=True)\naxs[0].hist(data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(pow_data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Power-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 3: Histogram of the negatively skewed data and its power transformation. Data is raised to the power ot 6. The skewness for the non-transformed data (left) is -0.9 and for the power-transformed data (right) is -0.3.\n\n\n\n\nNote: Since the data used in these figures are sampled from a skewed normal distribution the skewness calculated here are below 2. For a non-normally distributed skewed data it would be higher than 2. Log transformation is often used to bring a non-normal distribution closer to a normal distribution.\nLog transformation can only be performed on positive values. Mathematics principles doesn’t allow log calculation on negative values. In case our input data contains negative values and a log like transformation is desired inverse hyperbolic sin (arcsinh) transformation method can be used."
  },
  {
    "objectID": "posts/001-data-transformation/index.html#arcsinh-transformation",
    "href": "posts/001-data-transformation/index.html#arcsinh-transformation",
    "title": "Data Transformation",
    "section": "Arcsinh Transformation",
    "text": "Arcsinh Transformation\nInverse hyperbolic sin transformation is a non-linear transformation that is often used in situations where a log transformations can’t be used; such as in the presence of negative values. Flow and mass cytometry are popular examples where arcsinh transformation is a almost always a method of choice. Reason being older flow cytometry machines produced positive values that were displayed on a log scale. However, newer machines can produce both negative and positive values that can’t be displayed on a log scale. Therefore, to keep the data resemble a log transformation arcsinh transformation is used.\nArcsinh transformation can also be tweaked by using a cofactor to behave differently around zero. For both negative and positive values starting from zero to cofactor are presented in a linear fashion along the lines of raw data values and values beyond he cofactor are presented in a log like fashion. In flow and mass cytometry a cofactor of 150 and 5 are used respectively.\nFor all real x: \\[arcsinh(x) = log(x + \\sqrt{x^2 + 1})\\]\nLet’s use similar positively skewed data as in the log transformation to visualize how an arcsinh transformation affects the shape of the distribution. The only change that I would want to do in this data set is to add few negative values. As I mentioned earlier that our mathematical laws doesn’t allow us to take log on negative numbers arcsinh transformation is capable of transforming small negative values closer to zero. Figures Figure 4 and Figure 5 show the histograms comparing the original and the arcsinh transformed data for positive and negatively skewed data respectively. From the figures it’s evident that unlike log, arcsinh transformation works on both positively and negatively skewed data equally well.\n\n\nCode\n# Generate random data points from a skewed normal distribution\ndata_pos = np.round(skewnorm.rvs(10, size=10000, loc=0, random_state = 101), decimals = 2)\n#print('Skewness for the positively (right) skewed data before transformation : ', round(skew(data_pos), 2))\ndata_neg = np.round(skewnorm.rvs(-10, size=10000, loc=0, random_state = 101), decimals = 2)\n#print('Skewness for the negatively (left) skewed data before transformation : ', round(skew(data_neg), 2)) \n\n# Arcsinh transform the data\narcsinh_data_pos = np.arcsinh(data_pos)\n#print('Skewness for the positively skewed data after transformation : ', round(skew(arcsinh_data_pos), 2)) \narcsinh_data_neg = np.arcsinh(data_neg)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(arcsinh_data_neg), 2)) \n\n\n\n\nCode\nfig, axs = plt.subplots(ncols=2, sharey = \"all\", tight_layout=True)\naxs[0].hist(data_pos, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(arcsinh_data_pos, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Arcsinh-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 4: Histogram of the positively skewed data and its arcsinh transformation. The skewness for the non-transformed data (left) is 0.9 and for the arcsinh-transformed data (right) is 0.3.\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(ncols=2, sharey = \"all\", tight_layout=True)\naxs[0].hist(data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(arcsinh_data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Arcsinh-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 5: Histogram of the negatively skewed data and its arcsinh transformation. The skewness for the non-transformed data (left) is -0.9 and for the arcsinh-transformed data (right) is -0.3."
  },
  {
    "objectID": "posts/001-data-transformation/index.html#min-max-scaling",
    "href": "posts/001-data-transformation/index.html#min-max-scaling",
    "title": "Data Transformation",
    "section": "Min-Max Scaling",
    "text": "Min-Max Scaling\nIn min-max scaling for a given feature, we subtract the minimum value from each value and divide the residual by the difference between the maximum and the minimum value. The resulting transformed data is scaled between 0 and 1.\n\\[minmax(x) = x - min(x) / max(x) - min(x)\\]\nMin-max scaling can also be modified to scale the values to the desired range, for example, between -1 and 1.\n\\[minmax(x) = ((b - a) * (x - min(x)) / max(x) - min(x)) +  a\\]\nWhere \\(a\\) and \\(b\\) are the minimum and maximum range respectively.\n\nApplication(s)\n\nNeural networks"
  },
  {
    "objectID": "posts/001-data-transformation/index.html#standardization",
    "href": "posts/001-data-transformation/index.html#standardization",
    "title": "Data Transformation",
    "section": "Standardization",
    "text": "Standardization\nStandardization is also known as z-scaling, mean removal, or variance scaling. In standardization, the goal is to scale the data with a mean of zero and a standard deviation of one.\n\\[z = (x - \\mu)/\\sigma\\]\nWhere \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of a given feature. Then, the distribution of the transformed data is called the z-distribution.\n\nApplication(s)\n\nPrincipal Component Analysis (PCA)\nIn heatmaps to compare data among samples"
  },
  {
    "objectID": "posts/001-data-transformation/index.html#quantile-normalization",
    "href": "posts/001-data-transformation/index.html#quantile-normalization",
    "title": "Data Transformation",
    "section": "Quantile Normalization",
    "text": "Quantile Normalization\nQuantile normalization (QN) is a technique to make two distribution identical in statistical properties. QN involves first ranking the feature of each sample by magnitude, calculating the average value for genes occupying the same rank, and then substituting the values of all genes occupying that particular rank with this average value. The next step is to reorder the features of each sample in their original order."
  },
  {
    "objectID": "posts/006-google-colab-for-scientific-software/index.html",
    "href": "posts/006-google-colab-for-scientific-software/index.html",
    "title": "A case for using Google Colab notebooks as an alternative to web servers for scientific software",
    "section": "",
    "text": "Updates\n\n\n\n\n\n2022-10-18 Typo correction and included a list of links to learn more about Google Colab.\n2022-10-20 The title and description changed. A PDF version of the article is uploaded to Zenodo at https://doi.org/10.5281/zenodo.7232109\n\n\n\nI recently came across ColabFold (Mirdita et al. 2022), a slimmer and faster implementation of AlphaFold2 (Jumper et al. 2021) (the famous protein structure prediction software from DeepMind) implemented on Google Colab in the form of a Jupyter notebook, giving it an easy-to-use web server-like interface. I found this idea intriguing as it removes the overhead of maintaining a webserver while providing a web-based graphical user interface.\nGoogle Colab is a free (with options for pro subscriptions) Jupyter notebook environment for Python (R indirectly) provided by Google that runs on unoccupied Google servers. This free resource also includes access to GPU and TPU making it attractive to various machine learning and data science tasks. For the most part, Google Colab is utilized in machine learning and data science education. However, following the example of ColabFold and my implementation of ColabHDStIM, I want to make a case that it can also be used for providing an easy-to-use interface or live demo for scientific software without maintaining the complex infrastructure of a web server.\nComing from a bioinformatics/computational biology background, I know there is a craze for developing web servers worldwide. However, although many web servers are created yearly, many groups, especially in developing countries, lack the resources to build one. On the flip side, many of these initially well-funded web servers are either of low quality, are not kept updated, or go offline soon after the publication, thus squandering the resources (Veretnik, Fink, and Bourne 2008; Schultheiss et al. 2011; Kern, Fehlmann, and Keller 2020). Therefore, there is a need for an alternative where scientists can distribute their software in an easy-to-use interface like interactive notebooks. Even if the notebook environments are limited in executing production-scale software, they can still be utilized to provide a live demo on a minimal dataset. In my opinion, it is better than the vignettes accompanying software. \nBelow are some pros and cons of using Google Colab.\nPros\n\nEasy to implement\nFree hardware resources from Google, including GPU and TPU\nOption to buy more resources from Google as per need\nWhile hosted on Google’s server, the same notebook can be executed using a local runtime to take advantage of local hardware resources.\nForkable and hackable if the original maintainer stops the development.\n\nCons\n\nFree hardware resources can be limiting\nUploading and downloading data to a Colab is slow and require a workaround\nAll the instances are transient; therefore, on every restart, all the required software is re-installed, which takes time.\nColab notebooks are meant to run interactively; therefore, maintaining a long background session is hard or impossible.\nColab primarily supports Python and requires workarounds to support other languages.\n\nLearn more about Google Colab\n\nGoogle Colab frequently asked questions\nWelcome to Colab!\nPractical introduction to Google Colab for data science (YouTube video)\n\nReferences\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nKern, Fabian, Tobias Fehlmann, and Andreas Keller. 2020. “On the Lifetime of Bioinformatics Web Services.” Nucleic Acids Research 48 (22): 12523–33. https://doi.org/10.1093/nar/gkaa1125.\n\n\nMirdita, Milot, Konstantin Schütze, Yoshitaka Moriwaki, Lim Heo, Sergey Ovchinnikov, and Martin Steinegger. 2022. “ColabFold: Making Protein Folding Accessible to All.” Nature Methods 19 (6): 679–82. https://doi.org/10.1038/s41592-022-01488-1.\n\n\nSchultheiss, Sebastian J., Marc-Christian Münch, Gergana D. Andreeva, and Gunnar Rätsch. 2011. “Persistence and Availability of Web Services in Computational Biology.” Edited by Dongxiao Zhu. PLoS ONE 6 (9): e24914. https://doi.org/10.1371/journal.pone.0024914.\n\n\nVeretnik, Stella, J. Lynn Fink, and Philip E. Bourne. 2008. “Computational Biology Resources Lack Persistence and Usability.” Edited by Barbara Bryant. PLoS Computational Biology 4 (7): e1000136. https://doi.org/10.1371/journal.pcbi.1000136.\n\n\n\n\n\nCitationBibTeX citation:@article{farmer2022,\n  author = {Rohit Farmer},\n  title = {A Case for Using {Google} {Colab} Notebooks as an Alternative\n    to Web Servers for Scientific Software},\n  journal = {Zenodo},\n  date = {2022-10-17},\n  url = {https://dataalltheway.com/posts/006-google-colab-for-scientific-software/},\n  doi = {10.5281/zenodo.7232109},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRohit Farmer. 2022. “A Case for Using Google Colab Notebooks as an\nAlternative to Web Servers for Scientific Software.”\nZenodo, October. https://doi.org/10.5281/zenodo.7232109."
  },
  {
    "objectID": "posts/005-classify-the-bitter-or-sweet-taste-of-compounds/index.html",
    "href": "posts/005-classify-the-bitter-or-sweet-taste-of-compounds/index.html",
    "title": "Classify the bitter or sweet taste of compounds",
    "section": "",
    "text": "Originally posted on Kaggle at https://www.kaggle.com/datasets/rohitfarmer/classify-the-bitter-or-sweet-taste-of-compounds\n\nContext\nThroughout human evolution, we have been drawn toward sweet-tasting foods and averted from bitter tastes - sweet is good or desirable, bitter is undesirable, ear wax or medicinal. Therefore, a better understanding of molecular features that determine the bitter-sweet taste of substances is crucial for identifying natural and synthetic compounds for various purposes.\n\n\nSources\nThis dataset https://doi.org/10.34740/KAGGLE/DSV/4234193 is adapted from https://github.com/cosylabiiit/bittersweet, https://www.nature.com/articles/s41598-019-43664-y. In chemoinformatics, molecules are often represented as compact SMILES strings. In this dataset, SMILES structures, along with their names and targets (bitter, sweet, tasteless, and non-bitter), were obtained from the original study. Subsequently, SMILES were converted into canonical SMILES using RDKit, and the features (molecular descriptors, both 2D and 3D) were calculated using Mordred. Secondly, tasteless and non-bitter categories were merged into a single category of non-bitter-sweet. Finally, since many of the compounds were missing names, IUPAC names were fetched using PubChemPy for all the compounds, and for still missing names, a generic compound + incrementor name was assigned.\n\n\nInspiration\nThis is a classification dataset with the first three columns carrying names, SMILES, and canonical SMILES. Any of these columns can be used to refer to a molecule. The fourth column is the target (taste category). And all numeric features are from the 5th column until the end of the file. Many features have cells with string annotations due to errors produced by Mordred. Therefore, the following data science techniques can be learned while working on this dataset:\n\nData cleanup\nFeatures selection (since the number of features is quite large in proportion to the data points)\nFeature scaling/transformation/normalization\nDimensionality reduction\nBinomial classification (bitter vs. sweet) - utilize non-bitter-sweet as a negative class.\nMultinomial classification (bitter vs. sweet vs. non-bitter-sweet)\nSince SMILES can be converted into molecular graphs, graph-based modeling should also be possible.\n\n\n\nInitial data preparation\nA copy of the original dataset and the scripts and notebooks used to convert SMILES to canonical SMILES, generate features, fetch names, and export the final TSV file for Kaggle is loosely maintained at https://github.com/rohitfarmer/bittersweet.\n\n\n\n\nCitationBibTeX citation:@online{farmer2022,\n  author = {Rohit Farmer},\n  title = {Classify the Bitter or Sweet Taste of Compounds},\n  date = {2022-10-15},\n  url = {https://www.dataalltheway.com/posts/005-classify-the-bitter-or-sweet-taste-of-compounds},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRohit Farmer. 2022. “Classify the Bitter or Sweet Taste of\nCompounds.” October 15, 2022. https://www.dataalltheway.com/posts/005-classify-the-bitter-or-sweet-taste-of-compounds."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "",
    "text": "Note: This tutorial is written for Linux based systems."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#requirements",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#requirements",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Requirements",
    "text": "Requirements\n\nR >= 3.0.0\nTo install the latest version of R please follow the download and install instructions at https://cloud.r-project.org/\n\n\nNeovim >= 0.2.0\nNeovim (nvim) is the continuation and extension of Vim editor with the aim to keep the good parts of Vim and add more features. In this tutorial I will be using Neovim (nvim), however, most of the steps are equally applicable to Vim also. Please follow download and installation instructions on nvim’s GitHub wiki https://github.com/neovim/neovim/wiki/Installing-Neovim.\nOR\n\n\nVim >= 8.1\nVim usually comes installed in most of the Linux based operating system. However, it may not be the latest one. Therefore, to install the latest version please download and install it from Vim’s GitHub repository as mentioned below or a method that is more confortable to you.\ngit clone https://github.com/vim/vim.git\nmake -C vim/\nsudo make install -C vim/\n\n\nPlugin Manager\nThere are more than one plugin manager’s available for Vim that can be used to install the required plugins. In this tutorial I will be using vim-plug pluggin manager.\n\n\nPlugins\nIn the end below are the plugins that we would need to convert Vim editor into a fully functional IDE for R.\n\nNvim-R: https://github.com/jalvesaq/Nvim-R\n\nNvim-R is the main plugin that will add the functionality to execute R code from within the Vim editor.\n\nNcm-R: https://github.com/gaalcaras/ncm-R\n\nNcm-R adds synchronous auto completion features for R.\nIt is based on ncm2 and nvim-yarp plugins.\n\nNerd Tree: https://github.com/preservim/nerdtree\n\nNerd Tree will be used to toggle file explorer in the side panel.\n\nDelimitMate: https://github.com/Raimondi/delimitMate\n\nThis plug-in provides automatic closing of quotes, parenthesis, brackets, etc.\n\nVim-monokai-tasty: https://github.com/patstockwell/vim-monokai-tasty\n\nMonokai color scheme inspired by Sublime Text’s interpretation of monokai.\n\nLightline.vim: https://github.com/itchyny/lightline.vim\n\nLineline.vim adds asthetic enhancements to Vim’s statusline/tabline."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#procedure",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#procedure",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Procedure",
    "text": "Procedure\n\nMake sure that you have R >=3.0.0 installed.\nMake sure that you have Neovim >= 0.2.0 installed.\nInstall the vim-plug plugin manager.\n\ncurl -fLo ~/.local/share/nvim/site/autoload/plug.vim --create-dirs \\\n    https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n\nInstall the required plugins.\n\nFirst, create an init.vim file in ~/.config/nvim folder (create the folder if it doesn’t exist). This file is equivalent to a .vimrc file in the traditional Vim environment. To init.vim file start adding:\n\" Specify a directory for plugins\n\" - Avoid using standard Vim directory names like 'plugin'\ncall plug#begin('~/.vim/plugged')\n\n\" List of plugins.\n\" Make sure you use single quotes\n\n\" Shorthand notation\nPlug 'jalvesaq/Nvim-R'\nPlug 'ncm2/ncm2'\nPlug 'roxma/nvim-yarp'\nPlug 'gaalcaras/ncm-R'\nPlug 'preservim/nerdtree'\nPlug 'Raimondi/delimitMate'\nPlug 'patstockwell/vim-monokai-tasty'\nPlug 'itchyny/lightline.vim'\n\n\" Initialize plugin system\ncall plug#end()\n\nUpdate and add more features to the init.vim file.\n\n\" Set a Local Leader\n\n\" With a map leader it's possible to do extra key combinations\n\" like <leader>w saves the current file\nlet mapleader = \",\"\nlet g:mapleader = \",\"\n\n\n\" Plugin Related Settings\n\n\" NCM2\nautocmd BufEnter * call ncm2#enable_for_buffer()    \" To enable ncm2 for all buffers.\nset completeopt=noinsert,menuone,noselect           \" :help Ncm2PopupOpen for more\n                                                    \" information.\n\n\" NERD Tree\nmap <leader>nn :NERDTreeToggle<CR>                  \" Toggle NERD tree.\n\n\" Monokai-tasty\nlet g:vim_monokai_tasty_italic = 1                  \" Allow italics.\ncolorscheme vim-monokai-tasty                       \" Enable monokai theme.\n\n\" LightLine.vim \nset laststatus=2              \" To tell Vim we want to see the statusline.\nlet g:lightline = {\n   \\ 'colorscheme':'monokai_tasty',\n   \\ }\n\n\n\" General NVIM/VIM Settings\n\n\" Mouse Integration\nset mouse=i                   \" Enable mouse support in insert mode.\n\n\" Tabs & Navigation\nmap <leader>nt :tabnew<cr>    \" To create a new tab.\nmap <leader>to :tabonly<cr>     \" To close all other tabs (show only the current tab).\nmap <leader>tc :tabclose<cr>    \" To close the current tab.\nmap <leader>tm :tabmove<cr>     \" To move the current tab to next position.\nmap <leader>tn :tabn<cr>        \" To swtich to next tab.\nmap <leader>tp :tabp<cr>        \" To switch to previous tab.\n\n\n\" Line Numbers & Indentation\nset backspace=indent,eol,start  \" To make backscape work in all conditions.\nset ma                          \" To set mark a at current cursor location.\nset number                      \" To switch the line numbers on.\nset expandtab                   \" To enter spaces when tab is pressed.\nset smarttab                    \" To use smart tabs.\nset autoindent                  \" To copy indentation from current line \n                                \" when starting a new line.\nset si                          \" To switch on smart indentation.\n\n\n\" Search\nset ignorecase                  \" To ignore case when searching.\nset smartcase                   \" When searching try to be smart about cases.\nset hlsearch                    \" To highlight search results.\nset incsearch                   \" To make search act like search in modern browsers.\nset magic                       \" For regular expressions turn magic on.\n\n\n\" Brackets\nset showmatch                   \" To show matching brackets when text indicator \n                                \" is over them.\nset mat=2                       \" How many tenths of a second to blink \n                                \" when matching brackets.\n\n\n\" Errors\nset noerrorbells                \" No annoying sound on errors.\n\n\n\" Color & Fonts\nsyntax enable                   \" Enable syntax highlighting.\nset encoding=utf8                \" Set utf8 as standard encoding and \n                                 \" en_US as the standard language.\n\n\" Enable 256 colors palette in Gnome Terminal.\nif $COLORTERM == 'gnome-terminal'\n    set t_Co=256\nendif\n\ntry\n    colorscheme desert\ncatch\nendtry\n\n\n\" Files & Backup\nset nobackup                     \" Turn off backup.\nset nowb                         \" Don't backup before overwriting a file.\nset noswapfile                   \" Don't create a swap file.\nset ffs=unix,dos,mac             \" Use Unix as the standard file type.\n\n\n\" Return to last edit position when opening files\nau BufReadPost * if line(\"'\\\"\") > 1 && line(\"'\\\"\") <= line(\"$\") | exe \"normal! g'\\\"\" | endif"
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#frequently-used-keyboard-shortcutscommands",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#frequently-used-keyboard-shortcutscommands",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Frequently Used Keyboard Shortcuts/Commands",
    "text": "Frequently Used Keyboard Shortcuts/Commands\nNote: The commands below are according to the init.vim settings mentioned in this Gist.\n# Nvim-R\n\\rf               \" Connect to R console.\n\\rq               \" Quit R console.\n\\ro               \" Open object bowser.\n\\d                \" Execute current line of code and move to the next line.\n\\ss               \" Execute a block of selected code.\n\\aa               \" Execute the entire script. This is equivalent to source().\n\\xx               \" Toggle comment in an R script.\n\n# NERDTree\n,nn               \" Toggle NERDTree."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#example-code",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#example-code",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Example Code",
    "text": "Example Code\nlibrary(tidyverse)\n# \\rf               \" Connect to R console.\n# \\rq               \" Quit R console.\n# \\ro               \" Open object bowser.\n# \\d \\ss \\aa        \" Execution modes. \n# ?help\n# ,nn               \" NERDTree.\n# ,nt, tp, tn       \" Tab navigation.\n\ntheme_set(theme_bw())\ndata(\"midwest\", package = \"ggplot2\")\n\ngg  <- ggplot(midwest, aes(x=area, y = poptotal)) +\n        geom_point(aes(col = state, size = popdensity)) +\n        geom_smooth(method = \"loess\", se = F) +\n        xlim(c(0, 0.1)) +\n        ylim(c(0, 500000)) +\n        labs(subtitle = \"Area Vs Population\",\n             y = \"Population\",\n             x = \"Area\",\n             title = \"Scatterplot\",\n             caption = \"Source: midwest\")\n\nplot(gg) # Opens an external window with the plot.\n\nmidwest$county # To show synchronous auto completion. \n\nView(midwest) # Opens an external window to display a portion of the tibble."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#add-colour-etc.-to-vim-in-a-screen-session-optional",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#add-colour-etc.-to-vim-in-a-screen-session-optional",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Add Colour etc. to VIM in a Screen Session (optional)",
    "text": "Add Colour etc. to VIM in a Screen Session (optional)\nAdd these lines to ~/.screenrc file.\n# Use 256 colors\nattrcolor b \".I\"    # allow bold colors - necessary for some reason\ntermcapinfo xterm 'Co#256:AB=\\E[48;5;%dm:AF=\\E[38;5;%dm'   # tell screen how to set colors. AB = background, AF=foreground\ndefbce on    # use current bg color for erased chars]]'\n\n# Informative statusbar\nhardstatus off\nhardstatus alwayslastline\nhardstatus string '%{= kG}[ %{G}%H %{g}][%= %{= kw}%?%-Lw%?%{r}(%{W}%n*%f%t%?(%u)%?%{r})%{w}%?%+Lw%?%?%= %{g}][%{B} %m-%d %{W} %c %{g}]'\n\n# Use X scrolling mechanism\ntermcapinfo xterm* ti@:te@\n\n# Fix for residual editor text\naltscreen on"
  },
  {
    "objectID": "posts/003-how-to-download-a-shared-file-from-googledrive-in-r/index.html",
    "href": "posts/003-how-to-download-a-shared-file-from-googledrive-in-r/index.html",
    "title": "How to download a shared file from Google Drive in R",
    "section": "",
    "text": "To download a shared file with “anyone with the link” access rights from Google Drive in R, we can utilize the googledrive library from the tidyverse package. The method described here will utilize the file ID copied from the shared link. Typically googledrive package is used to work with a Google Drive of an authenticated user. However, since we are downloading a publicly shared file in this tutorial, we will work without user authentication. So, please follow the steps below.\n\n\nBelow is a share link from my Google Drive pointing to an R data frame.\nhttps://drive.google.com/file/d/1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4/view?usp=sharing\nThis string 1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4 is the file ID that we will use to download.\n\n\n\n\nif(!require(googledrive)) install.packages(\"googledrive\")\nlibrary(googledrive)\n\ndrive_deauth()\ndrive_user()\npublic_file <-  drive_get(as_id(\"1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4\"))\ndrive_download(public_file, overwrite = TRUE)\n\nFile downloaded:\n• hdstim-example-data.rds <id: 1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4>\nSaved locally as:\n• hdstim-example-data.rds\nThe downloaded data frame.\n\nlibrary(DT)\ndatatable(head(readRDS(\"hdstim-example-data.rds\")))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data All The Way",
    "section": "",
    "text": "Welcome to “data all the way,” a tutorial website with concepts, methods, and example code on various data science, statistics, and machine learning topics. This website, for me, is an attempt to deepen my understanding of data science by teaching core concepts and practical code. I intend to write blog posts that are as complete and explanatory as possible. However, many posts could just be codes with a minimal description initially, eventually evolving into more comprehensive articles. Therefore, most of the posts here are in a state of continuous development. If you want to get involved and help improve an article, please leave a comment or open a pull request at https://github.com/rohitfarmer/dataalltheway. To learn more about my other projects and research, please visit https://www.rohitfarmer.com."
  },
  {
    "objectID": "index.html#featured-posts",
    "href": "index.html#featured-posts",
    "title": "Data All The Way",
    "section": "Featured posts",
    "text": "Featured posts"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nOct 17, 2022\n\n\nA case for using Google Colab notebooks as an alternative to web servers for scientific software\n\n\nRohit Farmer\n\n\n\n\nOct 15, 2022\n\n\nClassify the bitter or sweet taste of compounds\n\n\nRohit Farmer\n\n\n\n\nOct 15, 2022\n\n\nHow to use Neovim or VIM editor as an IDE for R\n\n\nRohit Farmer\n\n\n\n\nOct 14, 2022\n\n\nHow to download a shared file from Google Drive in R\n\n\nRohit Farmer\n\n\n\n\nOct 5, 2022\n\n\nTweets from heads of governments and states\n\n\nRohit Farmer\n\n\n\n\nOct 5, 2022\n\n\nData Transformation\n\n\nRohit Farmer\n\n\n\n\n\n\nNo matching items"
  }
]