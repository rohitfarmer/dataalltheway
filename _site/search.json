[
  {
    "objectID": "contact/index.html",
    "href": "contact/index.html",
    "title": "Contact",
    "section": "",
    "text": "The best way to contact me is by email at rohit [dot] farmer [at] dataalltheway [dot] com.\nYou can also leave a comment at the bottom of this page and I will respond as soon as possible.\nI am also active on Mastodon and Twitter and would be happy to chat there.\n\n\n\n\n\n\nNote\n\n\n\nThis is a not-for-profit website without advertisements and a paywall, and I intend to support it fully in the foreseeable future. Therefore, I am not looking for any affiliate marketing or paid promotions. However, I appreciate your time and expertise in providing constructive criticism of the content on this website, suggesting corrections, contributing an article, or writing a comment on a post."
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-12-22 Section for ANOVA.\n2022-11-11 Added example code for one-tailed t-test; mentioned Welch’s t-test in a note callout.\n2022-11-10 First draft with live code on Kaggle."
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#paired-vs.-unpaired-tests",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#paired-vs.-unpaired-tests",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Paired vs. unpaired tests",
    "text": "Paired vs. unpaired tests\nPaired tests are used to compare two related data groups, such as before and after measurements. Unpaired tests are used to compare two unrelated data groups, such as men and women."
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#one-sample-vs.-two-sample-tests",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#one-sample-vs.-two-sample-tests",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "One sample vs. two sample tests",
    "text": "One sample vs. two sample tests\nThere are key differences between one-sample and two-sample hypothesis testing. Firstly, in one sample hypothesis testing, we are testing the mean of a single sample against a known population mean. In contrast, two-sample hypothesis testing involves comparing the means of two independent samples. Secondly, one-sample hypothesis testing only requires a single sample size, while two-sample hypothesis testing requires two. Finally, one-sample hypothesis testing is typically used when the population standard deviation is known, while two-sample hypothesis testing is used when the population standard deviation is unknown."
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#one-vs.-two-tailed-tests",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#one-vs.-two-tailed-tests",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "One vs. two-tailed tests",
    "text": "One vs. two-tailed tests\n\n\n\n\n\n\nNote\n\n\n\nAll the example codes in this tutorial are for two-tailed tests except one in the T-test Section 3.\n\n\nIn a hypothesis test, the null and alternate hypotheses are stated in terms of population parameters. These hypotheses are:\n\nNull hypothesis (\\(H_0\\)): The value of the population parameter is equal to the hypothesized value.\nThe alternate hypothesis (\\(H_1\\)): The value of the population parameter is not equal to the hypothesized value.\n\nThe hypothesis test is based on a sample from the population. This sample is used to test the hypotheses by deriving a test statistic. Finally, the value of the test statistic is compared to a critical value. The critical value depends on the alpha level, which is the likelihood of rejecting the null hypothesis when it is true.\nThe null and alternate hypotheses determine the direction of the test. There are two types of hypothesis tests: one-tailed and two-tailed tests.\n\nOne-tailed Test\nA one-tailed test is conducted when the null and alternate hypotheses are stated in terms of “greater than” or “less than”.\nFor example, let’s say that a company wants to test a new advertising campaign. The null hypothesis (\\(H_0\\)) is that the new campaign will have no effect on sales. The alternate hypothesis (\\(H_1\\)) is that the new campaign will increase sales.\nThe null hypothesis is stated as:\n\\(H_0\\) : The population mean is less than or equal to 10%.\nThe alternate hypothesis is stated as:\n\\(H_1\\) : The population mean is greater than 10%.\nThe test is conducted by taking a sample of data and calculating the mean. Then, the mean is compared to the critical value. The null hypothesis is rejected if the mean is greater than the critical value.\n\n\nTwo-tailed Test\nA two-tailed test is conducted when the null and alternate hypotheses are stated in terms of “not equal to”.\nTaking our advertising campaign example. The null hypothesis (\\(H_0\\)) is that the new campaign will have no effect on sales. The alternate hypothesis (\\(H_1\\)) is that the new campaign will increase or decrease sales.\nThe null hypothesis is stated as:\n\\(H_0\\) : The population mean is equal to 10%.\nThe alternate hypothesis is stated as:\n\\(H_1\\) : The population mean is not equal to 10%.\nThe test is conducted by taking a sample of data and calculating the mean. Then, the mean is compared to the critical value. The null hypothesis is rejected if the mean is not equal to the critical value."
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#the-method",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#the-method",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "The method",
    "text": "The method\nNo matter which hypothesis testing method is selected following are the steps that are executed:\n\nFirstly, identify the null hypothesis \\(H_0:\\mu = \\mu_0\\)\nThen identify the alternative hypothesis \\(H_1\\) and decide if it is of the form \\(H_1 : \\mu \\neq \\mu_0\\) (a two-tailed test) or if there is a specific direction for how the mean changes \\(H_1 : \\mu > \\mu_0\\) or \\(H_1 : \\mu < \\mu_0\\), (a one-tailed test).\nNext, calculate the test statistic. Compare the test statistic to the critical values and obtain a range for the p-value which is the probability that the difference between the two groups is due to chance. The test is usually used with a significance level of 0.05, which means that there is a 5% chance that the difference between the two groups is due to chance. However, per recommendations from the American Statistical Association we need to be careful when we state statements like “statistically significant”.\nForm conclusions. If your test statistic is greater than the critical values in the table, it is significant. You can reject the null hypothesis at that level, otherwise you accept it."
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#dataset",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#dataset",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Dataset",
    "text": "Dataset\nFor our example exercises, we will use the dataset from Open Case Studies “exploring global patterns of obesity across rural and urban regions” (Wright et al. 2020) . Body mass index (BMI) is often used as a proxy for adiposity (the condition of having excess body fat) and is measured as an individual’s weight in kilograms (\\(kg\\)) or pounds (\\(lbs\\)) divided by the individual’s height in meters (\\(m^2\\)) squared. Recently, an article published in Nature evaluated and compared the BMI of populations in rural and urban communities around the world (“Rising Rural Body-Mass Index Is the Main Driver of the Global Obesity Epidemic in Adults” 2019). The article challenged the widely-held view that increased urbanization was one of the major reasons for increased global obesity rates. This view came about because many countries around the world have shown increased urbanization levels in parallel with increased obesity rates. In this article, however, the NCD-RisC argued that this might not be the case and that in fact for most regions around the world, BMI measurements are increasing in rural populations just as much, if not more so, than urban populations. Furthermore, this study suggested that obesity has particularly increased in female populations in rural regions:\n\n“We noted a persistently higher rural BMI, especially for women.”\n\nWe will fetch the cleaned version of the data from the the Open Case Studies GitHub repository as data wrangling is out of the scope of this tutorial.\n\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(DT))\nsuppressMessages(library(kableExtra))\ndat <- readr::read_csv(\"https://raw.githubusercontent.com/opencasestudies/ocs-bp-rural-and-urban-obesity/master/data/wrangled/BMI_long.csv\", show_col_types = FALSE)\n\nDT::datatable(dat)"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-unpaired-z-test",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-unpaired-z-test",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Two sample unpaired z-test",
    "text": "Two sample unpaired z-test\n\\[\n\\begin{equation} z = \\dfrac{\\bar{x_1} - \\bar{x_2}}{\\sqrt{\\dfrac{\\sigma_1^2}{n_1} +\\dfrac{\\sigma_2^2}{n_2}}} \\end{equation}\n\\tag{2}\\]\nWhere \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) are the sample means, \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are the population variances, and \\({n_1}\\) and \\({n_2}\\) are the number of samples.\n\nExample code in R\n\nsuppressMessages(library(PASWR2))\n\n# Calculate population standard deviations\nsig_x <- dplyr::filter(dat, Sex == \"Women\", Year == 1985) %>%\n  dplyr::pull(BMI) %>% na.omit() %>% sd()\nsig_y <- dplyr::filter(dat, Sex == \"Women\", Year == 2017) %>%\n  dplyr::pull(BMI) %>% na.omit() %>% sd()\n\n# Fetch a random sample of BMI data for women in the year 1985 and 2017\nx1 <- dplyr::filter(dat, Sex == \"Women\", Year == 1985) %>%\n  dplyr::pull(BMI) %>% na.omit() %>%\n  sample(.,300)\nx2 <- dplyr::filter(dat, Sex == \"Women\", Year == 2017) %>%\n  dplyr::pull(BMI) %>% na.omit() %>%\n  sample(.,300)\n\n# Perform a two sample (unpaired) t-test between x1 and x2\n(z_res <- PASWR2::z.test(x1, x2, mu = 0, sigma.x = sig_x, sigma.y = sig_y))\n\n\n    Two Sample z-test\n\ndata:  x1 and x2\nz = -11.071, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.885719 -2.017614\nsample estimates:\nmean of x mean of y \n 24.08067  26.53233 \n\n\n\n# Fetch z-test result metrics and present them in a tidy table\nbroom::tidy(z_res) %>%\n  kbl() %>%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n \n  \n    estimate1 \n    estimate2 \n    statistic \n    p.value \n    conf.low \n    conf.high \n    method \n    alternative \n  \n \n\n  \n    24.08067 \n    26.53233 \n    -11.0705 \n    0 \n    -2.885719 \n    -2.017614 \n    Two Sample z-test \n    two.sided"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-paired-z-test",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-paired-z-test",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Two sample paired z-test",
    "text": "Two sample paired z-test\n\\[\n\\begin{equation} z= \\dfrac{\\bar{d}- D}{\\sqrt{\\dfrac{\\sigma_d^2}{n}}} \\end{equation}\n\\tag{3}\\]\nWhere \\(\\bar{d}\\) is the mean of the differences between the samples, \\(D\\) is the hypothesised mean of the differences (usually this is zero), \\(n\\) is the sample size and \\(\\sigma_d^2\\) is the population variance of the differences.\n\nExample code in R\n\n# Fetch a first 300 samples of BMI data for women in the year 1985 and 2017\nx1 <- dplyr::filter(dat, Sex == \"Women\", Year == 1985) %>%\n  dplyr::pull(BMI) %>% na.omit() \nx1 <- x1[1:300]\nx2 <- dplyr::filter(dat, Sex == \"Women\", Year == 2017) %>%\n  dplyr::pull(BMI) %>% na.omit()\nx2 <- x2[1:300]\n\n# Perform a two sample (unpaired) t-test between x1 and x2\n(z_res <- PASWR2::z.test(x1, x2, sigma.x = sig_x, sigma.y = sig_y, sigma.d = abs(sig_y - sig_x), paired = TRUE))\n\n\n    Paired z-test\n\ndata:  x1 and x2\nz = -393.33, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.36171 -2.33829\nsample estimates:\nmean of the differences \n                  -2.35 \n\n\n\n# Fetch z-test result metrics and present them in a tidy table\nbroom::tidy(z_res) %>%\n  kbl() %>%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n \n  \n    estimate \n    statistic \n    p.value \n    conf.low \n    conf.high \n    method \n    alternative \n  \n \n\n  \n    -2.35 \n    -393.3333 \n    0 \n    -2.36171 \n    -2.33829 \n    Paired z-test \n    two.sided"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-unpaired-independent-t-test",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-unpaired-independent-t-test",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Two sample unpaired (independent) t-test",
    "text": "Two sample unpaired (independent) t-test\n\\[\n\\begin{equation} t = \\dfrac{\\bar{x_1} - \\bar{x_2}}{\\sqrt{s^2\\bigg(\\dfrac{1}{n_1} + \\dfrac{1}{n_2}\\bigg)}} \\end{equation}\n\\tag{5}\\]\nWhere the pooled standard deviation \\(s\\) is:\n\\[\n\\begin{equation} s =\\sqrt{\\dfrac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 +n_2 -2}} \\end{equation}\n\\tag{6}\\]\nWhere \\(\\bar x_1\\) and \\(\\bar x_2\\) are the means from the two samples, likewise \\(n_1\\) and \\(n_2\\) are the sample sizes and \\(s_1^2\\) and \\(s_2^2\\) are the sample variances. This test statistic you will compare to t-tables on \\((n1+n2−2)\\) degrees of freedom.\n\nExample code for a two-tailed test in R\n\n# Fetch the BMI data for women from rural and urban areas in the year 1985\nx1 <- dplyr::filter(dat, Sex == \"Women\", Region == \"Rural\", Year == 1985) %>%\n  dplyr::pull(BMI) \nx2 <- dplyr::filter(dat, Sex == \"Women\", Region == \"Urban\", Year == 1985) %>%\n  dplyr::pull(BMI) \n\n# Perform a two sample (unpaired) t-test between x1 and x2\n(t_res <- t.test(x1, x2, var.equal = TRUE))\n\n\n    Two Sample t-test\n\ndata:  x1 and x2\nt = -3.8952, df = 394, p-value = 0.0001152\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.5744694 -0.5182378\nsample estimates:\nmean of x mean of y \n 23.58782  24.63417 \n\n\nWe use the var.equal = TRUE option here to use the pooled standard deviation \\(s\\) Equation 6.\n\n\n\n\n\n\nNote\n\n\n\nFor the pooled variance t-test to be appropriate you rely on the assumption that the two samples come from the same population and have equal variance. However, a modification of the t-test known as Welch’s test is said to correct for this problem by estimating the variances, and adjusting the degrees of freedom to use in the test. This correction is performed by default, but can be shut off by using the var.equal=TRUE argument as used in the example above.\n\n\n\n# Fetch t-test result metrics and present them in a tidy table\nbroom::tidy(t_res) %>%\n  kbl() %>%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n \n  \n    estimate \n    estimate1 \n    estimate2 \n    statistic \n    p.value \n    parameter \n    conf.low \n    conf.high \n    method \n    alternative \n  \n \n\n  \n    -1.046354 \n    23.58782 \n    24.63417 \n    -3.895234 \n    0.0001152 \n    394 \n    -1.574469 \n    -0.5182378 \n    Two Sample t-test \n    two.sided \n  \n\n\n\n\n\n\n\nExample code for a one-tailed test in R\nHere we will test if women in rural areas had a higher BMI than those in urban areas in 1985.\n\n# Perform a one-tailed two sample (unpaired) t-test between x1 and x2\n(t_res <- t.test(x1, x2, var.equal = TRUE, alternative = \"greater\"))\n\n\n    Two Sample t-test\n\ndata:  x1 and x2\nt = -3.8952, df = 394, p-value = 0.9999\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -1.489242       Inf\nsample estimates:\nmean of x mean of y \n 23.58782  24.63417 \n\n\n\n# Fetch t-test result metrics and present them in a tidy table\nbroom::tidy(t_res) %>%\n  kbl() %>%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n \n  \n    estimate \n    estimate1 \n    estimate2 \n    statistic \n    p.value \n    parameter \n    conf.low \n    conf.high \n    method \n    alternative \n  \n \n\n  \n    -1.046354 \n    23.58782 \n    24.63417 \n    -3.895234 \n    0.9999424 \n    394 \n    -1.489242 \n    Inf \n    Two Sample t-test \n    greater"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-paired-dependent-t-test",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#two-sample-paired-dependent-t-test",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Two sample paired (dependent) t-test",
    "text": "Two sample paired (dependent) t-test\n\\[\n\\begin{equation} t = \\dfrac{\\bar{d}}{\\sqrt{\\dfrac{s^2}{n}}} \\end{equation}\n\\tag{7}\\]\nWhere \\(\\bar d\\) is the mean of the differences between the samples. You will compare the t-statistic to the critical values in a t-table on \\((n−1)\\) degrees of freedom. Here \\(s\\) is the standard deviation of the differences.\n\nExample code in R\n\n# Perform a two sample (paired) t-test between x1 and x2\n(t_res <- t.test(x1, x2, paired = TRUE))\n\n\n    Paired t-test\n\ndata:  x1 and x2\nt = -14.095, df = 195, p-value < 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.1870263 -0.8956268\nsample estimates:\nmean difference \n      -1.041327 \n\n\n\n# Fetch t-test result metrics and present them in a tidy table\nbroom::tidy(t_res) %>%\n  kbl() %>%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n \n  \n    estimate \n    statistic \n    p.value \n    parameter \n    conf.low \n    conf.high \n    method \n    alternative \n  \n \n\n  \n    -1.041327 \n    -14.09549 \n    0 \n    195 \n    -1.187026 \n    -0.8956268 \n    Paired t-test \n    two.sided"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#example-code-in-r-4",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#example-code-in-r-4",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Example code in R",
    "text": "Example code in R\n\n(f_res <- var.test(x1, x2))\n\n\n    F test to compare two variances\n\ndata:  x1 and x2\nF = 1.3238, num df = 196, denom df = 198, p-value = 0.04957\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 1.000525 1.751931\nsample estimates:\nratio of variances \n          1.323819 \n\n\n\n# Fetch f-test result metrics and present them in a tidy table\nbroom::tidy(f_res) %>%\n  kbl() %>%\n  kable_paper(\"hover\", full_width = F)\n\nMultiple parameters; naming those columns num.df, den.df\n\n\n\n\n \n  \n    estimate \n    num.df \n    den.df \n    statistic \n    p.value \n    conf.low \n    conf.high \n    method \n    alternative \n  \n \n\n  \n    1.323819 \n    196 \n    198 \n    1.323819 \n    0.0495733 \n    1.000525 \n    1.751931 \n    F test to compare two variances \n    two.sided"
  },
  {
    "objectID": "posts/010-parametric-hypothesis-tests-r/index.html#websites",
    "href": "posts/010-parametric-hypothesis-tests-r/index.html#websites",
    "title": "Parametric hypothesis tests with examples in R",
    "section": "Website(s)",
    "text": "Website(s)\n\nParametric Hypothesis Tests (Psychology)"
  },
  {
    "objectID": "posts/002-tweets-from-heads-of-governments-and-states/index.html",
    "href": "posts/002-tweets-from-heads-of-governments-and-states/index.html",
    "title": "Tweets from heads of governments and states",
    "section": "",
    "text": "Since October 2018, I have been maintaining a bot written in Python and running on a Raspberry Pi 3B+ that collects tweets from heads of governments (worldwide) followed by https://twitter.com/headoffice. It was an excellent exercise learning Python, Twitter API, SQLite database, and using a Raspberry Pi for hobby projects. I have now released the data on Kaggle at https://doi.org/10.34740/KAGGLE/DSV/4208877 for the community to use.\nThe dataset contains an Excel workbook per year with data points on the rows and features on the columns. Features include the timestamp (UTC), language in which the tweet is written, user id, user name, tweet id, and tweet text. The first version includes the data from October 2018 until September 15, 2022. After that, future releases will be quarterly. It is a textual dataset and is primarily useful for analyses related to natural language processing.\nIn the Kaggle submission, I have also included a notebook (https://www.kaggle.com/code/rohitfarmer/dont-run-tweet-collection-and-preprocessing) with the Python code that collected the tweets and the additional code that I used to pre-process the data before submission. After releasing the first data set, I updated the code and moved the bot from Python to R using the rtweet library instead of tweepy. I found rtweet to perform better, especially in filtering out duplicated tweets.\nIn the current setup (https://github.com/rohitfarmer/government-tweets) that is still running on my Raspberry Pi 3B+, the main bot script runs every fifteen minutes via crontab and fetches data that is more recent than the latest tweet collected in the previous run. The data is stored in an SQLite database which is backed up to MEGA cloud storage via Rclone once every midnight ET.\nI enjoyed the process of creating the bot and being able to run it for a couple of years, and I hope I will soon find some time to look into the data and fetch some exciting insights. But, until then, the data is available to the data science community to utilize as they please. So, please open a discussion on the Kaggle page for questions, comments, or collaborations.\n\n\n\nCitationBibTeX citation:@dataset{farmer2022,\n  author = {Rohit Farmer},\n  publisher = {Kaggle},\n  title = {Tweets from Heads of Governments and States},\n  date = {2022-10-05},\n  url = {https://www.kaggle.com/dsv/4208877},\n  doi = {10.34740/KAGGLE/DSV/4208877},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRohit Farmer. 2022. “Tweets from Heads of Governments and\nStates.” Kaggle. https://doi.org/10.34740/KAGGLE/DSV/4208877."
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html",
    "href": "posts/008-build-a-singularity-container/index.html",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "",
    "text": "Singularity is a free and open-source container platform foroperating-system-level virtualization. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Singularity on your laptop (preferably Linux) and then run it on your local computer or a High-Performance Computer (HPC). A Singularity container is a single file that is easy to ship to an HPC or a friend.\nI heavily rely on Singularity for my work as I write arbitrary code that runs on an HPC. I often require a specific set of libraries, compilers, or other supporting software that are often hard to manage on an HPC. Also, from a reproducibility point of view, it’s easier to build a container with a fixed library and software version packaged in a single file than scattered in multiple modules on the HPC. And when the time comes for the publication, I can easily share the container on Zenodo, etc.\nThe title of this post emphasizes data science, machine learning, and chemistry because all the software we will install is related to these disciplines. However, this procedure applies to building containers for any field of application.\n\n\n\nBuild a Linux based Singularity container.\n\nFirst build a writable sandbox with essential elements.\nInspect the container.\nInstall additional software.\nConvert the sandbox to a read-only SquashFS container image.\n\nInstall software & packages from multiple sources.\n\nUsing apt-get package management system.\nCompiling from source code.\nUsing Python pip.\nUsing install.packages() function in R.\n\nSoftware highlight.\n\nJupyter notebook.\nTensorflow GPU version.\nOpenMPI.\nPopular datascience packages in Python and R.\nChemistry/chemoinformatics software: RDkit, OpenBabel, Pybel, & Mordred.\n\nTest the container.\n\nTest the GPU version of Tensorflow."
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html#inspect-container",
    "href": "posts/008-build-a-singularity-container/index.html#inspect-container",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "Inspect Container",
    "text": "Inspect Container\nTo get a list of the labels defined for the container singularity inspect --labels container/\nTo print the container’s help section singularity inspect --helpfile container/\nTo show container’s environment singularity inspect --environment container/\nTo retrieve the definition file used to build the container singularity inspect --deffile container/"
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html#install-data-science-and-chemistry-packages",
    "href": "posts/008-build-a-singularity-container/index.html#install-data-science-and-chemistry-packages",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "Install Data Science and Chemistry Packages",
    "text": "Install Data Science and Chemistry Packages\nOnce the core writable sandbox is built we will install the additional data science and chemistry packages.\nTo do that execute:\nsudo singularity shell --writable container/\nThen execute the following lines in the shell environment.\n    # Install Python packages.\n        python3 -m pip --no-cache-dir install numpy pandas h5py pyarrow sklearn statsmodels matplotlib seaborn plotly \n\n    # Install Tensorflow.\n        python3 -m pip --no-cache-dir install tensorflow==2.2.0 \n\n    # Install R packages.\n        R --quiet --slave -e 'install.packages(\"tidyverse\", version = \"1.3.0\", repos=\"https://cloud.r-project.org/\")'\n        R --quiet --slave -e 'install.packages(\"tidymodels\", version = \"0.1.0\", repos=\"https://cloud.r-project.org/\")'\n        R --quiet --slave -e 'install.packages(c(\"lme4\", \"glmnet\", \"yaml\", \"jsonlite\", \"rlang\"), repos=\"https://cloud.r-project.org/\")'\n\n    # Install RDKit\n        export RDBASE=/usr/local/share/rdkit\n        export LD_LIBRARY_PATH=\"$RDBASE/lib:$LD_LIBRARY_PATH\"\n        export PYTHONPATH=\"$RDBASE:$PYTHONPATH\"\n        mkdir -p /tmp/rdkit\n        cd /tmp/rdkit\n        wget https://github.com/rdkit/rdkit/archive/2020_03_3.tar.gz\n        tar zxf 2020_03_3.tar.gz\n        mv rdkit-2020_03_3 $RDBASE\n        mkdir $RDBASE/build\n        cd $RDBASE/build\n        cmake -DPYTHON_EXECUTABLE=/usr/bin/python3 ..\n        make -j $(nproc)\n        make install\n\n        ln -s /usr/local/share/rdkit/rdkit /usr/local/lib/python3.6/dist-packages/\n\n    # Install OpenBabel.\n        apt-get -qq -y update\n        apt-get -qq install -y --no-install-recommends openbabel python-openbabel\n\n    # Install Mordred Molecular Descriptor Calculator.\n        python3 -m pip --no-cache-dir install mordred\n\n    # Cleanup\n        rm -rf /tmp/rdkit"
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html#convert-a-writable-sandbox-to-a-read-only-compressed-container",
    "href": "posts/008-build-a-singularity-container/index.html#convert-a-writable-sandbox-to-a-read-only-compressed-container",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "Convert a Writable Sandbox to a Read Only Compressed Container",
    "text": "Convert a Writable Sandbox to a Read Only Compressed Container\nOnce you are satisfied that you have installed all the required packages you can convert the writable sandbox to a read only squashfs filesystem. Squashfs is a compressed read-only file system for Linux.\nsudo singularity build container.sif container/"
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html#install-kernel-spces-for-jupyter-notebook-for-r",
    "href": "posts/008-build-a-singularity-container/index.html#install-kernel-spces-for-jupyter-notebook-for-r",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "Install Kernel Spces for Jupyter Notebook for R",
    "text": "Install Kernel Spces for Jupyter Notebook for R\nKernel specs are installed from outside the container in the host’s home environment.\nsingularity exec container.sif R --quiet --slave -e 'IRkernel::installspec()'\nNOTE: You only have to do it once per host to install kernelspec."
  },
  {
    "objectID": "posts/008-build-a-singularity-container/index.html#tensorflow-gpu",
    "href": "posts/008-build-a-singularity-container/index.html#tensorflow-gpu",
    "title": "How to build a Singularity container for machine learning, data science, and chemistry",
    "section": "Tensorflow GPU",
    "text": "Tensorflow GPU\nimport tensorflow as tf\n\ntf.debugging.set_log_device_placement(True)\ngpus = tf.config.list_physical_devices('GPU')\n\nif gpus:\n    with tf.device('/GPU:0'):\n        tf.random.set_seed(123)\n        a = tf.random.normal([10000,20000], 0, 1, tf.float32, seed=1)\n        b = tf.random.normal([20000,10000], 0, 1, tf.float32, seed=1)\n        c = tf.matmul(a, b)\n        print(c)\nelse:\n    print(\"No GPUs found.\")\n\nprint(\"Num GPUs:\", len(gpus))\nTo execute the script singularity exec --nv container.sif python3 tf_gpu.py\nTo monitor NVIDIA GPU usage nvidia-smi"
  },
  {
    "objectID": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html",
    "href": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html",
    "title": "Non-parametric hypothesis tests with examples in Julia",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-11-30 First draft"
  },
  {
    "objectID": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html#import-packages",
    "href": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html#import-packages",
    "title": "Non-parametric hypothesis tests with examples in Julia",
    "section": "Import packages",
    "text": "Import packages\n\nimport Pkg\nPkg.activate(\".\")\nusing CSV\nusing Plots\nusing HypothesisTests\nusing DataFrames\n\n  Activating project at `~/sandbox/dataalltheway/posts/011-01-non-parametric-hypothesis-tests-julia`"
  },
  {
    "objectID": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html#right-tailed-test",
    "href": "posts/011-01-non-parametric-hypothesis-tests-julia/index.html#right-tailed-test",
    "title": "Non-parametric hypothesis tests with examples in Julia",
    "section": "Right tailed test",
    "text": "Right tailed test\n\npvalue(mwut_results, tail=:right)\n\n1.2040605143479147e-31"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-11-17 First draft"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#import-packages",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#import-packages",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "Import packages",
    "text": "Import packages\n\nimport Pkg\nPkg.activate(\".\")\nusing CSV\nusing DataFrames\nusing Statistics\nusing HypothesisTests\n\n  Activating project at `~/sandbox/dataalltheway/posts/010-01-parametric-hypothesis-tests-julia`"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-unpaired-z-test",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-unpaired-z-test",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "Two sample unpaired z-test",
    "text": "Two sample unpaired z-test\n\nuneqvarztest = let\n    # Fetch a random sample of BMI data for women in the year 1985 and 2017\n    x1 = filter([:Sex, :Year] => (s, y) -> s==\"Women\" && y==1985 , data) |>\n        x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    x2 = filter([:Sex, :Year] => (s, y) -> s==\"Women\" && y==2017 , data) |>\n        x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    UnequalVarianceZTest(x1, x2)\nend\n\nTwo sample z-test (unequal variance)\n------------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          -2.26\n    95% confidence interval: (-2.679, -1.841)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           <1e-25\n\nDetails:\n    number of observations:   [300,300]\n    z-statistic:              -10.560590588866509\n    population standard error: 0.21400318296427412"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-paired-z-test",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-paired-z-test",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "Two sample paired z-test",
    "text": "Two sample paired z-test\n\neqvarztest = let\n    # Fetch a random sample of BMI data for women in the year 1985 and 2017\n    x1 = filter([:Sex, :Year] => (s, y) -> s==\"Women\" && y==1985 , data) |>\n        x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    x2 = filter([:Sex, :Year] => (s, y) -> s==\"Women\" && y==2017 , data) |>\n        x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    EqualVarianceZTest(x1, x2)\nend\n\nTwo sample z-test (equal variance)\n----------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          -2.173\n    95% confidence interval: (-2.611, -1.735)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           <1e-21\n\nDetails:\n    number of observations:   [300,300]\n    z-statistic:              -9.724414586039652\n    population standard error: 0.22345818154642977"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#one-sample-t-test",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#one-sample-t-test",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "One sample t-test",
    "text": "One sample t-test\n\nonesamplettest = let \n    x1 = filter(\n        [:Sex, :Region, :Year] => \n            (s, r, y) -> s==\"Men\" && r==\"Rural\" && y == 2017,\n        data\n    ) |>\n    x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    OneSampleTTest(x1, 24.5)\nend\n\nOne sample t-test\n-----------------\nPopulation details:\n    parameter of interest:   Mean\n    value under h_0:         24.5\n    point estimate:          25.466\n    95% confidence interval: (25.16, 25.77)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           <1e-08\n\nDetails:\n    number of observations:   300\n    t-statistic:              6.280721563263261\n    degrees of freedom:       299\n    empirical standard error: 0.15380398418714467"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-unpaired-independent-t-test",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-unpaired-independent-t-test",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "Two sample unpaired (independent) t-test",
    "text": "Two sample unpaired (independent) t-test\n\nunpairedtwosamplettest = let \n    x1 = filter([:Sex, :Region, :Year] => \n            (s, r, y) -> s==\"Women\" && r==\"Rural\" && y == 1985,\n        data) |>\n        x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    x2 = filter([:Sex, :Region, :Year] => \n            (s, r, y) -> s==\"Women\" && r==\"Urban\" && y == 1985,\n        data) |>\n        x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    UnequalVarianceTTest(x1, x2)\nend\n\nTwo sample t-test (unequal variance)\n------------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          -1.05867\n    95% confidence interval: (-1.512, -0.6054)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           <1e-05\n\nDetails:\n    number of observations:   [300,300]\n    t-statistic:              -4.587387795167387\n    degrees of freedom:       575.968012373301\n    empirical standard error: 0.2307776699807073\n\n\n\n\n\n\n\n\nWelch’s Test\n\n\n\nThis test uses the Welch correction, and there is no way to turn it off in HypothesisTests.jl.\n\n\n\nOnly considering right tailed (one-tailed)\n\nunpairedtwosamplettest = let \n    x1 = filter([:Sex, :Region, :Year] => \n            (s, r, y) -> s==\"Women\" && r==\"Rural\" && y == 1985,\n        data) |>\n        x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    x2 = filter([:Sex, :Region, :Year] => \n            (s, r, y) -> s==\"Women\" && r==\"Urban\" && y == 1985,\n        data) |>\n        x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    UnequalVarianceTTest(x1, x2)\nend\npvalue(unpairedtwosamplettest, tail=:right)\n\n0.9999999445762"
  },
  {
    "objectID": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-paired-dependent-t-test",
    "href": "posts/010-01-parametric-hypothesis-tests-julia/index.html#two-sample-paired-dependent-t-test",
    "title": "Parametric hypothesis tests with examples in Julia",
    "section": "Two sample paired (dependent) t-test",
    "text": "Two sample paired (dependent) t-test\n\npairedtwosamplettest = let \n    x1 = filter([:Sex, :Region, :Year] => \n            (s, r, y) -> s==\"Women\" && r==\"Rural\" && y == 1985,\n        data) |>\n        x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    x2 = filter([:Sex, :Region, :Year] => \n            (s, r, y) -> s==\"Women\" && r==\"Urban\" && y == 1985,\n        data) |>\n        x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    x -> x[!, :BMI] |> skipmissing |> collect |> x->rand(x, 300)\n    EqualVarianceTTest(x1, x2)\nend\n\nTwo sample t-test (equal variance)\n----------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          -1.01167\n    95% confidence interval: (-1.44, -0.5838)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           <1e-05\n\nDetails:\n    number of observations:   [300,300]\n    t-statistic:              -4.64337449574737\n    degrees of freedom:       598\n    empirical standard error: 0.2178731583233696"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html",
    "title": "Type inference in readr and arrow",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-11-23 This article is cross-posted from https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/ with permission."
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#bit-integers",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#bit-integers",
    "title": "Type inference in readr and arrow",
    "section": "32-bit integers",
    "text": "32-bit integers\nAnother difference between readr and arrow is the difference between how integers larger than 32 bits are read in. Natively, R can only support 32-bit integers, though it can support 64-bit integers via the bit64 package. If we create a CSV with one column containing the largest integer that R can natively support, and then another column containing that value plus 1, we get different behaviour when we import this data with readr and arrow. In readr, when we enable integer guessing, the smaller value is read in as an integer, and the larger value is read in as a double. However, once we move over to manually specifying column types, we can use vroom::col_big_integer() to use bit64 and get us a large integer column. The arrow package also uses bit64, and its integer guessing results in 64-bit integer via inference.\n\nsixty_four <- data.frame(x = 2^31 - 1, y = 2^31)\n\nreadr::write_csv(sixty_four, \"sixty_four.csv\")\n\n# doubles by default\nreadr::read_csv(\"sixty_four.csv\")\n\nRows: 1 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 2\n           x          y\n       <dbl>      <dbl>\n1 2147483647 2147483648\n\n\n\n# 32 bit integer or double depending on value size\nreadr::read_csv(\"sixty_four.csv\", col_types = list(.default = col_character())) %>%\n  type_convert(guess_integer = TRUE)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  x = col_integer(),\n  y = col_double()\n)\n\n\n# A tibble: 1 × 2\n           x          y\n       <int>      <dbl>\n1 2147483647 2147483648\n\n\n\n# integers by specification\nreadr::read_csv(\n  \"sixty_four.csv\",\n  col_types = list(x = col_integer(), y = vroom::col_big_integer())\n)\n\n# A tibble: 1 × 2\n           x          y\n       <int>    <int64>\n1 2147483647 2147483648\n\n\n\n# integers by inference\narrow::read_csv_arrow(\"sixty_four.csv\")\n\n# A tibble: 1 × 2\n           x          y\n       <int>    <int64>\n1 2147483647 2147483648"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#the-number-parsing-strategy",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#the-number-parsing-strategy",
    "title": "Type inference in readr and arrow",
    "section": "The “number” parsing strategy",
    "text": "The “number” parsing strategy\nOne really cool feature in readr is the “number” parsing strategy. This allows values which have been stored as character data with commas to separate the thousands to be read in as doubles. This is not supported in arrow.\n\nnumber_type <- data.frame(\n  x = c(\"1,000\", \"1,250\")\n)\n\nreadr::write_csv(number_type, \"number_type.csv\")\n\n# double type, but parsed in as number in column spec shown below\nreadr::read_csv(\"number_type.csv\")\n\nRows: 2 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nnum (1): x\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 1\n      x\n  <dbl>\n1  1000\n2  1250\n\n\n\n# read in as character data in Arrow\narrow::read_csv_arrow(\"number_type.csv\")\n\n# A tibble: 2 × 1\n  x    \n  <chr>\n1 1,000\n2 1,250"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#dictionariesfactors",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#dictionariesfactors",
    "title": "Type inference in readr and arrow",
    "section": "Dictionaries/Factors",
    "text": "Dictionaries/Factors\nAnyone who’s been around long enough might remember that R’s native CSV reading function read.csv() had a default setting of importing character columns as factors (I definitely have read.csv(..., stringAsFactors=FALSE) carved into a groove in some dark corner of my memory). This default was changed in version 4.0.0, released in April 2020, reflecting the fact that in most cases users want their string data to be imported as characters unless otherwise specified. Still, some datasets contain character data which users do want to import as factors. In readr, this can be controlled by manually specifying the column as a factor\nIn arrow, if you don’t want to individually specify column types, you can set up an option to import character columns as dictionaries (the Arrow equivalent of factors), which are converted into factors.\n\ndict_type <- data.frame(\n  x = c(\"yes\", \"no\", \"yes\", \"no\")\n)\n\nreadr::write_csv(dict_type, \"dict_type.csv\")\n\n# character data\nreadr::read_csv(\"dict_type.csv\")\n\nRows: 4 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): x\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 4 × 1\n  x    \n  <chr>\n1 yes  \n2 no   \n3 yes  \n4 no   \n\n\n\n# factor data\nreadr::read_csv(\"dict_type.csv\", col_types = list(x = col_factor()))\n\n# A tibble: 4 × 1\n  x    \n  <fct>\n1 yes  \n2 no   \n3 yes  \n4 no   \n\n\n\n# set up the option. there's an open ticket to make this code a bit nicer to read.\nauto_dict_option <- arrow::CsvConvertOptions$create(auto_dict_encode = TRUE)\narrow::read_csv_arrow(\"dict_type.csv\", convert_options = auto_dict_option)\n\n# A tibble: 4 × 1\n  x    \n  <fct>\n1 yes  \n2 no   \n3 yes  \n4 no"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#custom-logicalboolean-values",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#custom-logicalboolean-values",
    "title": "Type inference in readr and arrow",
    "section": "Custom logical/boolean values",
    "text": "Custom logical/boolean values\nAnother slightly niche but potentially useful piece of functionality available in arrow is the ability to customise which values can be parsed as logical/boolean type and how they translate to TRUE/FALSE. This can be achieved by setting some custom conversion options.\n\nalternative_true_false <- arrow::CsvConvertOptions$create(\n  false_values = \"no\", true_values = \"yes\"\n)\narrow::read_csv_arrow(\"dict_type.csv\", convert_options = alternative_true_false)\n\n# A tibble: 4 × 1\n  x    \n  <lgl>\n1 TRUE \n2 FALSE\n3 TRUE \n4 FALSE"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#using-schemas-for-manual-control-of-data-types",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#using-schemas-for-manual-control-of-data-types",
    "title": "Type inference in readr and arrow",
    "section": "Using schemas for manual control of data types",
    "text": "Using schemas for manual control of data types\nAlthough relying on the reader itself to guess your column types can work well, what if you want more precise control?\nIn readr, you can use the col_types parameter to specify column types. You can use the same parameter in arrow to use R type specifications.\n\ngiven_types <- data.frame(x = c(1, 2, 3), y = c(4, 5, 6))\n\nreadr::write_csv(given_types, \"given_types.csv\")\n\nreadr::read_csv(\"given_types.csv\", col_types = list(col_integer(), col_double()))\n\n# A tibble: 3 × 2\n      x     y\n  <int> <dbl>\n1     1     4\n2     2     5\n3     3     6\n\n\nYou can also use this shortcode specification. Here, “i” means integer and “d” means double.\n\nreadr::read_csv(\"given_types.csv\", col_types = \"id\")\n\n# A tibble: 3 × 2\n      x     y\n  <int> <dbl>\n1     1     4\n2     2     5\n3     3     6\n\n\nIn arrow you can use the shortcodes (though not the col_*() functions), but you must specify the column names.\nWe skip the first row as our data has a header row - this is the same behaviour as when we use both names and types in readr::read_csv() which then assumes that the header row is data if we don’t skip it.\n\narrow::read_csv_arrow(\"given_types.csv\", col_names = c(\"x\", \"y\"), col_types = \"id\", skip = 1)\n\n# A tibble: 3 × 2\n      x     y\n  <int> <dbl>\n1     1     4\n2     2     5\n3     3     6\n\n\nWhat if you want to use Arrow types instead of R types though? In this case, you need to use a schema. I won’t go into detail here, but in short, schemas are lists of fields, each of which contain a field name and a data type. You can specify a schema like this:\n\n# this gives the same result as before - because our Arrow data has been converted to the relevant R type\narrow::read_csv_arrow(\"given_types.csv\", schema = schema(x = int8(), y = float32()), skip = 1)\n\n# A tibble: 3 × 2\n      x     y\n  <int> <dbl>\n1     1     4\n2     2     5\n3     3     6\n\n\n\n# BUT, if you don't read it in as a data frame you'll see the Arrow type\narrow::read_csv_arrow(\"given_types.csv\", schema = schema(x = int8(), y = float32()), skip = 1, as_data_frame = FALSE)\n\nTable\n3 rows x 2 columns\n$x <int8>\n$y <float>"
  },
  {
    "objectID": "posts/012-type-inference-in-readr-and-arrow/index.html#further-reading",
    "href": "posts/012-type-inference-in-readr-and-arrow/index.html#further-reading",
    "title": "Type inference in readr and arrow",
    "section": "Further Reading",
    "text": "Further Reading\nIf you want a much more detailed discussion of Arrow data types, see this excellent blog post by Danielle Navarro."
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-11-21 Code on Kaggle\n2022-11-18 First draft"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#dataset",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#dataset",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Dataset",
    "text": "Dataset\nFor our example exercises, we will work with the “Global CO2 emissions from cement production” dataset (Andrew 2022). I have subsetted the data from 1928 onward and dropped any columns with all NAs or zeros. The table below shows all the data we will use in this tutorial.\nFigure 1 shows per country yearly (x-axis) emissions logged to base 10 (y-axis). The log is taken for visualization purposes. All the statistical calculations will be done on the original values.\n\n\n\n\n\n\nNote\n\n\n\nThe emissions from the use of fossil fuels in cement production are not included in this dataset since they are usually included elsewhere in global datasets of fossil CO2 emissions. The process emissions in this dataset, which result from the decomposition of carbonates in the production of cement clinker, amounted to ~1.7 Gt CO2 in 2021, while emissions from the combustion of fossil fuels to produce the heat required amounted to an additional ~1.0 Gt CO2 in 2021.\n\n\n\nsuppressMessages(library(DT))\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(kableExtra))\n\n\n# Download the data from Zenodo\ndat <- readr::read_csv(\"https://zenodo.org/record/7081360/files/1.%20Cement_emissions_data.csv\", show_col_types = FALSE)\n\n# Filter the data and present it in a DT::datatable\ndat <- dat %>% dplyr::filter(Year >= 1928) %>%\n  select_if(function(x) all(!is.na(x))) %>%\n  select_if(function(x) all(!x == 0))\nDT::datatable(dat)\n\n\n\n\n\n\n\ndat_gather <- dat %>% gather(key = \"Country\", value = \"Emission\", -Year)\nggplot(dat_gather, aes(x = Year, y = as.numeric(log10(Emission)), color = Country)) +\n  geom_line(aes(group = Country)) + \n  labs(x = \"Year\", y = \"log10(Emission)\", color = \"\") +theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nFigure 1: Global CO2 emissions from cement production"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "The method",
    "text": "The method\n\nFirst, we rank all of the values (from both groups) from the smallest to largest. Equal values are allocated the average of the ranks they would have if there was tiny differences between them.\nNext we sum the ranks for each group. You call the sum of the ranks for the larger group \\(R_1\\) and for the smaller sized group, \\(R_2\\). If both groups are equally sized then we can label them whichever way round we like.\nWe then input \\(R_1\\) and \\(R_2\\) and also \\(N_1\\) and \\(N_2\\), the respective sizes of each group, into the Equation 1.\n\n\\[\n\\begin{equation} U = (N_1 \\times N_2) + \\dfrac{N_1 \\times (N_1+1)}{2} - R_1 \\end{equation}\n\\tag{1}\\]\n\nThen we compare the value of \\(U\\) to significance tables. You find the intersection of the column with the value of \\(N_1\\) and the row with the value of \\(N_2\\). In this intersection there will be two ranges of values of \\(U\\) which are significant at the \\(5\\%\\) level. If our value is within one of these ranges, then we have a significant result and we reject the null hypothesis. If our value is not in the range then it is not significant and then the independent variable is unrelated to the dependent variable, we accept the \\(H_0\\).\nAs a check, we also need to examine the means of the two groups, to see which has the higher scores on the dependent variable."
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-two-tailed-test-in-r",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-two-tailed-test-in-r",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Example code for a two-tailed test in R",
    "text": "Example code for a two-tailed test in R\nIn this example we will do a two-tailed test to measure if there is a difference in emission between the USA and Canada. Our null hypothesis \\(H_0\\) is that there is no difference.\n\n(w_res <- wilcox.test(dat$USA, dat$Canada, conf.int = TRUE))\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  dat$USA and dat$Canada\nW = 8763, p-value < 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 26384 29797\nsample estimates:\ndifference in location \n              28155.27 \n\n\nWe can fetch results from w_res object like w_res$p.value. However, it’s easier to fetch all the values and convert them into a data frame using the boom::tidy() function from the tidyverse suite. As we see in Table 1 the p-value is \\(0\\), which means we can reject our null hypothesis and accept our alternative hypothesis that there is a significant difference in CO2 emissions between the USA and Canada.\n\nbroom::tidy(w_res) %>%\n  kbl() %>%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 1:  Two-tailed Wilcoxon rank sum test between Co2 emissions from the USA and Canada \n \n  \n    estimate \n    statistic \n    p.value \n    conf.low \n    conf.high \n    method \n    alternative \n  \n \n\n  \n    28155.27 \n    8763 \n    0 \n    26384 \n    29797 \n    Wilcoxon rank sum test with continuity correction \n    two.sided"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-one-tailed-test-in-r",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-one-tailed-test-in-r",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Example code for a one-tailed test in R",
    "text": "Example code for a one-tailed test in R\nIn this example we will do a one-tailed test to measure if emissions from the USA is greater than Canada. Our null hypothesis \\(H_0\\) is that the emissions from the USA is not greater than Canada.\n\n(w_res <- wilcox.test(dat$USA, dat$Canada, conf.int = TRUE, alternative = \"greater\"))\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  dat$USA and dat$Canada\nW = 8763, p-value < 2.2e-16\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n 26696   Inf\nsample estimates:\ndifference in location \n              28155.27 \n\n\nAs we see in Table 2 the p-value is \\(0\\), which means we can reject our null hypothesis and accept our alternative hypothesis that the CO2 emissions are in the USA than Canada.\n\nbroom::tidy(w_res) %>%\n  kbl() %>%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 2:  One-tailed Wilcoxon rank sum test between Co2 emissions from the USA and Canada \n \n  \n    estimate \n    statistic \n    p.value \n    conf.low \n    conf.high \n    method \n    alternative \n  \n \n\n  \n    28155.27 \n    8763 \n    0 \n    26696 \n    Inf \n    Wilcoxon rank sum test with continuity correction \n    greater"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method-1",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method-1",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "The method",
    "text": "The method\n\nCalculate the difference values between your two samples of data. We then remove difference values of zero.\nRank them. If values are tied then you use the same method as in the Mann-Whitney tests. You assign the difference scores the average rank if it was possible to separate the tied difference scores.\nThe ranks of the differences can now have the sign of the difference reattached.\nThe sum of the positive ranks are calculated.\nThe sum of the negative ranks are calculated.\nYou then choose the smaller sum of ranks and we call this our \\(T\\) value, which we compare with significance tables. You choose the row which has the number of pairs of values in your sample.\nReport your findings and make your conclusion."
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-paired-two-tailed-test-in-r",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-for-a-paired-two-tailed-test-in-r",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Example code for a paired two-tailed test in R",
    "text": "Example code for a paired two-tailed test in R\nSince this a paired test we will test if there is difference in emission between two time periods say 2000 and 2020 across all the countries in our dataset. Our null hypothesis \\(H_0\\) is that there is no difference.\n\ndat_m <- dat %>% dplyr::select(-Year) %>% as.matrix()\nrownames(dat_m) <- dat$Year\ndat_t <- t(dat_m)\nx <- as.numeric(dat_t[,\"2000\"])\ny <- as.numeric(dat_t[,\"2020\"])\n\n(w_res <- wilcox.test(x, y, conf.int = TRUE, paired = TRUE))\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x and y\nV = 119, p-value = 0.5803\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -3848.5   621.5\nsample estimates:\n(pseudo)median \n          -297 \n\n\nAs we can see in the Table 3, the p.value is 0.5, which is above our alpha level of 0.05; therefore, we can accept our null hypothesis that there is indeed no significant difference in CO2 emissions between 2000 and 2020.\n\nbroom::tidy(w_res) %>%\n  kbl() %>%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 3:  Two-tailed Wilcoxon signed rank test between Co2 emissions from 2000 and 2020 \n \n  \n    estimate \n    statistic \n    p.value \n    conf.low \n    conf.high \n    method \n    alternative \n  \n \n\n  \n    -297 \n    119 \n    0.580338 \n    -3848.5 \n    621.5 \n    Wilcoxon signed rank exact test \n    two.sided"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method-2",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#the-method-2",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "The method",
    "text": "The method\n\nRank all data from all groups together; i.e., rank the data from 1 to \\(N\\) ignoring group membership. Assign any tied values the average of the ranks they would have received had they not been tied.\nThe test statistic is given by Equation 2.\n\n\\[\nH = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i}-3(N+1)\n\\tag{2}\\]\nWhere \\(N\\) is the total sample size, \\(k\\) is the number of groups we are comparing, \\(R_i\\) is the sum of ranks for group \\(i\\), and \\(n_i\\) is the sample size of group \\(i\\).\n\nThe decision to reject or not the null hypothesis is made by comparing \\(H\\) to a critical value \\(H_c\\) obtained from a table or a software for a given significance or alpha level. If \\(H\\) is bigger than \\(H_c\\), the null hypothesis is rejected.\nIf the statistic is not significant, then there is no evidence of stochastic dominance between the samples. However, if the test is significant then at least one sample stochastically dominates another sample."
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-in-r",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#example-code-in-r",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Example code in R",
    "text": "Example code in R\nWe will use the same long form of data that we used in the Figure 1.\n\n(k_res <- kruskal.test(dat_gather$Emission, as.factor(dat_gather$Country)))\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  dat_gather$Emission and as.factor(dat_gather$Country)\nKruskal-Wallis chi-squared = 1253.6, df = 22, p-value < 2.2e-16\n\n\n\nbroom::tidy(k_res) %>%\n  kbl() %>%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 4:  Kruskal-Wallis rank sum test between Co2 emissions from 23 countries from 1928 to 2021 \n \n  \n    statistic \n    p.value \n    parameter \n    method \n  \n \n\n  \n    1253.576 \n    0 \n    22 \n    Kruskal-Wallis rank sum test"
  },
  {
    "objectID": "posts/011-non-parametric-hypothesis-tests-r/index.html#websites",
    "href": "posts/011-non-parametric-hypothesis-tests-r/index.html#websites",
    "title": "Non-parametric hypothesis tests with examples in R",
    "section": "Website(s)",
    "text": "Website(s)\n\nNon-parametric Hypothesis Tests (Psychology)"
  },
  {
    "objectID": "posts/001-data-transformation/index.html",
    "href": "posts/001-data-transformation/index.html",
    "title": "Data Transformation",
    "section": "",
    "text": "Data transformation is a process of performing a mathematical function on each data point used in a statistical or machine learning analysis to either satisfy the underlying assumptions of a statistical test (e.g., normal distribution for a t-test), help a machine-learning algorithm to converge faster and or make a visualization interpretable. In addition to statistical analyses and modeling, data transformation can also be helpful in data visualization, for example, performing a log transformation on a skewed data set to plot it in a relatively unskewed and visually appealing scatter plot. Most of the data transformation methods are invertible and original values of a data set can be recovered by implementing a counter mathematical function. In mathematical form it can be expressed as:\n\\[x' = f(x)\\]\nWhere \\(x\\) is the original data, \\(x'\\) is the transformed data, and \\(f(x)\\) is a mathematical function performed on \\(x\\).\nIn data science, data transformation is also sometimes combined with the data cleaning step. In addition to performing a mathematical function to the data points, they are also checked for quality, for example, checking for missing values. I will discuss data cleaning procedures elsewhere. Data transformation can be considered as an umbrella term for both data scaling and data normalization. They are frequently used interchangeably, sometimes referring to the same mathematical operation. Although data scaling and normalization are used to achieve a similar result, it is better to understand them as two different operations that are happening under the hood.\nAlthough every data transformation method performs a mathematical operation on every data point (e.i. element wise), for some, this operation is not influenced if data points are either removed or added to the data set. Let’s consider a data set in the form of a two-dimensional data table with samples on the row and features on the column. Now take two methods to compare 1) log transformation 2) min-max scaling. In log transformation \\(log(x)\\), a log is taken for every data point individually, and the result will not change if some rows or columns are dropped or added in our example data table. However, in min-max scaling\n\\[x' = x-min(x)/max(x)-min(x)\\]\nthat is performed feature-wise (columns); if the data point that was selected as a min or max in a previous transformation is removed, then re-doing the transformation will change the result. The removal of a data point may happen; for example, if the min or max value selected in the first iteration was an outlier or that a particular sample had multiple missing values, and therefore, it had to be removed, amongst others. Min-max scaling will also influence if more data points are added to our data set. It may bring a new min or max data point and hence will change the scaling. Therefore while selecting a data transformation method, it must be noted if data points are dropped in the subsequent analysis, then should you perform the transformation again as a result of data point loss or it will be indifferent."
  },
  {
    "objectID": "posts/001-data-transformation/index.html#log-transformation",
    "href": "posts/001-data-transformation/index.html#log-transformation",
    "title": "Data Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\nIn a log transformation, logarithm is calculated for every value in the data set. Traditionally, log transformation is carried out to reduce the skewness of data or to bring data closer to a normal distribution. Usually the base to the log doesn’t matter unless it is a domain specific requirement. However, every feature of the data set should be transformed with the same base. Most of the programming languages have a core function to calculate the log of a number. In programming languages that support vector operation, for example, R, the same log function can be performed on both a single value or on all the values within a data frame, vector or matrix.\nFor example, let’s visualize the effect of log transformation on a synthetically generated dummy data. To generated figures Figure 1 and Figure 2, I have randomly sampled 10,000 positive real numbers from a skewed (positive and negative) normal distribution and performed a log transformation on every data point. The left sub-panel shows a histogram of the non-transformed data, and the right sub-panel shows a histogram of the log-transformed data. Although log transformation is known for reducing the skewness of the data and making the distribution more symmetric around the mean, it holds only for the positively skewed data. If the data are negatively skewed a log transformation will skew it further. In case of a negatively skewed data doing a power transformation may help to reduce the skewness (figure Figure 3). Usually raising the data to a power of 2 has slight effect on the skewness; a higher number may be required. In addition to the visual inspection, we can also numerically quantify the skewness of the data; that is mentioned in the figure caption.\nLog Transformation: \\[x' = log(x)\\]\nPower Transformation: \\[x' = x^n\\]\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import skewnorm\nfrom scipy.stats import skew \nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Generate random data points from a skewed normal distribution\ndata_pos = np.round(skewnorm.rvs(10, size=10000, loc=1, random_state = 101), decimals = 2)\n#print('Skewness for the positively (right) skewed data before transformation : ', round(skew(data_pos), 2)) \ndata_neg = np.round(skewnorm.rvs(-10, size=10000, loc=10, random_state = 101), decimals = 2)\n#print('Skewness for the negatively (left) skewed data before transformation : ', round(skew(data_neg), 2)) \n\n# Log transform the data\nlog_data_pos = np.log(data_pos)\n#print('Skewness for the positively skewed data after transformation : ', round(skew(log_data_pos), 2)) \nlog_data_neg = np.log(data_neg)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(log_data_neg), 2)) \n\n\n\n\nCode\nfig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n\n# We can set the number of bins with the *bins* keyword argument.\naxs[0].hist(data_pos, bins=20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(log_data_pos, bins=20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Log-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 1: Histogram of the positively skewed data and its log transformation. The skewness for the non-transformed data (left) is 0.9 and for the log-transformed data (right) is 0.2.\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\naxs[0].hist(data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(log_data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Log-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 2: Histogram of the negatively skewed data and its log transformation. The skewness for the non-transformed data (left) is -0.9 and for the log-transformed data (right) is -1.2.\n\n\n\n\n\n\nCode\n# Square data.\npow_data_neg = np.power(data_neg, 6)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(pow_data_neg), 2)) \nfig, axs = plt.subplots(ncols=2, sharey = \"all\", tight_layout=True)\naxs[0].hist(data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(pow_data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Power-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 3: Histogram of the negatively skewed data and its power transformation. Data is raised to the power ot 6. The skewness for the non-transformed data (left) is -0.9 and for the power-transformed data (right) is -0.3.\n\n\n\n\nNote: Since the data used in these figures are sampled from a skewed normal distribution the skewness calculated here are below 2. For a non-normally distributed skewed data it would be higher than 2. Log transformation is often used to bring a non-normal distribution closer to a normal distribution.\nLog transformation can only be performed on positive values. Mathematics principles doesn’t allow log calculation on negative values. In case our input data contains negative values and a log like transformation is desired inverse hyperbolic sin (arcsinh) transformation method can be used."
  },
  {
    "objectID": "posts/001-data-transformation/index.html#arcsinh-transformation",
    "href": "posts/001-data-transformation/index.html#arcsinh-transformation",
    "title": "Data Transformation",
    "section": "Arcsinh Transformation",
    "text": "Arcsinh Transformation\nInverse hyperbolic sin transformation is a non-linear transformation that is often used in situations where a log transformations can’t be used; such as in the presence of negative values. Flow and mass cytometry are popular examples where arcsinh transformation is a almost always a method of choice. Reason being older flow cytometry machines produced positive values that were displayed on a log scale. However, newer machines can produce both negative and positive values that can’t be displayed on a log scale. Therefore, to keep the data resemble a log transformation arcsinh transformation is used.\nArcsinh transformation can also be tweaked by using a cofactor to behave differently around zero. For both negative and positive values starting from zero to cofactor are presented in a linear fashion along the lines of raw data values and values beyond he cofactor are presented in a log like fashion. In flow and mass cytometry a cofactor of 150 and 5 are used respectively.\nFor all real x: \\[arcsinh(x) = log(x + \\sqrt{x^2 + 1})\\]\nLet’s use similar positively skewed data as in the log transformation to visualize how an arcsinh transformation affects the shape of the distribution. The only change that I would want to do in this data set is to add few negative values. As I mentioned earlier that our mathematical laws doesn’t allow us to take log on negative numbers arcsinh transformation is capable of transforming small negative values closer to zero. Figures Figure 4 and Figure 5 show the histograms comparing the original and the arcsinh transformed data for positive and negatively skewed data respectively. From the figures it’s evident that unlike log, arcsinh transformation works on both positively and negatively skewed data equally well.\n\n\nCode\n# Generate random data points from a skewed normal distribution\ndata_pos = np.round(skewnorm.rvs(10, size=10000, loc=0, random_state = 101), decimals = 2)\n#print('Skewness for the positively (right) skewed data before transformation : ', round(skew(data_pos), 2))\ndata_neg = np.round(skewnorm.rvs(-10, size=10000, loc=0, random_state = 101), decimals = 2)\n#print('Skewness for the negatively (left) skewed data before transformation : ', round(skew(data_neg), 2)) \n\n# Arcsinh transform the data\narcsinh_data_pos = np.arcsinh(data_pos)\n#print('Skewness for the positively skewed data after transformation : ', round(skew(arcsinh_data_pos), 2)) \narcsinh_data_neg = np.arcsinh(data_neg)\n#print('Skewness for the negatively skewed data after transformation : ', round(skew(arcsinh_data_neg), 2)) \n\n\n\n\nCode\nfig, axs = plt.subplots(ncols=2, sharey = \"all\", tight_layout=True)\naxs[0].hist(data_pos, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(arcsinh_data_pos, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Arcsinh-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 4: Histogram of the positively skewed data and its arcsinh transformation. The skewness for the non-transformed data (left) is 0.9 and for the arcsinh-transformed data (right) is 0.3.\n\n\n\n\n\n\nCode\nfig, axs = plt.subplots(ncols=2, sharey = \"all\", tight_layout=True)\naxs[0].hist(data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[0].set_title(\"Non-Transformed Data\")\naxs[0].set_xlabel(\"Feature\")\naxs[0].set_ylabel(\"Frequency\")\naxs[1].hist(arcsinh_data_neg, bins = 20, edgecolor='black', linewidth=1.0)\naxs[1].set_title(\"Arcsinh-Transformed Data\")\naxs[1].set_xlabel(\"Feature\")\nplt.show()\n\n\n\n\n\nFigure 5: Histogram of the negatively skewed data and its arcsinh transformation. The skewness for the non-transformed data (left) is -0.9 and for the arcsinh-transformed data (right) is -0.3."
  },
  {
    "objectID": "posts/001-data-transformation/index.html#min-max-scaling",
    "href": "posts/001-data-transformation/index.html#min-max-scaling",
    "title": "Data Transformation",
    "section": "Min-Max Scaling",
    "text": "Min-Max Scaling\nIn min-max scaling for a given feature, we subtract the minimum value from each value and divide the residual by the difference between the maximum and the minimum value. The resulting transformed data is scaled between 0 and 1.\n\\[minmax(x) = x - min(x) / max(x) - min(x)\\]\nMin-max scaling can also be modified to scale the values to the desired range, for example, between -1 and 1.\n\\[minmax(x) = ((b - a) * (x - min(x)) / max(x) - min(x)) +  a\\]\nWhere \\(a\\) and \\(b\\) are the minimum and maximum range respectively.\n\nApplication(s)\n\nNeural networks"
  },
  {
    "objectID": "posts/001-data-transformation/index.html#standardization",
    "href": "posts/001-data-transformation/index.html#standardization",
    "title": "Data Transformation",
    "section": "Standardization",
    "text": "Standardization\nStandardization is also known as z-scaling, mean removal, or variance scaling. In standardization, the goal is to scale the data with a mean of zero and a standard deviation of one.\n\\[z = (x - \\mu)/\\sigma\\]\nWhere \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of a given feature. Then, the distribution of the transformed data is called the z-distribution.\n\nApplication(s)\n\nPrincipal Component Analysis (PCA)\nIn heatmaps to compare data among samples"
  },
  {
    "objectID": "posts/001-data-transformation/index.html#quantile-normalization",
    "href": "posts/001-data-transformation/index.html#quantile-normalization",
    "title": "Data Transformation",
    "section": "Quantile Normalization",
    "text": "Quantile Normalization\nQuantile normalization (QN) is a technique to make two distribution identical in statistical properties. QN involves first ranking the feature of each sample by magnitude, calculating the average value for genes occupying the same rank, and then substituting the values of all genes occupying that particular rank with this average value. The next step is to reorder the features of each sample in their original order."
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html",
    "href": "posts/007-open-data-for-datascience/index.html",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "",
    "text": "Updates\n\n\n\n\n\n2022-10-26 Added sections for Kaggle and other platforms\n2022-10-25 Initial uncomplete post and Kaggle notebook"
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html#default-datasets-in-r",
    "href": "posts/007-open-data-for-datascience/index.html#default-datasets-in-r",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "Default datasets in R",
    "text": "Default datasets in R\nIn R (v4.1.3), there are 104 datasets for various statistical and machine-learning tasks. The commands in the cell below list all the datasets available by default (Table 1) and across all the installed packages, respectively. This article summarizes some of R’s popular datasets, namely mtcars, iris, etc.\n# Default datasets\ndata()\n\n# Datasets across all the installed packages\ndata(package = .packages(all.available = TRUE))\n\n\nCode\ndat <- data()\ndat <- as_tibble(dat$results) %>% dplyr::select(-LibPath) %>%\n  dplyr::filter(Package == \"datasets\")\nknitr::kable(dat) %>%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %>%\n    scroll_box(width = \"100%\", height = \"300px\")\n\n\n\n\nTable 1:  Default datasets in R \n \n  \n    Package \n    Item \n    Title \n  \n \n\n  \n    datasets \n    AirPassengers \n    Monthly Airline Passenger Numbers 1949-1960 \n  \n  \n    datasets \n    BJsales \n    Sales Data with Leading Indicator \n  \n  \n    datasets \n    BJsales.lead (BJsales) \n    Sales Data with Leading Indicator \n  \n  \n    datasets \n    BOD \n    Biochemical Oxygen Demand \n  \n  \n    datasets \n    CO2 \n    Carbon Dioxide Uptake in Grass Plants \n  \n  \n    datasets \n    ChickWeight \n    Weight versus age of chicks on different diets \n  \n  \n    datasets \n    DNase \n    Elisa assay of DNase \n  \n  \n    datasets \n    EuStockMarkets \n    Daily Closing Prices of Major European Stock Indices, 1991-1998 \n  \n  \n    datasets \n    Formaldehyde \n    Determination of Formaldehyde \n  \n  \n    datasets \n    HairEyeColor \n    Hair and Eye Color of Statistics Students \n  \n  \n    datasets \n    Harman23.cor \n    Harman Example 2.3 \n  \n  \n    datasets \n    Harman74.cor \n    Harman Example 7.4 \n  \n  \n    datasets \n    Indometh \n    Pharmacokinetics of Indomethacin \n  \n  \n    datasets \n    InsectSprays \n    Effectiveness of Insect Sprays \n  \n  \n    datasets \n    JohnsonJohnson \n    Quarterly Earnings per Johnson & Johnson Share \n  \n  \n    datasets \n    LakeHuron \n    Level of Lake Huron 1875-1972 \n  \n  \n    datasets \n    LifeCycleSavings \n    Intercountry Life-Cycle Savings Data \n  \n  \n    datasets \n    Loblolly \n    Growth of Loblolly pine trees \n  \n  \n    datasets \n    Nile \n    Flow of the River Nile \n  \n  \n    datasets \n    Orange \n    Growth of Orange Trees \n  \n  \n    datasets \n    OrchardSprays \n    Potency of Orchard Sprays \n  \n  \n    datasets \n    PlantGrowth \n    Results from an Experiment on Plant Growth \n  \n  \n    datasets \n    Puromycin \n    Reaction Velocity of an Enzymatic Reaction \n  \n  \n    datasets \n    Seatbelts \n    Road Casualties in Great Britain 1969-84 \n  \n  \n    datasets \n    Theoph \n    Pharmacokinetics of Theophylline \n  \n  \n    datasets \n    Titanic \n    Survival of passengers on the Titanic \n  \n  \n    datasets \n    ToothGrowth \n    The Effect of Vitamin C on Tooth Growth in Guinea Pigs \n  \n  \n    datasets \n    UCBAdmissions \n    Student Admissions at UC Berkeley \n  \n  \n    datasets \n    UKDriverDeaths \n    Road Casualties in Great Britain 1969-84 \n  \n  \n    datasets \n    UKgas \n    UK Quarterly Gas Consumption \n  \n  \n    datasets \n    USAccDeaths \n    Accidental Deaths in the US 1973-1978 \n  \n  \n    datasets \n    USArrests \n    Violent Crime Rates by US State \n  \n  \n    datasets \n    USJudgeRatings \n    Lawyers' Ratings of State Judges in the US Superior Court \n  \n  \n    datasets \n    USPersonalExpenditure \n    Personal Expenditure Data \n  \n  \n    datasets \n    UScitiesD \n    Distances Between European Cities and Between US Cities \n  \n  \n    datasets \n    VADeaths \n    Death Rates in Virginia (1940) \n  \n  \n    datasets \n    WWWusage \n    Internet Usage per Minute \n  \n  \n    datasets \n    WorldPhones \n    The World's Telephones \n  \n  \n    datasets \n    ability.cov \n    Ability and Intelligence Tests \n  \n  \n    datasets \n    airmiles \n    Passenger Miles on Commercial US Airlines, 1937-1960 \n  \n  \n    datasets \n    airquality \n    New York Air Quality Measurements \n  \n  \n    datasets \n    anscombe \n    Anscombe's Quartet of 'Identical' Simple Linear Regressions \n  \n  \n    datasets \n    attenu \n    The Joyner-Boore Attenuation Data \n  \n  \n    datasets \n    attitude \n    The Chatterjee-Price Attitude Data \n  \n  \n    datasets \n    austres \n    Quarterly Time Series of the Number of Australian Residents \n  \n  \n    datasets \n    beaver1 (beavers) \n    Body Temperature Series of Two Beavers \n  \n  \n    datasets \n    beaver2 (beavers) \n    Body Temperature Series of Two Beavers \n  \n  \n    datasets \n    cars \n    Speed and Stopping Distances of Cars \n  \n  \n    datasets \n    chickwts \n    Chicken Weights by Feed Type \n  \n  \n    datasets \n    co2 \n    Mauna Loa Atmospheric CO2 Concentration \n  \n  \n    datasets \n    crimtab \n    Student's 3000 Criminals Data \n  \n  \n    datasets \n    discoveries \n    Yearly Numbers of Important Discoveries \n  \n  \n    datasets \n    esoph \n    Smoking, Alcohol and (O)esophageal Cancer \n  \n  \n    datasets \n    euro \n    Conversion Rates of Euro Currencies \n  \n  \n    datasets \n    euro.cross (euro) \n    Conversion Rates of Euro Currencies \n  \n  \n    datasets \n    eurodist \n    Distances Between European Cities and Between US Cities \n  \n  \n    datasets \n    faithful \n    Old Faithful Geyser Data \n  \n  \n    datasets \n    fdeaths (UKLungDeaths) \n    Monthly Deaths from Lung Diseases in the UK \n  \n  \n    datasets \n    freeny \n    Freeny's Revenue Data \n  \n  \n    datasets \n    freeny.x (freeny) \n    Freeny's Revenue Data \n  \n  \n    datasets \n    freeny.y (freeny) \n    Freeny's Revenue Data \n  \n  \n    datasets \n    infert \n    Infertility after Spontaneous and Induced Abortion \n  \n  \n    datasets \n    iris \n    Edgar Anderson's Iris Data \n  \n  \n    datasets \n    iris3 \n    Edgar Anderson's Iris Data \n  \n  \n    datasets \n    islands \n    Areas of the World's Major Landmasses \n  \n  \n    datasets \n    ldeaths (UKLungDeaths) \n    Monthly Deaths from Lung Diseases in the UK \n  \n  \n    datasets \n    lh \n    Luteinizing Hormone in Blood Samples \n  \n  \n    datasets \n    longley \n    Longley's Economic Regression Data \n  \n  \n    datasets \n    lynx \n    Annual Canadian Lynx trappings 1821-1934 \n  \n  \n    datasets \n    mdeaths (UKLungDeaths) \n    Monthly Deaths from Lung Diseases in the UK \n  \n  \n    datasets \n    morley \n    Michelson Speed of Light Data \n  \n  \n    datasets \n    mtcars \n    Motor Trend Car Road Tests \n  \n  \n    datasets \n    nhtemp \n    Average Yearly Temperatures in New Haven \n  \n  \n    datasets \n    nottem \n    Average Monthly Temperatures at Nottingham, 1920-1939 \n  \n  \n    datasets \n    npk \n    Classical N, P, K Factorial Experiment \n  \n  \n    datasets \n    occupationalStatus \n    Occupational Status of Fathers and their Sons \n  \n  \n    datasets \n    precip \n    Annual Precipitation in US Cities \n  \n  \n    datasets \n    presidents \n    Quarterly Approval Ratings of US Presidents \n  \n  \n    datasets \n    pressure \n    Vapor Pressure of Mercury as a Function of Temperature \n  \n  \n    datasets \n    quakes \n    Locations of Earthquakes off Fiji \n  \n  \n    datasets \n    randu \n    Random Numbers from Congruential Generator RANDU \n  \n  \n    datasets \n    rivers \n    Lengths of Major North American Rivers \n  \n  \n    datasets \n    rock \n    Measurements on Petroleum Rock Samples \n  \n  \n    datasets \n    sleep \n    Student's Sleep Data \n  \n  \n    datasets \n    stack.loss (stackloss) \n    Brownlee's Stack Loss Plant Data \n  \n  \n    datasets \n    stack.x (stackloss) \n    Brownlee's Stack Loss Plant Data \n  \n  \n    datasets \n    stackloss \n    Brownlee's Stack Loss Plant Data \n  \n  \n    datasets \n    state.abb (state) \n    US State Facts and Figures \n  \n  \n    datasets \n    state.area (state) \n    US State Facts and Figures \n  \n  \n    datasets \n    state.center (state) \n    US State Facts and Figures \n  \n  \n    datasets \n    state.division (state) \n    US State Facts and Figures \n  \n  \n    datasets \n    state.name (state) \n    US State Facts and Figures \n  \n  \n    datasets \n    state.region (state) \n    US State Facts and Figures \n  \n  \n    datasets \n    state.x77 (state) \n    US State Facts and Figures \n  \n  \n    datasets \n    sunspot.month \n    Monthly Sunspot Data, from 1749 to \"Present\" \n  \n  \n    datasets \n    sunspot.year \n    Yearly Sunspot Data, 1700-1988 \n  \n  \n    datasets \n    sunspots \n    Monthly Sunspot Numbers, 1749-1983 \n  \n  \n    datasets \n    swiss \n    Swiss Fertility and Socioeconomic Indicators (1888) Data \n  \n  \n    datasets \n    treering \n    Yearly Treering Data, -6000-1979 \n  \n  \n    datasets \n    trees \n    Diameter, Height and Volume for Black Cherry Trees \n  \n  \n    datasets \n    uspop \n    Populations Recorded by the US Census \n  \n  \n    datasets \n    volcano \n    Topographic Information on Auckland's Maunga Whau Volcano \n  \n  \n    datasets \n    warpbreaks \n    The Number of Breaks in Yarn during Weaving \n  \n  \n    datasets \n    women \n    Average Heights and Weights for American Women"
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html#us-cities-and-counties",
    "href": "posts/007-open-data-for-datascience/index.html#us-cities-and-counties",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "US cities and counties",
    "text": "US cities and counties\n\n\nCode\ncity_county <- dplyr::filter(open_gov, Type == \"US City or County\")\nDT::datatable(city_county, options = list(pageLength = 5))"
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html#us-states",
    "href": "posts/007-open-data-for-datascience/index.html#us-states",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "US states",
    "text": "US states\n\n\nCode\nus_state <- dplyr::filter(open_gov, Type %in% c(\"US State\", \"Other State Related\"))\nDT::datatable(us_state, options = list(pageLength = 5))"
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html#international-countries-and-regions",
    "href": "posts/007-open-data-for-datascience/index.html#international-countries-and-regions",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "International countries and regions",
    "text": "International countries and regions\n\n\nCode\nint_count <- dplyr::filter(open_gov, Type %in% c(\"International Country\", \"International Regional\"))\nDT::datatable(int_count, options = list(pageLength = 5))"
  },
  {
    "objectID": "posts/007-open-data-for-datascience/index.html#sec-marylandapi",
    "href": "posts/007-open-data-for-datascience/index.html#sec-marylandapi",
    "title": "Sources of open data for statistics, data science, and machine learning",
    "section": "An example of using Maryland state open data via an API",
    "text": "An example of using Maryland state open data via an API\nSince I live and work in Maryland, I want to see how wages in Maryland and its counties have changed over time. I also want to test if Montgomery county (where I live) has different wages compared to Frederick, Howard, and Prince George’s counties which borders Montgomery on the north, east, and south sides. Therefore, in this example, I will fetch Maryland Average Wage Per Job (Current Dollars): 2010-2020 data via API using RSocrata library in R and carry out some analysis.\n\n\n\n\n\n\nNote\n\n\n\nSee https://dev.socrata.com/ to learn more about how to work with open data APIs in various programming languages.\n\n\nIn Table 2, each row has an average wage for a year for Maryland, and each of its counties (columns) from 2010-2020 and Figure 1 shows the same data as a line graph depicting the change in wages (y-axis) over time (x-axis).\nTable 3 lists the results of an unpaired two-sample t-test between wages from Montgomery and Frederick, Howard, and Prince George’s counties. As you can see from the t-test results, wages differ between Montgomery and Frederick, Howard, and Prince George’s counties, with Montogomery county residents earning higher than all its three bordering counties.\n\n\nCode\nlibrary(RSocrata)\n# Fetch the data using the API endpoint\nmaw <- read.socrata(\"https://opendata.maryland.gov/resource/mk5a-nf44.json\")\nknitr::kable(dplyr::select(maw, -date_created)) %>%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %>%\n    scroll_box(width = \"100%\", height = \"400px\")\n\n\n\n\nTable 2:  Maryland Average Wage Per Job (Current Dollars): 2010-2020 \n \n  \n    year \n    maryland \n    allegany_county \n    anne_arundel_county \n    baltimore_city \n    baltimore_county \n    calvert_county \n    caroline_county \n    carroll_county \n    cecil_county \n    charles_county \n    dorchester_county \n    frederick_county \n    garrett_county \n    harford_county \n    howard_county \n    kent_county \n    montgomery_county \n    prince_george_s_county \n    queen_anne_s_county \n    somerset_county \n    st_mary_s_county \n    talbot_county \n    washington_county \n    wicomico_county \n    worcester_county \n  \n \n\n  \n    2010 \n    53096 \n    35771 \n    56745 \n    55640 \n    49986 \n    42726 \n    34616 \n    38027 \n    42027 \n    41290 \n    35489 \n    48018 \n    31591 \n    46741 \n    58130 \n    36334 \n    65178 \n    51808 \n    36018 \n    38228 \n    60032 \n    37845 \n    38228 \n    38472 \n    30799 \n  \n  \n    2011 \n    54517 \n    36677 \n    58011 \n    57027 \n    50914 \n    43431 \n    35981 \n    39039 \n    42465 \n    42200 \n    35718 \n    48794 \n    32484 \n    48558 \n    60448 \n    36815 \n    67247 \n    52844 \n    36437 \n    39652 \n    63057 \n    38462 \n    39420 \n    38915 \n    31438 \n  \n  \n    2012 \n    55466 \n    36983 \n    58706 \n    58876 \n    51722 \n    44239 \n    37506 \n    39919 \n    43260 \n    42888 \n    37172 \n    49972 \n    32506 \n    49772 \n    62371 \n    36622 \n    68159 \n    53292 \n    37258 \n    39853 \n    63698 \n    39807 \n    39564 \n    39066 \n    31641 \n  \n  \n    2013 \n    55555 \n    37827 \n    59384 \n    59318 \n    51778 \n    44126 \n    38404 \n    40736 \n    44214 \n    42909 \n    37773 \n    49570 \n    33477 \n    49624 \n    62271 \n    37572 \n    67437 \n    53441 \n    36848 \n    40744 \n    63501 \n    39901 \n    40032 \n    39714 \n    32384 \n  \n  \n    2014 \n    56924 \n    38449 \n    60551 \n    61112 \n    52961 \n    45162 \n    39383 \n    41607 \n    45051 \n    44260 \n    39094 \n    50747 \n    34195 \n    50205 \n    64784 \n    38411 \n    68731 \n    54985 \n    37932 \n    41802 \n    64691 \n    40118 \n    41018 \n    40863 \n    33635 \n  \n  \n    2015 \n    58729 \n    39888 \n    62195 \n    63389 \n    54248 \n    48825 \n    41043 \n    43325 \n    46776 \n    44919 \n    40022 \n    51510 \n    35067 \n    52418 \n    66677 \n    38741 \n    71480 \n    56456 \n    38970 \n    43397 \n    65497 \n    41313 \n    42270 \n    42599 \n    34524 \n  \n  \n    2016 \n    59710 \n    40708 \n    63147 \n    64481 \n    55159 \n    53657 \n    40832 \n    43815 \n    47300 \n    46958 \n    40431 \n    51630 \n    34925 \n    52862 \n    67621 \n    39504 \n    72904 \n    57251 \n    39941 \n    43575 \n    65937 \n    41740 \n    42725 \n    43875 \n    35260 \n  \n  \n    2017 \n    61298 \n    42143 \n    64629 \n    66365 \n    56887 \n    55922 \n    42034 \n    45576 \n    48662 \n    47673 \n    41711 \n    52270 \n    35971 \n    53775 \n    68958 \n    40446 \n    74709 \n    58829 \n    42099 \n    45988 \n    67622 \n    43105 \n    44039 \n    45491 \n    35802 \n  \n  \n    2018 \n    62836 \n    43197 \n    66458 \n    67005 \n    58793 \n    53557 \n    43190 \n    45690 \n    49981 \n    48225 \n    41987 \n    53624 \n    37575 \n    54921 \n    71300 \n    42422 \n    76867 \n    60383 \n    43582 \n    45381 \n    68887 \n    44670 \n    45846 \n    45567 \n    37231 \n  \n  \n    2019 \n    64690 \n    44692 \n    68586 \n    69930 \n    60116 \n    51598 \n    45190 \n    47189 \n    52177 \n    49193 \n    43271 \n    55621 \n    38290 \n    57349 \n    74136 \n    42575 \n    78386 \n    62096 \n    44011 \n    49234 \n    70807 \n    45115 \n    46965 \n    46620 \n    38234 \n  \n  \n    2020 \n    70446 \n    48294 \n    74533 \n    74483 \n    65743 \n    55903 \n    49336 \n    51470 \n    55854 \n    53404 \n    47182 \n    60646 \n    40690 \n    62395 \n    82780 \n    45891 \n    86138 \n    66777 \n    48385 \n    53880 \n    77490 \n    48338 \n    50743 \n    50556 \n    41605 \n  \n\n\n\n\n\n\n\n\nCode\nmaw_gather <- maw %>% dplyr::select(-date_created) %>%\n  gather(key = \"county\", value = \"wage\", -year ) %>% as_tibble()\nggplot(maw_gather, aes(x = year, y = as.numeric(wage), color = county)) +\n  geom_line(aes(group = county)) + \n  labs(x = \"Year\", y = \"Wage\", color = \"\") +theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\nFigure 1: Maryland Average Wage Per Job (Current Dollars): 2010-2020\n\n\n\n\n\n\nCode\nmft <- broom::tidy(t.test(as.numeric(maw$montgomery_county), \n                   as.numeric(maw$frederick_county))) %>% \n                   dplyr::mutate(\"test\" = \"Montgomery vs. Frederick\")\n\nmht <- broom::tidy(t.test(as.numeric(maw$montgomery_county), \n                   as.numeric(maw$howard_county))) %>% \n                   dplyr::mutate(\"test\" = \"Montgomery vs. Howard\")\n\nmpgt <- broom::tidy(t.test(as.numeric(maw$montgomery_county), \n                    as.numeric(maw$prince_george_s_county))) %>% \n                    dplyr::mutate(\"test\" = \"Montgomery vs. Prince George's\")\n\nall_t <- dplyr::bind_rows(mft, mht, mpgt) %>%\n  dplyr::select(all_of(c(\"test\", \"estimate\", \"estimate1\", \n  \"estimate2\", \"statistic\", \"p.value\")))\n\nknitr::kable(all_t) %>%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\nTable 3:  T-test results between wages from Montgomery and Frederick, Howard, and Prince George’s county \n \n  \n    test \n    estimate \n    estimate1 \n    estimate2 \n    statistic \n    p.value \n  \n \n\n  \n    Montgomery vs. Frederick \n    20439.455 \n    72476 \n    52036.55 \n    9.452393 \n    0.0000001 \n  \n  \n    Montgomery vs. Howard \n    5250.909 \n    72476 \n    67225.09 \n    1.858408 \n    0.0781124 \n  \n  \n    Montgomery vs. Prince George's \n    15370.364 \n    72476 \n    57105.64 \n    6.597893 \n    0.0000030"
  },
  {
    "objectID": "posts/006-google-colab-for-scientific-software/index.html",
    "href": "posts/006-google-colab-for-scientific-software/index.html",
    "title": "A case for using Google Colab notebooks as an alternative to web servers for scientific software",
    "section": "",
    "text": "Updates\n\n\n\n\n\n2022-10-18 Typo correction and included a list of links to learn more about Google Colab.\n2022-10-20 The title and description changed. A PDF version of the article is uploaded to Zenodo at https://doi.org/10.5281/zenodo.7232109\n\n\n\nI recently came across ColabFold (Mirdita et al. 2022), a slimmer and faster implementation of AlphaFold2 (Jumper et al. 2021) (the famous protein structure prediction software from DeepMind) implemented on Google Colab in the form of a Jupyter notebook, giving it an easy-to-use web server-like interface. I found this idea intriguing as it removes the overhead of maintaining a webserver while providing a web-based graphical user interface.\nGoogle Colab is a free (with options for pro subscriptions) Jupyter notebook environment for Python (R indirectly) provided by Google that runs on unoccupied Google servers. This free resource also includes access to GPU and TPU making it attractive to various machine learning and data science tasks. For the most part, Google Colab is utilized in machine learning and data science education. However, following the example of ColabFold and my implementation of ColabHDStIM, I want to make a case that it can also be used for providing an easy-to-use interface or live demo for scientific software without maintaining the complex infrastructure of a web server.\nComing from a bioinformatics/computational biology background, I know there is a craze for developing web servers worldwide. However, although many web servers are created yearly, many groups, especially in developing countries, lack the resources to build one. On the flip side, many of these initially well-funded web servers are either of low quality, are not kept updated, or go offline soon after the publication, thus squandering the resources (Veretnik, Fink, and Bourne 2008; Schultheiss et al. 2011; Kern, Fehlmann, and Keller 2020). Therefore, there is a need for an alternative where scientists can distribute their software in an easy-to-use interface like interactive notebooks. Even if the notebook environments are limited in executing production-scale software, they can still be utilized to provide a live demo on a minimal dataset. In my opinion, it is better than the vignettes accompanying software. \nBelow are some pros and cons of using Google Colab.\nPros\n\nEasy to implement\nFree hardware resources from Google, including GPU and TPU\nOption to buy more resources from Google as per need\nWhile hosted on Google’s server, the same notebook can be executed using a local runtime to take advantage of local hardware resources.\nForkable and hackable if the original maintainer stops the development.\n\nCons\n\nFree hardware resources can be limiting\nUploading and downloading data to a Colab is slow and require a workaround\nAll the instances are transient; therefore, on every restart, all the required software is re-installed, which takes time.\nColab notebooks are meant to run interactively; therefore, maintaining a long background session is hard or impossible.\nColab primarily supports Python and requires workarounds to support other languages.\n\nLearn more about Google Colab\n\nGoogle Colab frequently asked questions\nWelcome to Colab!\nPractical introduction to Google Colab for data science (YouTube video)\n\nReferences\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nKern, Fabian, Tobias Fehlmann, and Andreas Keller. 2020. “On the Lifetime of Bioinformatics Web Services.” Nucleic Acids Research 48 (22): 12523–33. https://doi.org/10.1093/nar/gkaa1125.\n\n\nMirdita, Milot, Konstantin Schütze, Yoshitaka Moriwaki, Lim Heo, Sergey Ovchinnikov, and Martin Steinegger. 2022. “ColabFold: Making Protein Folding Accessible to All.” Nature Methods 19 (6): 679–82. https://doi.org/10.1038/s41592-022-01488-1.\n\n\nSchultheiss, Sebastian J., Marc-Christian Münch, Gergana D. Andreeva, and Gunnar Rätsch. 2011. “Persistence and Availability of Web Services in Computational Biology.” Edited by Dongxiao Zhu. PLoS ONE 6 (9): e24914. https://doi.org/10.1371/journal.pone.0024914.\n\n\nVeretnik, Stella, J. Lynn Fink, and Philip E. Bourne. 2008. “Computational Biology Resources Lack Persistence and Usability.” Edited by Barbara Bryant. PLoS Computational Biology 4 (7): e1000136. https://doi.org/10.1371/journal.pcbi.1000136.\n\n\n\n\n\nCitationBibTeX citation:@misc{farmer2022,\n  author = {Rohit Farmer},\n  publisher = {Zenodo},\n  title = {A Case for Using {Google} {Colab} Notebooks as an Alternative\n    to Web Servers for Scientific Software},\n  date = {2022-10-17},\n  url = {https://doi.org/10.5281/zenodo.7232109},\n  doi = {10.5281/zenodo.7232109},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRohit Farmer. 2022. “A Case for Using Google Colab Notebooks as an\nAlternative to Web Servers for Scientific Software.” Zenodo. https://doi.org/10.5281/zenodo.7232109."
  },
  {
    "objectID": "posts/005-classify-the-bitter-or-sweet-taste-of-compounds/index.html",
    "href": "posts/005-classify-the-bitter-or-sweet-taste-of-compounds/index.html",
    "title": "Classify the bitter or sweet taste of compounds",
    "section": "",
    "text": "Original Post\n\n\n\nThis post is an identical copy of “About Dataset” at Kaggle: https://www.kaggle.com/dsv/4234193\n\n\n\nContext\nThroughout human evolution, we have been drawn toward sweet-tasting foods and averted from bitter tastes - sweet is good or desirable, bitter is undesirable, ear wax or medicinal. Therefore, a better understanding of molecular features that determine the bitter-sweet taste of substances is crucial for identifying natural and synthetic compounds for various purposes.\n\n\nSources\nThis dataset is adapted from https://github.com/cosylabiiit/bittersweet, https://www.nature.com/articles/s41598-019-43664-y. In chemoinformatics, molecules are often represented as compact SMILES strings. In this dataset, SMILES structures, along with their names and targets (bitter, sweet, tasteless, and non-bitter), were obtained from the original study. Subsequently, SMILES were converted into canonical SMILES using RDKit, and the features (molecular descriptors, both 2D and 3D) were calculated using Mordred. Secondly, tasteless and non-bitter categories were merged into a single category of non-bitter-sweet. Finally, since many of the compounds were missing names, IUPAC names were fetched using PubChemPy for all the compounds, and for still missing names, a generic compound + incrementor name was assigned.\n\n\nInspiration\nThis is a classification dataset with the first three columns carrying names, SMILES, and canonical SMILES. Any of these columns can be used to refer to a molecule. The fourth column is the target (taste category). And all numeric features are from the 5th column until the end of the file. Many features have cells with string annotations due to errors produced by Mordred. Therefore, the following data science techniques can be learned while working on this dataset:\n\nData cleanup\nFeatures selection (since the number of features is quite large in proportion to the data points)\nFeature scaling/transformation/normalization\nDimensionality reduction\nBinomial classification (bitter vs. sweet) - utilize non-bitter-sweet as a negative class.\nMultinomial classification (bitter vs. sweet vs. non-bitter-sweet)\nSince SMILES can be converted into molecular graphs, graph-based modeling should also be possible.\n\n\n\nInitial data preparation\nA copy of the original dataset and the scripts and notebooks used to convert SMILES to canonical SMILES, generate features, fetch names, and export the final TSV file for Kaggle is loosely maintained at https://github.com/rohitfarmer/bittersweet.\n\n\n\n\nCitationBibTeX citation:@dataset{farmer2022,\n  author = {Rohit Farmer},\n  publisher = {Kaggle},\n  title = {Classify the Bitter or Sweet Taste of Compounds},\n  date = {2022-10-15},\n  url = {https://www.kaggle.com/dsv/4234193},\n  doi = {10.34740/KAGGLE/DSV/4234193},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRohit Farmer. 2022. “Classify the Bitter or Sweet Taste of\nCompounds.” Kaggle. https://doi.org/10.34740/KAGGLE/DSV/4234193."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "",
    "text": "Note: This tutorial is written for Linux based systems."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#requirements",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#requirements",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Requirements",
    "text": "Requirements\n\nR >= 3.0.0\nTo install the latest version of R please follow the download and install instructions at https://cloud.r-project.org/\n\n\nNeovim >= 0.2.0\nNeovim (nvim) is the continuation and extension of Vim editor with the aim to keep the good parts of Vim and add more features. In this tutorial I will be using Neovim (nvim), however, most of the steps are equally applicable to Vim also. Please follow download and installation instructions on nvim’s GitHub wiki https://github.com/neovim/neovim/wiki/Installing-Neovim.\nOR\n\n\nVim >= 8.1\nVim usually comes installed in most of the Linux based operating system. However, it may not be the latest one. Therefore, to install the latest version please download and install it from Vim’s GitHub repository as mentioned below or a method that is more confortable to you.\ngit clone https://github.com/vim/vim.git\nmake -C vim/\nsudo make install -C vim/\n\n\nPlugin Manager\nThere are more than one plugin manager’s available for Vim that can be used to install the required plugins. In this tutorial I will be using vim-plug pluggin manager.\n\n\nPlugins\nIn the end below are the plugins that we would need to convert Vim editor into a fully functional IDE for R.\n\nNvim-R: https://github.com/jalvesaq/Nvim-R\n\nNvim-R is the main plugin that will add the functionality to execute R code from within the Vim editor.\n\nNcm-R: https://github.com/gaalcaras/ncm-R\n\nNcm-R adds synchronous auto completion features for R.\nIt is based on ncm2 and nvim-yarp plugins.\n\nNerd Tree: https://github.com/preservim/nerdtree\n\nNerd Tree will be used to toggle file explorer in the side panel.\n\nDelimitMate: https://github.com/Raimondi/delimitMate\n\nThis plug-in provides automatic closing of quotes, parenthesis, brackets, etc.\n\nVim-monokai-tasty: https://github.com/patstockwell/vim-monokai-tasty\n\nMonokai color scheme inspired by Sublime Text’s interpretation of monokai.\n\nLightline.vim: https://github.com/itchyny/lightline.vim\n\nLineline.vim adds asthetic enhancements to Vim’s statusline/tabline."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#procedure",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#procedure",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Procedure",
    "text": "Procedure\n\nMake sure that you have R >=3.0.0 installed.\nMake sure that you have Neovim >= 0.2.0 installed.\nInstall the vim-plug plugin manager.\n\ncurl -fLo ~/.local/share/nvim/site/autoload/plug.vim --create-dirs \\\n    https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n\nInstall the required plugins.\n\nFirst, create an init.vim file in ~/.config/nvim folder (create the folder if it doesn’t exist). This file is equivalent to a .vimrc file in the traditional Vim environment. To init.vim file start adding:\n\" Specify a directory for plugins\n\" - Avoid using standard Vim directory names like 'plugin'\ncall plug#begin('~/.vim/plugged')\n\n\" List of plugins.\n\" Make sure you use single quotes\n\n\" Shorthand notation\nPlug 'jalvesaq/Nvim-R'\nPlug 'ncm2/ncm2'\nPlug 'roxma/nvim-yarp'\nPlug 'gaalcaras/ncm-R'\nPlug 'preservim/nerdtree'\nPlug 'Raimondi/delimitMate'\nPlug 'patstockwell/vim-monokai-tasty'\nPlug 'itchyny/lightline.vim'\n\n\" Initialize plugin system\ncall plug#end()\n\nUpdate and add more features to the init.vim file.\n\n\" Set a Local Leader\n\n\" With a map leader it's possible to do extra key combinations\n\" like <leader>w saves the current file\nlet mapleader = \",\"\nlet g:mapleader = \",\"\n\n\n\" Plugin Related Settings\n\n\" NCM2\nautocmd BufEnter * call ncm2#enable_for_buffer()    \" To enable ncm2 for all buffers.\nset completeopt=noinsert,menuone,noselect           \" :help Ncm2PopupOpen for more\n                                                    \" information.\n\n\" NERD Tree\nmap <leader>nn :NERDTreeToggle<CR>                  \" Toggle NERD tree.\n\n\" Monokai-tasty\nlet g:vim_monokai_tasty_italic = 1                  \" Allow italics.\ncolorscheme vim-monokai-tasty                       \" Enable monokai theme.\n\n\" LightLine.vim \nset laststatus=2              \" To tell Vim we want to see the statusline.\nlet g:lightline = {\n   \\ 'colorscheme':'monokai_tasty',\n   \\ }\n\n\n\" General NVIM/VIM Settings\n\n\" Mouse Integration\nset mouse=i                   \" Enable mouse support in insert mode.\n\n\" Tabs & Navigation\nmap <leader>nt :tabnew<cr>    \" To create a new tab.\nmap <leader>to :tabonly<cr>     \" To close all other tabs (show only the current tab).\nmap <leader>tc :tabclose<cr>    \" To close the current tab.\nmap <leader>tm :tabmove<cr>     \" To move the current tab to next position.\nmap <leader>tn :tabn<cr>        \" To swtich to next tab.\nmap <leader>tp :tabp<cr>        \" To switch to previous tab.\n\n\n\" Line Numbers & Indentation\nset backspace=indent,eol,start  \" To make backscape work in all conditions.\nset ma                          \" To set mark a at current cursor location.\nset number                      \" To switch the line numbers on.\nset expandtab                   \" To enter spaces when tab is pressed.\nset smarttab                    \" To use smart tabs.\nset autoindent                  \" To copy indentation from current line \n                                \" when starting a new line.\nset si                          \" To switch on smart indentation.\n\n\n\" Search\nset ignorecase                  \" To ignore case when searching.\nset smartcase                   \" When searching try to be smart about cases.\nset hlsearch                    \" To highlight search results.\nset incsearch                   \" To make search act like search in modern browsers.\nset magic                       \" For regular expressions turn magic on.\n\n\n\" Brackets\nset showmatch                   \" To show matching brackets when text indicator \n                                \" is over them.\nset mat=2                       \" How many tenths of a second to blink \n                                \" when matching brackets.\n\n\n\" Errors\nset noerrorbells                \" No annoying sound on errors.\n\n\n\" Color & Fonts\nsyntax enable                   \" Enable syntax highlighting.\nset encoding=utf8                \" Set utf8 as standard encoding and \n                                 \" en_US as the standard language.\n\n\" Enable 256 colors palette in Gnome Terminal.\nif $COLORTERM == 'gnome-terminal'\n    set t_Co=256\nendif\n\ntry\n    colorscheme desert\ncatch\nendtry\n\n\n\" Files & Backup\nset nobackup                     \" Turn off backup.\nset nowb                         \" Don't backup before overwriting a file.\nset noswapfile                   \" Don't create a swap file.\nset ffs=unix,dos,mac             \" Use Unix as the standard file type.\n\n\n\" Return to last edit position when opening files\nau BufReadPost * if line(\"'\\\"\") > 1 && line(\"'\\\"\") <= line(\"$\") | exe \"normal! g'\\\"\" | endif"
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#frequently-used-keyboard-shortcutscommands",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#frequently-used-keyboard-shortcutscommands",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Frequently Used Keyboard Shortcuts/Commands",
    "text": "Frequently Used Keyboard Shortcuts/Commands\nNote: The commands below are according to the init.vim settings mentioned in this Gist.\n# Nvim-R\n\\rf               \" Connect to R console.\n\\rq               \" Quit R console.\n\\ro               \" Open object bowser.\n\\d                \" Execute current line of code and move to the next line.\n\\ss               \" Execute a block of selected code.\n\\aa               \" Execute the entire script. This is equivalent to source().\n\\xx               \" Toggle comment in an R script.\n\n# NERDTree\n,nn               \" Toggle NERDTree."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#example-code",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#example-code",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Example Code",
    "text": "Example Code\nlibrary(tidyverse)\n# \\rf               \" Connect to R console.\n# \\rq               \" Quit R console.\n# \\ro               \" Open object bowser.\n# \\d \\ss \\aa        \" Execution modes. \n# ?help\n# ,nn               \" NERDTree.\n# ,nt, tp, tn       \" Tab navigation.\n\ntheme_set(theme_bw())\ndata(\"midwest\", package = \"ggplot2\")\n\ngg  <- ggplot(midwest, aes(x=area, y = poptotal)) +\n        geom_point(aes(col = state, size = popdensity)) +\n        geom_smooth(method = \"loess\", se = F) +\n        xlim(c(0, 0.1)) +\n        ylim(c(0, 500000)) +\n        labs(subtitle = \"Area Vs Population\",\n             y = \"Population\",\n             x = \"Area\",\n             title = \"Scatterplot\",\n             caption = \"Source: midwest\")\n\nplot(gg) # Opens an external window with the plot.\n\nmidwest$county # To show synchronous auto completion. \n\nView(midwest) # Opens an external window to display a portion of the tibble."
  },
  {
    "objectID": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#add-colour-etc.-to-vim-in-a-screen-session-optional",
    "href": "posts/004-how-to-use-neovim-or-vim-editor-as-an-ide-for-r/index.html#add-colour-etc.-to-vim-in-a-screen-session-optional",
    "title": "How to use Neovim or VIM editor as an IDE for R",
    "section": "Add Colour etc. to VIM in a Screen Session (optional)",
    "text": "Add Colour etc. to VIM in a Screen Session (optional)\nAdd these lines to ~/.screenrc file.\n# Use 256 colors\nattrcolor b \".I\"    # allow bold colors - necessary for some reason\ntermcapinfo xterm 'Co#256:AB=\\E[48;5;%dm:AF=\\E[38;5;%dm'   # tell screen how to set colors. AB = background, AF=foreground\ndefbce on    # use current bg color for erased chars]]'\n\n# Informative statusbar\nhardstatus off\nhardstatus alwayslastline\nhardstatus string '%{= kG}[ %{G}%H %{g}][%= %{= kw}%?%-Lw%?%{r}(%{W}%n*%f%t%?(%u)%?%{r})%{w}%?%+Lw%?%?%= %{g}][%{B} %m-%d %{W} %c %{g}]'\n\n# Use X scrolling mechanism\ntermcapinfo xterm* ti@:te@\n\n# Fix for residual editor text\naltscreen on"
  },
  {
    "objectID": "posts/009-rules-for-naming-files/index.html",
    "href": "posts/009-rules-for-naming-files/index.html",
    "title": "Rules for naming files and folders that are cross platform and helpful in datascience",
    "section": "",
    "text": "Update history\n\n\n\n\n\n2022-11-03 Corrections made as pointed by https://fosstodon.org/@rudolf read this thread https://fosstodon.org/@swatantra/109281773145979327"
  },
  {
    "objectID": "posts/009-rules-for-naming-files/index.html#some-examples",
    "href": "posts/009-rules-for-naming-files/index.html#some-examples",
    "title": "Rules for naming files and folders that are cross platform and helpful in datascience",
    "section": "Some Examples",
    "text": "Some Examples\n2022-08-31-labnotebook-for-hdstim.docx\n\nfigure-01.png\nfigure-02.png\nfigure-03.png\n\n/path/to/folder/exploring-flow"
  },
  {
    "objectID": "posts/003-how-to-download-a-shared-file-from-googledrive-in-r/index.html",
    "href": "posts/003-how-to-download-a-shared-file-from-googledrive-in-r/index.html",
    "title": "How to download a shared file from Google Drive in R",
    "section": "",
    "text": "To download a shared file with “anyone with the link” access rights from Google Drive in R, we can utilize the googledrive library from the tidyverse package. The method described here will utilize the file ID copied from the shared link. Typically googledrive package is used to work with a Google Drive of an authenticated user. However, since we are downloading a publicly shared file in this tutorial, we will work without user authentication. So, please follow the steps below.\n\n\nBelow is a share link from my Google Drive pointing to an R data frame.\nhttps://drive.google.com/file/d/1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4/view?usp=sharing\nThis string 1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4 is the file ID that we will use to download.\n\n\n\n\nif(!require(googledrive)) install.packages(\"googledrive\")\nlibrary(googledrive)\n\ndrive_deauth()\ndrive_user()\npublic_file <-  drive_get(as_id(\"1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4\"))\ndrive_download(public_file, overwrite = TRUE)\n\nFile downloaded:\n• hdstim-example-data.rds <id: 1vj607etanUVYzVFj_HXkznHTd0Ltv_Y4>\nSaved locally as:\n• hdstim-example-data.rds\nThe downloaded data frame.\n\nlibrary(DT)\ndatatable(head(readRDS(\"hdstim-example-data.rds\")))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data All The Way",
    "section": "",
    "text": "Welcome to Data All The Way, “yet another” tutorial website with concepts, methods, and example code on data science, statistics, and machine learning. All the posts with example code accompany a Kaggle or Google Colab notebook for a live demo. In addition, posts are citable, and some posts are hosted on Zenodo with a permanent DOI and indexed in Data All The Way community. If you want to become a contributing author, please write to rohit [dot] farmer [at] dataalltheway [dot] com. Comments and suggestions are always welcomed. To learn more about my other projects and research, please visit https://rohitfarmer.com."
  },
  {
    "objectID": "index.html#featured-posts",
    "href": "index.html#featured-posts",
    "title": "Data All The Way",
    "section": "Featured posts",
    "text": "Featured posts"
  },
  {
    "objectID": "bookmarks/index.html",
    "href": "bookmarks/index.html",
    "title": "Bookmarks",
    "section": "",
    "text": "Below are some of the articles, blog posts, and stack exchange threads that I have found helpful. Please comment if you know or found something interesting that I should list here.\n\nCheat sheets\n\nChoosing the right estimator\nMachine learning cheat sheet\nMachine learning glossary\nThe neural network zoo\n\n\n\nData processing\n\nNormalize data before or after split of training and testing data?\n\n\n\nR\n\nBook: Advanced R by Hadley Wickham\nTeaching R in a Kinder, Gentler, More Effective Manner: Teach Base-R, Not Just the Tidyverse Author: Prof. Norm Matloff, University of California, Davis\nR Workflow - An overview of R Workflow, which covers how to use R effectively all the way from importing data to analysis, and making use of Quarto for reproducible reporting.\n\n\n\nStatistics\n\nThe ASA Statement on p-Values: Context, Process, and Purpose\n\n\n\n\n\n\n\nStatistical significance is not equivalent to scientific, human, or economic significance\n\n\n\n\n\nStatistical significance is not equivalent to scientific, human, or economic significance. Smaller p-values do not necessarily imply the presence of larger or more important effects, and larger p-values do not imply alack of importance or even lack of effect… No single index should substitute for scientific reasoning.\n\n\n\n\nStatistical Inference in the 21st Century: A World Beyond p < 0.05\n\n\n\n\n\n\n\nDon’t Say “Statistically Significant”\n\n\n\n\n\nThe ASA Statement on P-Values and Statistical Significance stopped just short of recommending that declarations of “statistical significance” be abandoned. We take that step here. We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term “statistically significant” entirely. Nor should variants such as “significantly different,” “p < 0.05,” and “nonsignificant” survive, whether expressed in words, by asterisks in a table, or in some other way.\nRegardless of whether it was ever useful, a declaration of “statistical significance” has today become meaningless. Made broadly known by Fisher’s use of the phrase (1925), Edgeworth’s (1885) original intention for statistical significance was simply as a tool to indicate when a result warrants further scrutiny. But that idea has been irretrievably lost. Statistical significance was never meant to imply scientific importance, and the confusion of the two was decried soon after its widespread use (Boring 1919). Yet a full century later the confusion persists.\n\n\n\n\nBayesian and frequentist reasoning in plain English\nUltimate Guide to Statistics for Data Science\nBook: Improving Your Statistical Inferences\nBook/course: Online Statistics Education: An Interactive Multimedia Course of Study\nBook: Bayesian Data Analysis Third edition by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin.\nBook: Regression and Other Stories by Andrew Gelman, Jennifer Hill, Aki Vehtari\n\n\n\nMachine learning\n\nA free deep learning course"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nNov 30, 2022\n\n\nNon-parametric hypothesis tests with examples in Julia\n\n\nDhruva Sambrani\n\n\n\n\nNov 21, 2022\n\n\nType inference in readr and arrow\n\n\nNic Crane\n\n\n\n\nNov 18, 2022\n\n\nNon-parametric hypothesis tests with examples in R\n\n\nRohit Farmer\n\n\n\n\nNov 17, 2022\n\n\nParametric hypothesis tests with examples in Julia\n\n\nDhruva Sambrani\n\n\n\n\nNov 10, 2022\n\n\nParametric hypothesis tests with examples in R\n\n\nRohit Farmer\n\n\n\n\nNov 3, 2022\n\n\nRules for naming files and folders that are cross platform and helpful in datascience\n\n\nRohit Farmer\n\n\n\n\nOct 31, 2022\n\n\nHow to build a Singularity container for machine learning, data science, and chemistry\n\n\nRohit Farmer\n\n\n\n\nOct 25, 2022\n\n\nSources of open data for statistics, data science, and machine learning\n\n\nRohit Farmer\n\n\n\n\nOct 17, 2022\n\n\nA case for using Google Colab notebooks as an alternative to web servers for scientific software\n\n\nRohit Farmer\n\n\n\n\nOct 15, 2022\n\n\nClassify the bitter or sweet taste of compounds\n\n\nRohit Farmer\n\n\n\n\nOct 15, 2022\n\n\nHow to use Neovim or VIM editor as an IDE for R\n\n\nRohit Farmer\n\n\n\n\nOct 14, 2022\n\n\nHow to download a shared file from Google Drive in R\n\n\nRohit Farmer\n\n\n\n\nOct 5, 2022\n\n\nTweets from heads of governments and states\n\n\nRohit Farmer\n\n\n\n\nOct 5, 2022\n\n\nData Transformation\n\n\nRohit Farmer\n\n\n\n\n\n\nNo matching items"
  }
]